{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Include source to web search\n",
    "# Include a case if the user asks for more than one action on the modelling tools in a single query (for now we'll take care of just one at a time)\n",
    "# Connect all outputs of modelling tools to a decision router to verify if there was a valid selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary dependencies\n",
    "\n",
    "## Model instantiation\n",
    "# %pip install transformers -U\n",
    "# %pip -q install langchain-groq\n",
    "\n",
    "## RAG node\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install faiss-cpu\n",
    "\n",
    "## Web search node\n",
    "# %pip install -U langchain-community tavily-python\n",
    "\n",
    "## Graph building\n",
    "# %pip install -U langgraph\n",
    "\n",
    "## Model modifications in Excel\n",
    "# %pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../metadata/secrets.yml', 'r') as f:\n",
    "    secrets = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "To test it, first run 'ollama serve' in a local terminal. (necessary only for the embeddings of the RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = secrets['groq'][0]\n",
    "chat_model = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "json_model = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "        ).bind(response_format={\"type\": \"json_object\"})\n",
    "\n",
    "test_model = ChatGroq(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "        )\n",
    "test_json_model = ChatGroq(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "        ).bind(response_format={\"type\": \"json_object\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative model\n",
    "\n",
    "Use this one if GROQ stops working (will need to figure out the response as JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import HuggingFaceEndpoint\n",
    "# from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", huggingfacehub_api_token=secrets['huggingface'][0])\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you! I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation, answer questions, and even generate text. I can chat with you about a wide range of topics, from science and history to entertainment and culture. I'm constantly learning and improving, so please bear with me if I make any mistakes. What would you like to talk about?\", response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 16, 'total_tokens': 119, 'completion_time': 0.327875371, 'prompt_time': 0.003940442, 'queue_time': None, 'total_time': 0.331815813}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None}, id='run-f8456faf-65ff-468c-8603-6092be18ea23-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke('Hello, who are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State\n",
    "\n",
    "Defines the graph's state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        initial_query: user input\n",
    "        next_query: partial query generated by the agent\n",
    "        num_steps: number of steps\n",
    "        selected_tool: name of the selected tool\n",
    "        identified_model: name of the model identified by the agent\n",
    "        rag_questions: questions used for retrieval\n",
    "        tool_parameters: parameters to be used by tools\n",
    "        context: list of context generated for the query\n",
    "        complete_data: indicates completeness of data\n",
    "        final_answer: LLM generation\n",
    "    \"\"\"\n",
    "    initial_query : str\n",
    "    query_type: str\n",
    "    next_query: str\n",
    "    num_steps : int\n",
    "    selected_tool: str\n",
    "    rag_questions : List[str]\n",
    "    tool_parameters: str\n",
    "    context : List[str]\n",
    "    complete_data : bool\n",
    "    model: str\n",
    "    scenario: str\n",
    "    parameter: str\n",
    "    cs: str\n",
    "    variable: str\n",
    "    plot_type: str\n",
    "    new_value: float\n",
    "    selection_is_valid: bool\n",
    "    history: List[dict]\n",
    "    final_answer : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_builder(history, new_message, role):\n",
    "    history = history.append({\"role\": role, \"content\": new_message})\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type identifier node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': ['modify_model', 'run_model', 'compare_results', 'plot_result'], 'action_context': ['production cost of coal electricity to 800 Euro/kW', '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "esm_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at breaking down a multi step command given by the user into many smaller\n",
    "    and single step commands to be run by other LLMs.\\n\n",
    "\n",
    "    The following actions are available and can be referenced as individual actions: ['modify_model',\n",
    "    'run_model', 'compare_results', 'compare_yearly_inputs', 'compare_inputs', 'plot_result',\n",
    "    'plot_comparison']\\n\n",
    "    \n",
    "    You must output a JSON with two keys 'action' and 'action_context', where action must be\n",
    "    a list with the actions to be taken (in order), where each value should be part of the given array, and context \n",
    "    is the information needed for the selected action (also a list in the same order as action). If you\n",
    "    don't need any extra information for an action, simply use an empty string.\\n\n",
    "    \n",
    "    You can use the comparison actions to explain details about the model to the user.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    COMMAND : {command} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"command\"],\n",
    ")\n",
    "esm_chain = esm_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "command = 'How would the model react if we changed the production cost of coal electricity to 800 Euro/kW?'\n",
    "print(esm_chain.invoke({\"command\": command}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_identifier_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at identifying the type of a query provided by the user among\n",
    "    the types \"general\", \"energy_system\" and \"mixed\".\n",
    "\n",
    "    \"general\": the query is related to some generic topic, it may consist of one or more\n",
    "    points that require searching for information. \\n\n",
    "    \n",
    "    \"energy_system\": the query is a direct command related to the energy system model, it can\n",
    "    be a request to change parameters, plot data, run simulations, or anything on this lines.\n",
    "    To be characterized as this class, it should need no external information. Names of\n",
    "    simulations, scenarios, parameters and any other potential name is assumed to be know by our tools. \\n\n",
    "    \n",
    "    \"mixed\": the query is related to the energy system model, but it requires external data for the\n",
    "    command to be complete. It MUST be related to running anything related to the energy system,\n",
    "    otherwise it is not mixed. \\n\n",
    "    \n",
    "    You must output a JSON with a single key 'query_type' containing exclusivelly the \n",
    "    selected type. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "type_identifier_chain = type_identifier_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# query = 'Modify the cost of CO2 in 2020 to be same price as a liter of Coca Cola'\n",
    "# print(type_identifier_chain.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_identifier(state):\n",
    "    \n",
    "    print(\"---TYPE IDENTIFIER---\")\n",
    "    query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "\n",
    "    gen = type_identifier_chain.invoke({\"query\": query})\n",
    "    selected_type = gen['query_type']\n",
    "    \n",
    "    print(f'IDENTIFIED_TYPE: {selected_type}\\n')\n",
    "    \n",
    "    return {\"query_type\": selected_type,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy System tool selector node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_tool_selector_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading the user QUERY and routing it to the correct tool in our\n",
    "    modelling system. \\n\n",
    "\n",
    "    Use the following criteria to decide how to route the query to one of our available tools: \\n\\n\n",
    "    \n",
    "    If the user asks for any modification on any particular model, select 'model_modifier'. \\n\n",
    "    \n",
    "    If the user asks to plot anything, select 'data_plotter'. \\n\n",
    "    \n",
    "    If the user asks to run a simulation of any particular model, select 'sim_runner'. \\n\n",
    "\n",
    "    You must output a JSON object with two keys:\n",
    "    'selected_tool' containing one of the following values ['model_modificator', 'data_plotter', 'sim_runner'];\n",
    "    'selected_model' containing the name of the model to be manipulated. \\n\n",
    "    \n",
    "    If the user didn't provide a model name, fill the key 'selected_model' with 'NO_MODEL'. \\n\n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "es_tool_selector_chain = es_tool_selector_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# initial_query = 'Modify the lifetime of wind power plants to 20 years'\n",
    "# print(es_tool_selector_chain.invoke({\"query\": initial_query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_tool_selector(state):\n",
    "    \n",
    "    print(\"---ENERGY SYSTEM TOOL SELECTION---\")\n",
    "    query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "\n",
    "    router = es_tool_selector_chain.invoke({\"query\": query})\n",
    "    router_decision = router['selected_tool']\n",
    "    identified_model = router['selected_model']\n",
    "    \n",
    "    print(f'SELECTED TOOL: {router_decision}')\n",
    "    print(f'IDENTIFIED MODEL: {identified_model}\\n')\n",
    "    \n",
    "    return {\"selected_tool\": router_decision,\n",
    "            \"identified_model\": identified_model,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selector node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "def model_selector(state):\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(\"No valid model was found for the requested action, the available models are:\\n\")\n",
    "    \n",
    "    available_models = next(walk('Models'), (None, None, []))[2]\n",
    "    for i in range(len(available_models)):\n",
    "        print(f'{i+1}: {available_models[i]}')\n",
    "    \n",
    "    selected_model = input('Please, inform the number of the desired model:\\n')\n",
    "    \n",
    "    return {\"model\": available_models[int(selected_model)-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validated_model(state):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading the user QUERY and the available CONTEXT to decide if there\n",
    "    is already enough information gathered to fulfill the energy system related command\n",
    "    made by the user. \\n\n",
    "    \n",
    "    You must be certain that you have all the data before deciding to send it to the\n",
    "    modelling section of the pipeline. If any of the values asked by the user is not\n",
    "    directly given by him, you can't consider the data complete unless you have the\n",
    "    desired value in the CONTEXT. \\n\n",
    "\n",
    "    You must output a JSON object with a single key 'complete_data' containing a boolean\n",
    "    on whether you have enough data for the user's request or not. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\",\"context\"],\n",
    ")\n",
    "mixed_chain = mixed_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# initial_query = 'Modify the lifetime of wind power plants to be the age of Ronaldinho Gaucho plus Oprah age'\n",
    "# context = ['The current age of Ronaldinho Gaucho is 44 years old', 'Oprah is 68 years old']\n",
    "# print(mixed_chain.invoke({\"query\": initial_query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed(state):\n",
    "    \n",
    "    print(\"---TOOL SELECTION---\")\n",
    "    query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "    print(f'CONTEXT: {context}')\n",
    "\n",
    "    decision = mixed_chain.invoke({\"query\": query, \"context\": context})\n",
    "    decision = decision['complete_data']\n",
    "    \n",
    "    print(f'DATA IS COMPLETE: {decision}\\n')\n",
    "    \n",
    "    return {\"complete_data\": decision,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool selector node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_selector_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading a QUERY from a user and routing to our internal knowledge system\\\n",
    "     or directly to final answer. \\n\n",
    "\n",
    "    Use the following criteria to decide how to route the query to one of our available tools: \\n\\n\n",
    "    \n",
    "    If the user asks anything about LangSmith, you should use the 'RAG_retriever' tool.\n",
    "    \n",
    "    For any mathematical problem you should use 'calculator'. Be sure that you have all the necessary\n",
    "    data before routing to this tool.\n",
    "\n",
    "    If you are unsure or the person is asking a question you don't understand then choose 'web_search'\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web_search.\n",
    "    Give a choice contained in ['RAG_retriever','calculator','web_search'].\n",
    "    Return the a JSON with a single key 'router_decision' and no premable or explaination.\n",
    "    Use the initial query of the user and any available context to make your decision about the tool to be used.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "tool_selector_chain = tool_selector_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# initial_query = 'Please, let me know the weather in San Francisco'\n",
    "# print(tool_selector_chain.invoke({\"query\": initial_query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_selector(state):\n",
    "    \n",
    "    print(\"---TOOL SELECTION---\")\n",
    "    query = state['next_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "\n",
    "    router = tool_selector_chain.invoke({\"query\": query})\n",
    "    router_decision = router['router_decision']\n",
    "    \n",
    "    print(f'SELECTED TOOL: {router_decision}\\n')\n",
    "    \n",
    "    return {\"selected_tool\": router_decision,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web/RAG answer analyzer prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_analyzer_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at summarizing a bunch of data to extract only the important bits from it.\n",
    "\n",
    "    Given the user's QUERY and the SEARCH_RESULTS, summarize as briefly as possible the information\n",
    "    searched by the user. Don't give any preamble or introduction, go directly to the summary\n",
    "    of the requested information.\n",
    "    \n",
    "    If it helps to provide a more precise answer, you can also make use of the CONTEXT.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY: {query} \\n\n",
    "    SEARCH_RESULTS: {search_results} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\",\"search_results\",\"context\"],\n",
    ")\n",
    "answer_analyzer_chain = answer_analyzer_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# query = 'How much does a liter of Coca Cola cost in Brazil?'\n",
    "# search = page_content=\"Brazil - Coca-Cola - price, May 2024. The price is 0.86 USD. The average price for all countries is 1.04 USD. The database includes 90 countries. Definition: The Coca - Cola prices are for a bottle of 0.5 l. Adjustments were made to the various measuring units across countries to arrive at a uniform measure of 0.5 l.\\nBased on 90 countries included in our data base, the average price is 1.04 USD. Looking at the latest data, the lowest price was 0.22 USD (Nigeria) and the highest price was 2.60 USD (Norway). Definition: The Coca - Cola prices are for a bottle of 0.5 l.\\nSee current prices by country for prices of items we do track. You can see prices only for countries where we have decent number of contributors. Prices by Country of Coke/Pepsi (0.33 liter bottle) (Restaurants)\\nCoca-Cola FEMSA is the largest independent bottler of Coca-Cola products in the world, and the largest of several local bottling partners in Brazil. Within FEMSA's South America operating division (of which Brazil is the largest single market), FEMSA reported a 25.9%% increase in the average price per unit case for the first six months of 2022.\\nLarge corporations. There are three main soda companies in the country. Data below is provided by Afrebras. Coca-Cola Company, which has a market share of 55%% in volume and 62%% in value. AmBev, with a market share of 19%% in volume and 21%% in value. Brasil Kirin, with a market share of 5%% in volume and 4%% in value.\"\n",
    "# print(answer_analyzer_chain.invoke({\"query\": query, \"search_results\": search, \"context\": []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG node\n",
    "\n",
    "For now it's just a placeholder that searchs questions about LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG QUESTIONS\n",
    "search_rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best questions to ask our knowledge agent to get the best info for the customer.\n",
    "\n",
    "    Given the INITIAL_QUERY, work out the best questions that will find the best \\\n",
    "    info for helping to write the final answer. Write the questions to our knowledge system not to the customer.\n",
    "\n",
    "    Return a JSON with a single key 'questions' with no more than 3 strings of and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "question_rag_chain = search_rag_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# query = 'What are the main benefits of using LangSmith for developing a tool to levarage LLMs?'\n",
    "# print(question_rag_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load the data that will be used by the retriever\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Set the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Split the data and vectorize it\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Define a chain to gather data and a retriever\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG Chain\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUESTION: {question} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_rag(state):\n",
    "\n",
    "    print(\"---RAG LANGSMITH RETRIEVER---\")\n",
    "    initial_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    questions = question_rag_chain.invoke({\"initial_query\": initial_query})\n",
    "    questions = questions['questions']\n",
    "\n",
    "    rag_results = []\n",
    "    for idx, question in enumerate(questions):\n",
    "        print(f'QUESTION {idx}: {question}')\n",
    "        temp_docs = rag_chain.invoke(question)\n",
    "        print(f'ANSWER FOR QUESTION {idx}: {temp_docs}')\n",
    "        question_results = question + '\\n\\n' + temp_docs + \"\\n\\n\\n\"\n",
    "        if rag_results is not None:\n",
    "            rag_results.append(question_results)\n",
    "        else:\n",
    "            rag_results = [question_results]\n",
    "    print(f'FULL ANSWERS: {rag_results}\\n')\n",
    "    \n",
    "    processed_searches = answer_analyzer_chain.invoke({\"query\": initial_query, \"search_results\": rag_results, \"context\": context})\n",
    "    \n",
    "    return {\"context\": context + [processed_searches],\n",
    "            \"rag_questions\": questions,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web search node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search keywords\n",
    "search_keyword_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best keywords to search for in a web search to get the best info for the user.\n",
    "\n",
    "    Given the INITIAL_QUERY, work out the best keywords that will find the info requested by the user\n",
    "    The keywords should have between 3 and 5 words each, if the query allows for it.\n",
    "\n",
    "    Return a JSON with a single key 'keywords' with no more than 3 keywords and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "search_keyword_chain = search_keyword_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# query = 'Who is the current holder of the speed skating world record on 500 meters?'\n",
    "# print(search_keyword_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = secrets['tavily'][0]\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_web(state):\n",
    "\n",
    "    print(\"---RESEARCH INFO SEARCHING---\")\n",
    "    initial_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    # Web search\n",
    "    keywords = search_keyword_chain.invoke({\"initial_query\": initial_query, \"context\": context})\n",
    "    keywords = keywords['keywords']\n",
    "    full_searches = []\n",
    "    for idx, keyword in enumerate(keywords):\n",
    "        print(f'KEYWORD {idx}: {keyword}')\n",
    "        temp_docs = web_search_tool.invoke({\"query\": keyword})\n",
    "        if type(temp_docs) == list:\n",
    "            web_results = \"\\n\".join([d[\"content\"] for d in temp_docs])\n",
    "            web_results = Document(page_content=web_results)\n",
    "        elif type(temp_docs) == dict:\n",
    "            web_results = temp_docs[\"content\"]\n",
    "            web_results = Document(page_content=web_results)\n",
    "        else:\n",
    "            web_results = 'No results'\n",
    "        print(f'RESULTS FOR KEYWORD {idx}: {web_results}')\n",
    "        if full_searches is not None:\n",
    "            full_searches.append(web_results)\n",
    "        else:\n",
    "            full_searches = [web_results]\n",
    "    print(f'FULL RESULTS: {full_searches}\\n')\n",
    "    \n",
    "    processed_searches = answer_analyzer_chain.invoke({\"query\": initial_query, \"search_results\": full_searches, \"context\": context})\n",
    "    \n",
    "    print(f'PROCESSED RESULT: {processed_searches}')\n",
    "    \n",
    "    return {\"context\": context + [processed_searches],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculator node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATOR\n",
    "calculator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to do calculations using a calculator tool.\n",
    "    \n",
    "    You can only output a single format of JSON object consisting in two operands\n",
    "    and the operation. The name of the only three keys are 'operation', 'op_1' and 'op_2' \\n\n",
    "    \n",
    "    'operation' can only be [+,-,*,/,^]\n",
    "    'op_1' and 'op_2' must be integers or float\\n\n",
    "    \n",
    "    If you judge that the equation consists of more than one operation, solve only one,\n",
    "    the calculator can be called multiple times and the other results will be solved\n",
    "    later.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "calculator_chain = calculator_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "# query = 'How much is 27 to the power of 5 plus 7?'\n",
    "# print(calculator_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(state):\n",
    "\n",
    "    print(\"---CALCULATOR TOOL---\")\n",
    "    \n",
    "    query = state['next_query']\n",
    "    context = state['context']\n",
    "    parameters = calculator_chain.invoke({\"initial_query\": query})\n",
    "    operation = parameters['operation']\n",
    "    op_1 = parameters['op_1']\n",
    "    op_2 = parameters['op_2']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'OPERATION: {operation}')\n",
    "    print(f'OPERAND 1: {op_1}')\n",
    "    print(f'OPERAND 2: {op_2}')\n",
    "\n",
    "    if operation == \"+\":\n",
    "        result = op_1 + op_2\n",
    "    elif operation == \"-\":\n",
    "        result = op_1 - op_2\n",
    "    elif operation == \"/\":\n",
    "        result = op_1 / op_2\n",
    "    elif operation == \"*\":\n",
    "        result = op_1 * op_2\n",
    "    elif operation == \"^\":\n",
    "        result = op_1 ** op_2\n",
    "    else:\n",
    "        result = 'ERROR'\n",
    "        \n",
    "    if result == 'ERROR':\n",
    "        str_result = 'Unable to execute the selected operation'\n",
    "    else:\n",
    "        str_result = f'{op_1} {operation} {op_2} = {result}'\n",
    "        \n",
    "    print(f'RESULT: {str_result}\\n')\n",
    "        \n",
    "    return {\"context\": context + [str_result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context analyzer node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ready_to_answer': False, 'next_query': 'What is your approximate location within the Frankfurt area?', 'user_input': False}\n"
     ]
    }
   ],
   "source": [
    "## CONTEXT ANALYZER\n",
    "context_analyzer_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at deciding if the already available information is enough to\n",
    "    fully answer the user query. \\n\n",
    "    \n",
    "    Given a INITIAL_QUERY and the available CONTEXT, decide if the available information\n",
    "    is already enough to answer the query proposed by the user. \\n\n",
    "    \n",
    "    Your job is to coordinate the usage of many tools, one at a time. To do this you will\n",
    "    decide what information you need next, with the restriction that you can only get one\n",
    "    information per iteration, and request it to the pipeline. \\n\n",
    "    \n",
    "    Your output should be a JSON object containing three keys, 'ready_to_answer',\n",
    "    'next_query' and 'user_input'. 'ready_to_answer' is a boolean that indicates if all\n",
    "    necessary info is present, 'next_query' is a query that you should develop so the next\n",
    "    agent in the pipeline can search for the required information and 'user_input' is also\n",
    "    a boolean that indicates if the question can ONLY be answered by the user. If there is\n",
    "    any chance of finding the information without asking further questions to the user, leave\n",
    "    this field as false.\\n\n",
    "    \n",
    "    In the following situations you must output 'next_query' as \"<KEEP_QUERY>\":\n",
    "    - User asks to modify parameters or characteristics of an energy system model;\n",
    "    - Plotting, they don't require extra information, the tools can handle it perfectly;\n",
    "    - User asks you to run a new simulation on an energy modeling system;\n",
    "    - User gives you a direct command related to modelling;\n",
    "    - The user asks anything about LangSmith (understand that as having the word LangSmith) \\n\n",
    "    \n",
    "    You also have access to the last NEXT_QUERY you generated, to avoid repeating yourself.\n",
    "    Never output the same 'next_query' that you've already asked in NEXT_QUERY. \\n\n",
    "    \n",
    "    Consider that for you boolean answer the words false and true should always be written\n",
    "    in full lower case. \\n\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    NEXT_QUERY: {next_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\",\"next_query\"],\n",
    ")\n",
    "context_analyzer_chain = context_analyzer_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Is my car more powerful than a GT-R R32?'\n",
    "query = 'Can you suggest any mechanic near the Frankfurt area where I could make modifications in my honda civic?'\n",
    "# context = ['The car owned by the user is from 2010']\n",
    "print(context_analyzer_chain.invoke({\"initial_query\": query, \"context\": [], \"next_query\": ''}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_analyzer(state):\n",
    "    print(\"---CONTEXT ANALYZER---\")\n",
    "    ## Get the state\n",
    "    initial_query = state['initial_query']\n",
    "    next_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    output = context_analyzer_chain.invoke({\"initial_query\": initial_query,\n",
    "                                           \"next_query\": next_query,\n",
    "                                           \"context\": context\n",
    "                                           })\n",
    "    \n",
    "    if output['next_query'] == '<KEEP_QUERY>':\n",
    "        output['next_query'] = state['initial_query']\n",
    "    \n",
    "    if output['user_input']:\n",
    "        context = context + [output['next_query']]\n",
    "    \n",
    "    return {\"next_query\": output,\n",
    "            \"context\": context,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Getter Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def date_getter(state):\n",
    "\n",
    "    print(\"---DATE GETTER TOOL---\")\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    current_date = datetime.now().strftime(\"%d %B %Y, %H:%M:%S\")\n",
    "    \n",
    "    result = f'The current date and time are {current_date}'\n",
    "    \n",
    "    print(f'CURRENT DATE: {current_date}\\n')\n",
    "\n",
    "    return {\"context\": context + [result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at identifying the correct conversion subprocess and the correct parameter\n",
    "    selected by the user in his QUERY. \\n\n",
    "    \n",
    "    As a context, you will receive two data arrays. PARAMS provides you the name of the parameters\n",
    "    available to be selected. CONVERSION_SUBPROCESSES provides you the combination of 'cp' (conversion process name),\n",
    "    'cin' (commodity in), 'cout' (commodity out) and 'scen' (scenario) in the format 'cp@cin@cout@scen'.\\n\n",
    "    \n",
    "    Your goal is to output a JSON object containing three keys: 'param', 'value', 'cs_list'.\n",
    "    'param' must receive the name of the selected parameter;\n",
    "    'value' is the new value selected by the user;\n",
    "    'cs_list' is a list with all matching conversion subprocesses (idealy only one if possible); \\n\n",
    "    \n",
    "    NEVER MAKE UP DATA, USE ONLY DATA FROM THE GIVEN LIST. NEVER MODIFY THE SELECTED ENTRY, USE IT AS YOU\n",
    "    FOUND IT IN THE LIST! \\n\n",
    "    \n",
    "    If you can't find any match to the 'cp' name, leave the field 'cs_list' empty. If you can't find any match\n",
    "    to the 'param' name, fill the field param with 'NOT_FOUND'. \\n\n",
    "    \n",
    "    For the value required by the user, if the value is not directly stated in the QUERY, it will be\n",
    "    available in the CONTEXT, use the data found there, never try to guess the desired value. If you can't find\n",
    "    the value in the context leave the field 'value' empty. \\n\n",
    "    \n",
    "    The field 'value' only accepts numeric input, unless the input given by the user contains [], in this\n",
    "    case you should output it as a string. \\n\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY: {query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    PARAMS: {params} \\n\n",
    "    CONVERSION_SUBPROCESSES: {CSs} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\",\"context\",\"params\",\"CSs\"],\n",
    ")\n",
    "\n",
    "params_chain = params_prompt | json_model | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def param_selector(state):\n",
    "\n",
    "    print(\"---PARAM SELECTOR---\")\n",
    "    query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    tmap = pd.ExcelFile('Models/DEModel.xlsx')\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "\n",
    "    conversion_processes = np.asarray(df.iloc[:,0].dropna())\n",
    "    mask = np.where(conversion_processes != 'DEBUG')\n",
    "    conversion_processes = conversion_processes[mask]\n",
    "    parameters = np.asarray(df.columns[4:])\n",
    "\n",
    "    cs = np.asarray(df.iloc[:,0:4].dropna())\n",
    "    mask = np.where(cs[:,0] != 'DEBUG')\n",
    "    cs = cs[mask]\n",
    "    conversion_subprocesses = np.empty((len(cs),1),dtype=object)\n",
    "\n",
    "    for i in range(len(cs)):\n",
    "        conversion_subprocesses[i] = f'{cs[i,0]}@{cs[i,1]}@{cs[i,2]}@{cs[i,3]}'\n",
    "\n",
    "    output = params_chain.invoke({\"query\": query, \"context\": context, \"params\": parameters, \"CSs\": conversion_subprocesses})\n",
    "    \n",
    "    print('---CONFIRM SELECTION---')\n",
    "    \n",
    "    cs_list = output['cs_list']\n",
    "    param = output['param']\n",
    "    new_value = output['value']\n",
    "    data = []\n",
    "    for i in range(len(cs_list)):\n",
    "        elements = cs_list[i].split('@')\n",
    "        data.append([i+1,elements[0],elements[1],elements[2],elements[3]])\n",
    "        table = tabulate(data, headers=[\"Index\", \"CP\", \"CIN\", \"COUT\", \"Scen\"])\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print('No matching conversion subprocess was found.')\n",
    "        cs_confirm = 'N'\n",
    "    elif len(data) == 1:\n",
    "        print('The following matching conversion subprocess was found:\\n')\n",
    "        print(table)\n",
    "        cs_confirm = input('Is that correct? (Y or N)\\n')\n",
    "        cs_select = 0 if cs_confirm == 'Y' else 'NONE'\n",
    "    else:\n",
    "        print('The following conversion subprocesses were found:\\n')\n",
    "        print(table)\n",
    "        cs_select = int(input('Input the number of the correct CS (or 0 if it\\'s none of these):\\n')) - 1\n",
    "        cs_confirm = 'Y' if cs_select != -1 else 'N'\n",
    "    \n",
    "    if cs_confirm == 'N':\n",
    "        print('FINAL ANSWER: No matching selection.')\n",
    "        return {\"num_steps\": num_steps,\n",
    "                \"cs\": 'NO_MATCH',\n",
    "                \"selection_is_valid\": False,\n",
    "                \"parameter\": 'NO_MATCH'}\n",
    "        \n",
    "    if param in parameters:\n",
    "        param_confirm = input(f'You want to modify the parameter {param}, is that correct? (Y or N)\\n')\n",
    "    else:\n",
    "        print('No matching parameter was found.')\n",
    "        param_confirm = 'N'\n",
    "        \n",
    "    if param_confirm == 'N':\n",
    "        print('FINAL ANSWER: No matching selection.')\n",
    "        return {\"num_steps\": num_steps,\n",
    "                \"cs\": cs_list[cs_select],\n",
    "                \"selection_is_valid\": False,\n",
    "                \"parameter\": 'NO_MATCH'}\n",
    "    else:\n",
    "        print(f'FINAL ANSWER: CS: {cs_list[cs_select]}; Param: {param}')\n",
    "        return {\"num_steps\": num_steps,\n",
    "                \"cs\": cs_list[cs_select],\n",
    "                \"new_value\": new_value,\n",
    "                \"selection_is_valid\": True,\n",
    "                \"parameter\": param}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scenario_name': 'Base8760h'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at identifying the correct scenario choosen by the user\n",
    "    in his QUERY to have the simulation run. \\n\n",
    "    \n",
    "    As a context, you will receive a data array called SCENARIOS, which contains\n",
    "    all of the scenarios that are available to be simulated. \\n\n",
    "    \n",
    "    Your goal is to output a JSON object containing one key called 'scenario_name' that contains\n",
    "    the name of the scenario selected by the user. \\n\n",
    "    \n",
    "    NEVER MAKE UP DATA, USE ONLY DATA FROM THE GIVEN LIST. If you can't find any match to the asked scenario,\n",
    "    simply fill the key 'scenario_name' with 'NOT_FOUND'. \\n\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY: {query} \\n\n",
    "    SCENARIOS: {scenarios} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\",\"scenarios\"],\n",
    ")\n",
    "scenario_chain = scenario_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Run 8760h for DEModel'\n",
    "scenarios = ['Base', 'Test', 'Base4twk', 'Base8760h', 'Base8twk']\n",
    "scenario_chain.invoke({\"query\": query, \"scenarios\": scenarios})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def scenario_selector(state):\n",
    "    print('---SCENARIO SELECTOR---')\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    query = state['initial_query']\n",
    "    \n",
    "    tmap = pd.ExcelFile('Models/DEModel.xlsx')\n",
    "    df = pd.read_excel(tmap,\"Scenario\")\n",
    "    scenarios = np.asarray(df.iloc[:,0].dropna())\n",
    "    \n",
    "    output = scenario_chain.invoke({'query': query, 'scenarios': scenarios})\n",
    "    identified_scenario = output['scenario_name']\n",
    "    print(f'IDENTIFIED SCENARIO: {identified_scenario}')\n",
    "    \n",
    "    if identified_scenario == 'NOT_FOUND' or not(identified_scenario in scenarios):\n",
    "        print('No valid scenario was identified in the request, here are the available scenarios:\\n')\n",
    "        for i in range(len(scenarios)):\n",
    "            print(f'{i+1} - {scenarios[i]}')\n",
    "        selection = int(input('Select the desired scenario to be run (select 0 if none of these):\\n'))-1\n",
    "        if selection != -1:\n",
    "            identified_scenario = scenarios[selection]\n",
    "        else:\n",
    "            identified_scenario = 'NOT_FOUND'\n",
    "    \n",
    "    if identified_scenario == 'NOT_FOUND':\n",
    "        message = 'No valid scenario was found'\n",
    "        valid = False\n",
    "    else:\n",
    "        message = f'Selected scenario for simulation: {identified_scenario}'\n",
    "        valid = True\n",
    "        \n",
    "    print(message)\n",
    "    return {'num_steps': num_steps,\n",
    "            'scenario': identified_scenario,\n",
    "            'selection_is_valid': valid,\n",
    "            'final_answer': message}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plot_type': 'Sankey', 'variable': 'DEModel'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotter_id_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at identifying from the user's QUERY the correct plot type requested by\n",
    "    the user and the desired variable the user wants to plot. \\n\n",
    "    \n",
    "    As a context, you will receive two data arrays:\n",
    "    PLOT_TYPES will provide you information about the available plot types;\n",
    "    VARIABLES will provide you information about the available variables to be plotted. \\n\n",
    "    \n",
    "    Your goal is to output a JSON OBJECT containing only two keys 'plot_type' and 'variable'.\n",
    "    'plot_type' will receive the selected plot type from PLOT_TYPES, if you can't find the plot\n",
    "    requested by the user in the list, fill the key with 'NOT_FOUND';\n",
    "    'variable' will receive the selected variable from VARIABLES, if you can't find the\n",
    "    variable requested by the user fill the key with 'NOT_FOUND'. \\n\n",
    "    \n",
    "    NEVER MAKE UP DATA, USE ONLY DATA FROM THE GIVEN LISTS. \\n\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY: {query} \\n\n",
    "    PLOT_TYPES: {plot_types} \\n\n",
    "    VARIABLES: {variables} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"query\",\"plot_types\",\"variables\"],\n",
    ")\n",
    "plotter_id_chain = plotter_id_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Show me the sankey plot for DEModel with Base scenario'\n",
    "plot_types = ['Bar', 'TimeSeries', 'Sankey', 'SingleValue']\n",
    "variables = ['TOTEX','OPEX','CAPEX','total_annual_co2_emission','cap_active','cap_new','cap_res','pin (power input)','pout (power output)']\n",
    "plotter_id_chain.invoke({\"query\": query, \"plot_types\": plot_types, \"variables\": variables})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_selector(state):\n",
    "    print('---PLOT SELECTOR---')\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    query = state['initial_query']\n",
    "    \n",
    "    plot_types = ['Bar', 'TimeSeries', 'Sankey', 'SingleValue']\n",
    "    variables = ['TOTEX','OPEX','CAPEX','total_annual_co2_emission','cap_active','cap_new','cap_res','pin','pout']\n",
    "    \n",
    "    output = plotter_id_chain.invoke({\"query\": query, \"plot_types\": plot_types, \"variables\": variables})\n",
    "    identified_plot = output['plot_type']\n",
    "    identified_variable = output['variable']\n",
    "    print(f'IDENTIFIED PLOT: {identified_plot}\\nIDENTIFIED VARIABLE: {identified_variable}')\n",
    "    \n",
    "    if identified_plot == 'NOT_FOUND' or not(identified_plot in plot_types):\n",
    "        print('No valid plot type was identified in the request, here are the available plot types:\\n')\n",
    "        for i in range(len(plot_types)):\n",
    "            print(f'{i+1} - {plot_types[i]}')\n",
    "        selection = int(input('Select the desired type of plot (select 0 if none of these):\\n'))-1\n",
    "        if selection != -1:\n",
    "            identified_plot = plot_types[selection]\n",
    "        else:\n",
    "            identified_plot = 'NOT_FOUND'\n",
    "        \n",
    "    if identified_variable == 'NOT_FOUND' or not(identified_variable in variables):\n",
    "        print('No valid variable was identified in the request, here are the available variables:\\n')\n",
    "        for i in range(len(variables)):\n",
    "            print(f'{i+1} - {variables[i]}')\n",
    "        selection = int(input('Select the desired variable to be plotted (select 0 if none of these):\\n'))-1\n",
    "        if selection != -1:\n",
    "            identified_variable = plot_types[variables]\n",
    "        else:\n",
    "            identified_variable = 'NOT_FOUND'\n",
    "    \n",
    "    if identified_plot == 'NOT_FOUND' or identified_variable == 'NOT_FOUND':\n",
    "        message = 'No valid plot was identified'\n",
    "        valid = False\n",
    "    else:\n",
    "        message = f'Selected plot: {identified_plot} for {identified_variable}'\n",
    "        valid = True\n",
    "    \n",
    "    print(message)\n",
    "    \n",
    "    return {'num_steps': num_steps,\n",
    "            'plot_type': identified_plot,\n",
    "            'variable': identified_variable,\n",
    "            'selection_is_valid': valid,\n",
    "            'final_answer': message}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model modifier node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "\n",
    "def model_modifier(state):\n",
    "    print('---MODEL MODIFIER---')\n",
    "    \n",
    "    model = state['model']\n",
    "    parameter = state['parameter']\n",
    "    cs = state['cs']\n",
    "    new_value = state['new_value']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    model_file = model if '.xlsx' in model else f'{model}.xlsx'\n",
    "    workbook = load_workbook(filename=f'Models/{model_file}')\n",
    "    cs_sheet = workbook['ConversionSubProcess']\n",
    "    \n",
    "    #open workbook\n",
    "    param_idx = '0'\n",
    "    cs_idx = '0'\n",
    "    for idx, row in enumerate(cs_sheet.rows):\n",
    "        if idx == 0:\n",
    "            for i in range(len(row)):\n",
    "                if row[i].value == parameter:\n",
    "                    param_idx = row[i].coordinate\n",
    "        else:\n",
    "            if f'{row[0].value}@{row[1].value}@{row[2].value}@{row[3].value}' == cs:\n",
    "                cs_idx = row[0].coordinate\n",
    "    if param_idx == '0' or cs_idx == '0':\n",
    "        final_answer = 'Selected param or cs not found.'\n",
    "        print('Selected param or cs not found.')\n",
    "    else:\n",
    "        print(f'Cell: {param_idx[0]}{cs_idx[1:]}')\n",
    "        old_value = cs_sheet[f'{param_idx[0]}{cs_idx[1:]}'].value\n",
    "        cs_sheet[f'{param_idx[0]}{cs_idx[1:]}'].value = new_value\n",
    "        workbook.save(filename=\"Models/DEModel_modified.xlsx\")\n",
    "        final_answer = f'Value successfully modified from {old_value} to {new_value}'\n",
    "        print(final_answer)\n",
    "    return {\"num_steps\": num_steps,\n",
    "            \"final_answer\": final_answer}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sim runner node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_runner(state):\n",
    "    print('---SIMULATION RUNNER---')\n",
    "    \n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    model = state['model']\n",
    "    scenario = state['scenario']\n",
    "    \n",
    "    print(f'FINAL COMMAND: python cesm.py run {model} {scenario}\\n')\n",
    "    \n",
    "    return {\"num_steps\": num_steps,\n",
    "            \"final_answer\": 'The requested simulation was successfully submited!'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotter node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(state):\n",
    "    print('---PLOTTER---')\n",
    "    \n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    model = state['model']\n",
    "    scenario = state['scenario']\n",
    "    plot_type = state['plot_type']\n",
    "    variable = state['variable']\n",
    "    \n",
    "    print(f'FINAL COMMAND: python cesm.py plot {model} {scenario} {plot_type} {variable} \\n')\n",
    "    \n",
    "    return {\"num_steps\": num_steps,\n",
    "            \"final_answer\": 'The requested data was successfully plotted!'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output generator node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT GENERATOR\n",
    "output_generator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at answering the user based on context given. \\n\n",
    "    \n",
    "    Given the INITIAL_QUERY and a CONTEXT, generate an answer for the query\n",
    "    asked by the user. You should make use of the provided information\n",
    "    to answer the user in the best possible way. If you think the answer\n",
    "    does not answer the user completely, ask the user for the necessary\n",
    "    information if possible. \\n\n",
    "    \n",
    "    It's important never to cite that you got it from a context, the user should\n",
    "    think that you know the information.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\"],\n",
    ")\n",
    "output_generator_chain = output_generator_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# query = 'Is my car more powerful than a GT-R R32?'\n",
    "# context = 'The car owned by the user is from 2010 and has 100 hp'\n",
    "# print(output_generator_chain.invoke({\"initial_query\": query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_generator(state):\n",
    "    print(\"---GENERATE OUTPUT---\")\n",
    "    ## Get the state\n",
    "    initial_query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    answer = output_generator_chain.invoke({\"initial_query\": initial_query,\n",
    "                                            \"context\": context})\n",
    "    print(f'GENERATED OUTPUT:\\n{answer}\\n')\n",
    "    \n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_node(state):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_validator(state):\n",
    "    selection_is_valid = state['selection_is_valid']\n",
    "    selected_tool = state['selected_tool']\n",
    "    \n",
    "    if selection_is_valid:\n",
    "        return selected_tool\n",
    "    else:\n",
    "        return \"end_not_valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_printer(state):\n",
    "    \"\"\"print the state\"\"\"\n",
    "    print(\"------------------STATE PRINTER------------------\")\n",
    "    print(f\"Num Steps: {state['num_steps']} \\n\")\n",
    "    print(f\"Initial Query: {state['initial_query']} \\n\" )\n",
    "    print(f\"Next Query: {state['next_query']} \\n\" )\n",
    "    print(f\"RAG Questions: {state['rag_questions']} \\n\")\n",
    "    print(f\"Tool Parameters: {state['tool_parameters']} \\n\")\n",
    "    print(f\"Context: {state['context']} \\n\" )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def final_answer_printer(state):\n",
    "    \"\"\"prints final answer\"\"\"\n",
    "    print(\"------------------FINAL ANSWER------------------\")\n",
    "    print(f\"Final Answer: {state['final_answer']} \\n\")\n",
    "    history = state['history']\n",
    "    history.append({\"role\": \"assistant\", \"content\": state['final_answer']})\n",
    "    \n",
    "    with open(\"chat_history.pkl\", \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_type(state):\n",
    "    \"\"\"\n",
    "    Route to the right path based on query type.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    type = state['query_type']\n",
    "    \n",
    "    if type == 'general':\n",
    "        print(\"---ROUTE QUERY TO GENERAL PATH---\")\n",
    "        return \"general\"\n",
    "    elif type == 'energy_system':\n",
    "        print(\"---ROUTE QUERY TO ENERGY SYSTEM PATH---\")\n",
    "        return \"energy_system\"\n",
    "    elif type == 'mixed':\n",
    "        print(\"---ROUTE QUERY TO MIXED PATH---\")\n",
    "        return \"mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_from_mix(state):\n",
    "\n",
    "    print(\"---ROUTE TO MIX---\")\n",
    "    data_completeness = state['complete_data']\n",
    "\n",
    "    print(data_completeness)\n",
    "    if data_completeness:\n",
    "        print(\"---APPLY COMMAND---\")\n",
    "        return \"complete_data\"\n",
    "    else:\n",
    "        print(\"---GATHER MORE CONTEXT---\")\n",
    "        return \"needs_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "def validate_selected_model(state):\n",
    "    identified_model = state['identified_model']\n",
    "    available_models = next(walk('Models'), (None, None, []))[2]\n",
    "    \n",
    "    if identified_model == 'NO_MODEL' or not(f'{identified_model}.xlsx' in available_models):\n",
    "        return 'select_model'\n",
    "    else:\n",
    "        return 'model_is_valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_es_tool(state):\n",
    "    \"\"\"\n",
    "    Route to the necessary tool.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    selection = state['selected_tool']\n",
    "    \n",
    "    if selection == 'data_plotter':\n",
    "        print(\"---ROUTE QUERY TO DATA PLOTTER---\")\n",
    "        return \"data_plotter\"\n",
    "    elif selection == 'sim_runner':\n",
    "        print(\"---ROUTE QUERY TO SIMULATION RUNNER---\")\n",
    "        return \"sim_runner\"\n",
    "    elif selection == 'model_modifier':\n",
    "        print(\"---ROUTE QUERY TO MODEL MODIFIER---\")\n",
    "        return \"model_modifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_tool(state):\n",
    "    \"\"\"\n",
    "    Route to the necessary tool.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    selection = state['selected_tool']\n",
    "    \n",
    "    if selection == 'RAG_retriever':\n",
    "        print(\"---ROUTE QUERY TO RAG RETRIEVER---\")\n",
    "        return \"RAG_retriever\"\n",
    "    elif selection == 'web_search':\n",
    "        print(\"---ROUTE QUERY TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif selection == 'calculator':\n",
    "        print(\"---ROUTE QUERY TO CALCULATOR---\")\n",
    "        return \"calculator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_iterate(state):\n",
    "\n",
    "    print(\"---ROUTE TO ITERATE---\")\n",
    "    next_query = state['next_query']\n",
    "\n",
    "    print(next_query)\n",
    "    if next_query['ready_to_answer'] or next_query['user_input']:\n",
    "        print(\"---GENERATE FINAL ANSWER---\")\n",
    "        return \"ready_to_answer\"\n",
    "    else:\n",
    "        print(\"---GATHER MORE CONTEXT---\")\n",
    "        return \"need_context\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"type_identifier\", type_identifier)\n",
    "workflow.add_node(\"es_tool_selector\", es_tool_selector)\n",
    "workflow.add_node(\"model_selector\", model_selector)\n",
    "workflow.add_node(\"validated_model\", validated_model)\n",
    "workflow.add_node(\"mixed\", mixed)\n",
    "workflow.add_node(\"tool_selector\", tool_selector)\n",
    "workflow.add_node(\"research_info_rag\", research_info_rag) # RAG search\n",
    "workflow.add_node(\"research_info_web\", research_info_web) # web search\n",
    "workflow.add_node(\"state_printer\", state_printer)\n",
    "workflow.add_node(\"calculator\", calculator)\n",
    "workflow.add_node(\"date_getter\", date_getter)\n",
    "workflow.add_node(\"inter_node\", empty_node)\n",
    "workflow.add_node(\"param_selector\", param_selector)\n",
    "workflow.add_node(\"scenario_selector\", scenario_selector)\n",
    "workflow.add_node(\"plot_selector\", plot_selector)\n",
    "workflow.add_node(\"model_modifier\", model_modifier)\n",
    "workflow.add_node(\"sim_runner\", sim_runner)\n",
    "workflow.add_node(\"plotter\", plotter)\n",
    "workflow.add_node(\"output_generator\", output_generator)\n",
    "workflow.add_node(\"context_analyzer\", context_analyzer)\n",
    "workflow.add_node(\"final_answer_printer\", final_answer_printer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"date_getter\")\n",
    "workflow.add_edge(\"date_getter\", \"type_identifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"type_identifier\",\n",
    "    route_to_type,\n",
    "    {\n",
    "        \"general\": \"context_analyzer\",\n",
    "        \"energy_system\": \"es_tool_selector\",\n",
    "        \"mixed\": \"mixed\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"mixed\",\n",
    "    route_from_mix,\n",
    "    {\n",
    "        \"complete_data\": \"es_tool_selector\",\n",
    "        \"needs_data\": \"context_analyzer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"es_tool_selector\",\n",
    "    validate_selected_model,\n",
    "    {\n",
    "        \"select_model\": \"model_selector\",\n",
    "        \"model_is_valid\": \"validated_model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"validated_model\",\n",
    "    route_to_es_tool,\n",
    "    {\n",
    "        \"data_plotter\": \"scenario_selector\",\n",
    "        \"sim_runner\": \"scenario_selector\",\n",
    "        \"model_modifier\": \"param_selector\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"model_selector\",\n",
    "    route_to_es_tool,\n",
    "    {\n",
    "        \"data_plotter\": \"scenario_selector\",\n",
    "        \"sim_runner\": \"scenario_selector\",\n",
    "        \"model_modifier\": \"param_selector\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"scenario_selector\",\n",
    "    route_to_es_tool,\n",
    "    {\n",
    "        \"data_plotter\": \"plot_selector\",\n",
    "        \"sim_runner\": \"inter_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"param_selector\",\n",
    "    selection_validator,\n",
    "    {\n",
    "        \"model_modifier\": \"model_modifier\",\n",
    "        \"end_not_valid\": \"output_generator\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"plot_selector\",\n",
    "    selection_validator,\n",
    "    {\n",
    "        \"data_plotter\": \"plotter\",\n",
    "        \"end_not_valid\": \"output_generator\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"inter_node\",\n",
    "    selection_validator,\n",
    "    {\n",
    "        \"sim_runner\": \"sim_runner\",\n",
    "        \"end_not_valid\": \"output_generator\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"model_modifier\", \"output_generator\")\n",
    "workflow.add_edge(\"plotter\", \"output_generator\")\n",
    "workflow.add_edge(\"sim_runner\", \"output_generator\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"context_analyzer\",\n",
    "    route_to_iterate,\n",
    "    {\n",
    "        \"ready_to_answer\": \"output_generator\",\n",
    "        \"need_context\": \"tool_selector\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"tool_selector\",\n",
    "    route_to_tool,\n",
    "    {\n",
    "        \"RAG_retriever\": \"research_info_rag\",\n",
    "        \"web_search\": \"research_info_web\",\n",
    "        \"calculator\": \"calculator\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"research_info_rag\", \"state_printer\")\n",
    "workflow.add_edge(\"research_info_web\", \"state_printer\")\n",
    "workflow.add_edge(\"calculator\", \"state_printer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"state_printer\",\n",
    "    route_to_type,\n",
    "    {\n",
    "        \"general\": \"context_analyzer\",\n",
    "        \"mixed\": \"mixed\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"output_generator\", \"final_answer_printer\")\n",
    "workflow.add_edge(\"final_answer_printer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}]\n",
      "---DATE GETTER TOOL---\n",
      "CURRENT DATE: 09 August 2024, 10:07:12\n",
      "\n",
      "Finished running <date_getter> \n",
      "\n",
      "---TYPE IDENTIFIER---\n",
      "QUERY: [{'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}, {'role': 'assistant', 'content': 'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'}, {'role': 'user', 'content': 'what is the temperature in darmstadt?'}]\n",
      "IDENTIFIED_TYPE: general\n",
      "\n",
      "---ROUTE QUERY TO GENERAL PATH---\n",
      "Finished running <type_identifier> \n",
      "\n",
      "---CONTEXT ANALYZER---\n",
      "---ROUTE TO ITERATE---\n",
      "{'ready_to_answer': True, 'next_query': '', 'user_input': False}\n",
      "---GENERATE FINAL ANSWER---\n",
      "Finished running <context_analyzer> \n",
      "\n",
      "---GENERATE OUTPUT---\n",
      "GENERATED OUTPUT:\n",
      "As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).\n",
      "\n",
      "Finished running <output_generator> \n",
      "\n",
      "------------------FINAL ANSWER------------------\n",
      "Final Answer: As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F). \n",
      "\n",
      "Finished running <final_answer_printer> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#query = 'If I pay half the age of Tom Jobim plus the height of the Empire State for a car, how much I\\'ve paid?'\n",
    "#query = 'What is 10 to the power of 0.4?'\n",
    "#query = 'What is the temperature and humidity in Darmstadt right now? And also, what time is it?'\n",
    "#query = 'Modify the parameter X to 24 for me please'\n",
    "#query = 'What are some of the most important things that happened today in past years?'\n",
    "#query = 'What day is today?'\n",
    "#query = 'How can LangSmith help in my project?'\n",
    "#query = 'I am always coming but never arrive. What am I?'\n",
    "#query = 'Change the lifetime of wind power plants to 25 years please'\n",
    "#query = 'Divide the height of the Burj Khalifa by Ronaldinho Gaucho\\'s age, then add the current temperature in Paris (in Celsius)'\n",
    "#query = 'What are good famous and more casual board games that can be played by two players?'\n",
    "#query = 'Divide the number of visitors that the Eiffel tower receives yearly by the number of cars in the city of So Paulo, Brazil'\n",
    "#query = 'Change the technical lifetime of wind power plants to be the age of Olaf Scholz'\n",
    "#query = 'Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.'\n",
    "#query = 'Modify the investment cost power of the Biomass CHP to be the number of years michael jackson has been dead to the power of 1.5'\n",
    "\n",
    "query = 'Is my car faster than a Ferrari?'\n",
    "query = 'Of course, my car is a 1998 Honda Civic 1.6'\n",
    "#query = 'Well, what would I need to make it faster than a ferrari?'\n",
    "#query = 'What would be the risk to the engine when upgrading it to reach about 500hp?'\n",
    "query = 'Can you suggest any mechanic near the Frankfurt area where I could ask about modifications?'\n",
    "query = 'Okay, more specifically it can be in Darmstadt'\n",
    "query = 'Can you tell me their address?'\n",
    "#query = 'None of them exist in the map, where did you get this information from?'\n",
    "#query = 'Can you use the built-in web search tool to find actual mechanics?'\n",
    "# run the agent\n",
    "query = 'what is the temperature in darmstadt?'\n",
    "try:\n",
    "    with open(\"chat_history.pkl\", \"rb\") as f:\n",
    "        history = pickle.load(f)\n",
    "except:\n",
    "    history = []\n",
    "history.append({\"role\": \"user\", \"content\": query})\n",
    "print(history)\n",
    "inputs = {\"initial_query\": history, \"next_query\": '', \"num_steps\": 0, \"context\": [], \"history\": history}\n",
    "for output in app.stream(inputs, {\"recursion_limit\": 50}):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished running <{key}> \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of now, the temperature in Darmstadt is 31.5C (88.6F), feeling like 32.3C (90.1F).'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value['final_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_and_cs_list(techmap_file):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "\n",
    "    conversion_processes = np.asarray(df.iloc[:,0].dropna())\n",
    "    mask = np.where(conversion_processes != 'DEBUG')\n",
    "    conversion_processes = conversion_processes[mask]\n",
    "    parameters = np.asarray(df.columns[4:])\n",
    "    descriptions = np.asarray(df.iloc[0,4:])\n",
    "    param_n_desc = np.empty((len(parameters),1),dtype=object)\n",
    "    \n",
    "    for i in range(len(parameters)):\n",
    "        param_n_desc[i] = f'{parameters[i]} - {descriptions[i]}'\n",
    "\n",
    "    cs = np.asarray(df.iloc[:,0:3].dropna())\n",
    "    mask = np.where(cs[:,0] != 'DEBUG')\n",
    "    cs = cs[mask]\n",
    "    conversion_subprocesses = np.empty((len(cs),1),dtype=object)\n",
    "\n",
    "    for i in range(len(cs)):\n",
    "        conversion_subprocesses[i] = f'{cs[i,0]}@{cs[i,1]}@{cs[i,2]}'\n",
    "\n",
    "    return param_n_desc, conversion_subprocesses\n",
    "\n",
    "params, CSs = get_params_and_cs_list('../CESM/Data/Techmap/DEModel.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def get_scenario_params(techmap_file):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"Scenario\")\n",
    "\n",
    "    base_index = df.index[df['scenario_name'] == 'Base'].tolist()[0]\n",
    "    discount_rate = df['discount_rate'][base_index]\n",
    "    annual_co2_limit = df['annual_co2_limit'][base_index]\n",
    "    co2_price = df['co2_price'][base_index]\n",
    "    \n",
    "    if math.isnan(discount_rate):\n",
    "        discount_rate = None\n",
    "    if math.isnan(co2_price):\n",
    "        co2_price = None\n",
    "\n",
    "    return {'discount_rate': discount_rate, 'annual_co2_limit': annual_co2_limit, 'co2_price': co2_price}\n",
    "\n",
    "def get_conversion_processes(techmap_file):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"Commodity\")\n",
    "\n",
    "    cond = df['commodity_name'] != 'Dummy'\n",
    "    cond = cond & (df['commodity_name'] != 'DEBUG')\n",
    "    cond = cond & (df['commodity_name'].str.contains('Help') == False)\n",
    "\n",
    "    return df['commodity_name'][cond].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discount_rate': 0.05,\n",
       " 'annual_co2_limit': '[2016 870; 2030 540; 2050 240; 2060 140]',\n",
       " 'co2_price': None}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scenario_params('../CESM/Data/Techmap/DEModel.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap = pd.ExcelFile('../CESM/Data/Techmap/DEModel.xlsx')\n",
    "df = pd.read_excel(tmap,\"Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(df.iloc[0, df.columns == 'scenario_name'] == 'Base'):\n",
    "    discount_rate = df.iloc[0, df.columns == 'discount_rate'].values[0]\n",
    "    co2_limit = df.iloc[0, df.columns == 'annual_co2_limit'].values[0]\n",
    "    co2_price = df.iloc[0, df.columns == 'co2_price'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sheet': 'scenario'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_selector = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are a specialist at identifying the correct sheet to be modified based on the user's QUERY. \\n\n",
    "            \n",
    "            You must output a JSON object with a single key 'sheet', that should receive 'conversionsubprocess'.\n",
    "            You should only output 'scenario' instead if the user explicitly talks about co2 limit, co2 price\n",
    "            or discount rate, these terms must be included in the query. \\n\n",
    "            \n",
    "            The only exception to this rule is that if the QUERY leads to the selection to be 'scenario' but\n",
    "            SCENARIO_READY is True, then you should output 'conversionsubprocess' anyway. \\n\n",
    "            \n",
    "            Remember that for the sheet to be 'scenario' you need to receive explicitly the defined terms from\n",
    "            the QUERY. \\n\n",
    "\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            QUERY: {query} \\n\n",
    "            SCENARIO_READY: {scen_ready}\n",
    "            Answer:\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    input_variables=[\"query\", \"scen_ready\"],\n",
    ")\n",
    "\n",
    "sheet_selector_chain = sheet_selector | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'I want the energy cost of coal furnaces to be the same as biomass furnaces'\n",
    "query = 'Suppose the co2 limit is killed after 2030. Would there be stranded assets?'\n",
    "#query = 'What if we stopped using wind energy?'\n",
    "#query = 'How will the usage of renewable energy look like if we reduce the coal usage by half?'\n",
    "#query = 'what happens if we limit the CO2 to be half of todays?'\n",
    "query = 'Show me what happens if we increase the discount rate by 0.1'\n",
    "\n",
    "sheet_selector_chain.invoke({\"query\": query, \"scen_ready\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_values': [0.05, '[2016 870; 2030 540; 2031 0; 2050 0; 2060 0]', '']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_param = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are a specialist at modifying the scenario of a model based on the user's QUERY. \\n\n",
    "            \n",
    "            There are three parameters you can change: discount_rate, annual_co2_limit and co2_price.\n",
    "            In SCEN_PARAMS you have a dictionary with the current values for each of them, and you should\n",
    "            decide how to change them to fulfill the user's request. \\n\n",
    "            \n",
    "            You must output a JSON object with a single element 'new_values' that contains the modified\n",
    "            values for the three in a three elements list. If you haven't modified some of them, simply\n",
    "            output the same value you received. The order should be the same as the input.\\n\n",
    "            \n",
    "            For the anual co2 limit, if the user request a change after a specific year you should always\n",
    "            add the next year to the yearly list as a starting point for the modification, and keep the remaining\n",
    "            years, just changing their values. Also, for this list the only possibility are numbers for the \n",
    "            values, not 'infinity' or anything like that. \\n\n",
    "\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            QUERY: {query} \\n\n",
    "            SCEN_PARAMS: {scen_params}\n",
    "            Answer:\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    input_variables=[\"query\", \"scen_params\"],\n",
    ")\n",
    "\n",
    "scenario_param_chain = scenario_param | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Suppose the co2 limit is killed after 2030. Would there be stranded assets?'\n",
    "\n",
    "scen_params = {'discount_rate': 0.05, \n",
    "               'annual_co2_limit': '[2016 870; 2030 540; 2050 240; 2060 140]',\n",
    "               'co2_price': ''}\n",
    "\n",
    "scenario_param_chain.invoke({\"query\": query, \"scen_params\": scen_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parametrization_type': 'defined'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_general = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are a specialist at identifying the correct way to modify a model based on the user's QUERY. \\n\n",
    "            \n",
    "            You are part of a tool where there are two other agents ready to execute their specific tasks\n",
    "            related to model modification. Your goal is to identify which one to use depending on the user's\n",
    "            input. \\n\n",
    "            \n",
    "            The two possibilities are:\n",
    "            1. The user provides one or more specific instructions of modifications that should be done to the model,\n",
    "            with the selection of specific values for each modification;\n",
    "            2. The user asks for a specific scenario where the tool should decide which parameters and subprocesses\n",
    "            should be modified, as well as deciding the correct value to change the combinations to. \\n\n",
    "            \n",
    "            Your output must be a JSON object with a single key called 'parametrization_type', where your options are\n",
    "            'defined' for the first case and 'undefined' for the second. These are your only possibilities. \\n\n",
    "\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            QUERY: {query} \\n\n",
    "            Answer:\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "\n",
    "params_general_chain = params_general | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'I want the energy cost of coal furnaces to be the same as biomass furnaces'\n",
    "\n",
    "params_general_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param_cs_selection': [['opex_cost_power',\n",
       "   ['Biomass_CHP@Biomass@Help_Biomass_CHP',\n",
       "    'Biomass_CHP@Help_Biomass_CHP@Electricity'],\n",
       "   10],\n",
       "  ['opex_cost_power',\n",
       "   ['Coal_CHP@Coal@Help_Coal_CHP', 'Coal_CHP@Help_Coal_CHP@Electricity'],\n",
       "   10]]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_defined = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are a specialist at identifying the parameters the user wish to modify in the model, as well as the \n",
    "            conversion subprocesses and new values. \\n\n",
    "            \n",
    "            As a context, you will receive two data arrays. PARAMS provides you the name of the parameters\n",
    "            available to be selected formated as a combination of the actual parameter name and their description\n",
    "            in the format 'name - description'. CONVERSION_SUBPROCESSES provides you the combination of 'cp'\n",
    "            (conversion process name), 'cin' (commodity in), 'cout' (commodity out)  in the format 'cp@cin@cout'. \\n\n",
    "            \n",
    "            The user's QUERY may contain a request to change one or more combinations of parameter and \n",
    "            conversion subprocess. \\n\n",
    "            \n",
    "            Your goal is to output a JSON object containing a single key 'param_cs_selection', which should contain\n",
    "            a list. \\n\n",
    "            \n",
    "            The composition of the list is the following: For each combination of conversion subprocess, parameter and\n",
    "            value asked by the user there should be a list with three elements in 'params_cs_selection'. Each sub list\n",
    "            is composed of ['parameter', ['cs_match'], 'new_value']. The output structure you should\n",
    "            always follow is [[combination], [combination], [combination], ...], and this list that comprehends\n",
    "            all of the combinations of parameter, cs and value is the single element of 'params_cs_selection'.\\n\n",
    "            \n",
    "            NEVER MAKE UP DATA, USE ONLY DATA FROM THE GIVEN LISTS. NEVER MODIFY THE SELECTED ENTRY, USE IT AS YOU\n",
    "            FOUND IT IN THE LIST! THE COMBINATION 'cp@cin@cout' MUST MATCH EXACTLY WITH THE ENTRIES OF THE LIST.\n",
    "            YOU CAN NEVER GET cp, cin OR cout FROM DIFFERENT ENTRIES, THEY MUST ALWAYS COME FROM THE SAME. \\n\n",
    "            \n",
    "            For 'parameter', the value is a string and you must always output either one selected param or 'NOT_FOUND'\n",
    "            if you couldn't match any of the available ones (never output more than one per combination). \\n\n",
    "            \n",
    "            For 'cs_match' you should always output a list with the conversion suprocesses matches (in format cp@cin@cout).\n",
    "            If there is a sigle matching conversion subprocess, then the list will have a single element but will still \n",
    "            be a list. And if you can't match any conversion subprocess, then you must output an empty list. Important that \n",
    "            there is a difference between asking for a generic conversion process with more than a single conversion \n",
    "            subprocess related to it (situation in which you should match all options to the combination) and asking to\n",
    "            change a set of conversion subprocesses with the same conversion process name (situation in which each of\n",
    "            the conversion subprocesses should be given their own combinations) \\n\n",
    "            \n",
    "            For the 'new_value' you have four possibilities:\n",
    "            1. The user specified a single value for the combination. In this case you put the value as a numeric value\n",
    "            in the combination list;\n",
    "            2. The user specified a value containing [] in it. In this case you should use the exact same value as a string.\n",
    "            DON'T CHANGE ANYTHING IN THE VALUE. For example, the user inputs [2020 10; 2030 5; 2040 2], you should output\n",
    "            exactly [2020 10; 2030 5; 2040 2] as a string for the value of this combination;\n",
    "            3. There is a indirect reference to the desired value, in this case the value will probably be in the provided\n",
    "            CONTEXT, check there before going to the fourth possibility;\n",
    "            4. If there is no value to be found for a specific combination, simply output 'NOT_PROVIDED' for that one,\n",
    "            NEVER MAKE UP VALUES. 'NOT_PROVIDED' is the only possible text that you can output as a value.\\n\n",
    "\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            QUERY: {query} \\n\n",
    "            CONTEXT: {context} \\n\n",
    "            PARAMS: {params} \\n\n",
    "            CONVERSION_SUBPROCESSES: {CSs} \\n\n",
    "            Answer:\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    input_variables=[\"query\",\"context\",\"params\",\"CSs\"],\n",
    ")\n",
    "\n",
    "params_defined_chain = params_defined | json_model | JsonOutputParser()\n",
    "\n",
    "params, CSs = get_params_and_cs_list('../models/DEModel.xlsx')\n",
    "query = 'Change the maximum residual capacity for biomass power plants to [2016 10; 2030 2] and for coal furnaces to [2016 30; 2040 1]'\n",
    "#query = 'Change the maximum energy output of nuclear power plants to be the same as gas power plants'\n",
    "query = 'Modify the fixed OM cost of the biomass CHP and of coal CHP with electricity output to 10'\n",
    "#query = 'Modify the fixed OM cost of all conversion subprocesses related to coal CHP'\n",
    "\n",
    "params_defined_chain.invoke({\"query\": query, \"context\": ['The maximum Eout for PP_gas is 200'], \"params\": params, \"CSs\": CSs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param_cs_selection': [['max_eout',\n",
       "   'PP_WindOff_Res@Dummy@Electricity',\n",
       "   [0.5],\n",
       "   'multiplier'],\n",
       "  ['max_eout', 'PP_WindOff_New@Dummy@Electricity', [0.5], 'multiplier'],\n",
       "  ['max_eout', 'PP_WindOn_Res@Dummy@Electricity', [0.5], 'multiplier'],\n",
       "  ['max_eout', 'PP_WindOn_New@Dummy@Electricity', [0.5], 'multiplier']]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_undefined = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            You are a specialist at deciding the necessary modifications in the model based on the user's USER_INPUT. \\n\n",
    "            \n",
    "            As a context, you will receive two data arrays. PARAMS provides you the name of the parameters\n",
    "            available to be selected formated as a combination of the actual parameter name and their description\n",
    "            in the format 'name - description'. CONVERSION_SUBPROCESSES provides you the combination of 'cp'\n",
    "            (conversion process name), 'cin' (commodity in), 'cout' (commodity out)  in the format 'cp@cin@cout'. \\n\n",
    "            \n",
    "            Your goal is to output a JSON object containing a single key 'param_cs_selection', which should contain\n",
    "            a list. This list contains each combination of conversion subprocess, parameters, values and modification\n",
    "            type that you judge necessary to fulfill the user's scenario. \\n\n",
    "            \n",
    "            For each combination of conversion subprocess, parameter, value and modification type that you define as\n",
    "            necessary to change there should be a list with four elements in 'params_cs_selection'. Each of the combination\n",
    "            lists is composed of [['parameters'], 'cs', ['new_values'], 'mod_type']. It MUST be in this order, you can't\n",
    "            change it by any means, since it will be identified by it's order later in the pipeline. The output structure you\n",
    "            should always follow is [[combination], [combination], [combination], ...], and this list that comprehends\n",
    "            all of the combinations of parameter, cs, value and modification type is the single element of 'params_cs_selection'.\\n\n",
    "            \n",
    "            NEVER MAKE UP DATA, USE ONLY DATA FROM THE GIVEN LISTS. NEVER MODIFY THE SELECTED ENTRY, USE IT AS YOU\n",
    "            FOUND IT IN THE LIST! FOR THE CONVERSION SUBPROCESSES YOU MUST USE THE ENTIRE CS NAME, IN THE FORMAT (CP@CIN@COUT),\n",
    "            NEVER USE ONLY A PART OF IT. \\n\n",
    "            \n",
    "            You must analyze the user's scenario request and decide all conversion subprocesses (cp@cin@cout) that should be modified,\n",
    "            as well as the parameters necessary to be modified on each of the subprocesses. Each element of 'params_cs_selection'\n",
    "            should contain a list with the parameters to be modified, the selected conversion subprocess to be modified \n",
    "            and a list with all the new values. 'parameters' and 'new_values' should have the same size. \\n\n",
    "            \n",
    "            You have only two possibilities for the new values:\n",
    "            1. You want to change something to 0, in this case you output a numerical 0 in the value;\n",
    "            2. In any other case you should use percentual changes through decimal numbers, these decimals will be multiplied\n",
    "            with the current value of the parameter. In this case you should try to be as comprehensive as possible with the\n",
    "            parameter selection, since some of the parameters may be empty, leading to problems with the multiplication. In\n",
    "            other words, give as much selection options as you possibly can while trying to get the user's proposed scenario. \\n\n",
    "            \n",
    "            For the 'mod_type' you can use two different types, 'fixed' or 'multiplier'. 'fixed' tells the tool\n",
    "            that you want the value to be exactly what you specified, 'multiplier' indicates that you want the\n",
    "            current value to be multiplied by the value you defined. \\n\n",
    "            \n",
    "            Always use double quotes in the JSON object. \\n\n",
    "\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            QUERY: {query} \\n\n",
    "            CONTEXT: {context} \\n\n",
    "            PARAMS: {params} \\n\n",
    "            CONVERSION_SUBPROCESSES: {CSs} \\n\n",
    "            Answer:\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\",\n",
    "    input_variables=[\"query\",\"context\",\"params\",\"CSs\"],\n",
    ")\n",
    "\n",
    "params_undefined_chain = params_undefined | json_model | JsonOutputParser()\n",
    "\n",
    "params, CSs = get_params_and_cs_list('../CESM/Data/Techmap/DEModel.xlsx')\n",
    "query = 'What if we stopped using wind energy?'\n",
    "query = 'How will the usage of renewable energy look like if we reduce the coal usage by half?'\n",
    "query = 'What would change if I dont want wind turbines?'\n",
    "query = 'What if offshore wind becomes very cheap?'\n",
    "query = 'What if we stopped using wind energy?'\n",
    "query = 'What happens if we reduce the wind turbine power generation by half?'\n",
    "\n",
    "params_undefined_chain.invoke({\"query\": query, \"context\": [], \"params\": params, \"CSs\": CSs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sheet': 'conversionsubprocess'}\n",
      "{'parametrization_type': 'undefined'}\n",
      "{'param_cs_selection': [[['cap_max', 'cap_min'], 'PP_WindOff_Res@Dummy@Electricity', [0, 0]], [['cap_max', 'cap_min'], 'PP_WindOff_New@Dummy@Electricity', [0, 0]], [['cap_max', 'cap_min'], 'PP_WindOn_Res@Dummy@Electricity', [0, 0]], [['cap_max', 'cap_min'], 'PP_WindOn_New@Dummy@Electricity', [0, 0]]]}\n",
      "cs\n",
      "cap_max of PP_WindOff_Res@Dummy@Electricity modified from [2016 4.1;2030 4.1;2040 0] to 0\n",
      "cap_min of PP_WindOff_Res@Dummy@Electricity modified from None to 0\n",
      "cap_max of PP_WindOff_New@Dummy@Electricity modified from [2016 0;2035 355] to 0\n",
      "cap_min of PP_WindOff_New@Dummy@Electricity modified from None to 0\n",
      "cap_max of PP_WindOn_Res@Dummy@Electricity modified from [2016 45.4;2030 30;2040 0] to 0\n",
      "cap_min of PP_WindOn_Res@Dummy@Electricity modified from None to 0\n",
      "cap_max of PP_WindOn_New@Dummy@Electricity modified from [2016 0;2035 180] to 0\n",
      "cap_min of PP_WindOn_New@Dummy@Electricity modified from None to 0\n",
      "['cap_max of PP_WindOff_Res@Dummy@Electricity modified from [2016 4.1;2030 4.1;2040 0] to 0', 'cap_min of PP_WindOff_Res@Dummy@Electricity modified from None to 0', 'cap_max of PP_WindOff_New@Dummy@Electricity modified from [2016 0;2035 355] to 0', 'cap_min of PP_WindOff_New@Dummy@Electricity modified from None to 0', 'cap_max of PP_WindOn_Res@Dummy@Electricity modified from [2016 45.4;2030 30;2040 0] to 0', 'cap_min of PP_WindOn_Res@Dummy@Electricity modified from None to 0', 'cap_max of PP_WindOn_New@Dummy@Electricity modified from [2016 0;2035 180] to 0', 'cap_min of PP_WindOn_New@Dummy@Electricity modified from None to 0']\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "params, CSs = get_params_and_cs_list('../models/DEModel.xlsx')\n",
    "\n",
    "scen_params = {'discount_rate': 0.05, \n",
    "               'annual_co2_limit': '[2016 870; 2030 540; 2050 240; 2060 140]',\n",
    "               'co2_price': None}\n",
    "\n",
    "query = 'Change the maximum residual capacity for biomass power plants to [2016 10; 2030 2] and for coal furnaces to [2016 30; 2040 1]'\n",
    "#query = 'Change the maximum energy output of nuclear power plants to be the same as gas power plants'\n",
    "query = 'Modify the fixed OM cost of the biomass CHP and of coal CHP with electricity output to 10'\n",
    "#query = 'Modify the fixed OM cost of all conversion subprocesses related to coal CHP'\n",
    "#query = 'What if we stopped using wind energy?'\n",
    "#query = 'How will the usage of renewable energy look like if we reduce the coal usage by half?'\n",
    "query = 'What would change if I dont want wind turbines?'\n",
    "#query = 'What if offshore wind becomes very cheap?'\n",
    "#query = 'I want the energy cost of coal furnaces to be the same as biomass furnaces'\n",
    "#query = 'Suppose the co2 limit is killed after 2030. Would there be stranded assets?'\n",
    "\n",
    "output = sheet_selector_chain.invoke({\"query\": query, \"scen_ready\": False})\n",
    "\n",
    "print(output)\n",
    "\n",
    "if output['sheet'] == 'scenario':\n",
    "    new_params = scenario_param_chain.invoke({\"query\": query, \"scen_params\": scen_params})\n",
    "else:\n",
    "    output = params_general_chain.invoke({\"query\": query})\n",
    "    \n",
    "    print(output)\n",
    "\n",
    "    if output['parametrization_type'] == 'defined':\n",
    "        new_params = params_defined_chain.invoke({\"query\": query, \"context\": ['The maximum Eout for PP_gas is 200'], \"params\": params, \"CSs\": CSs})\n",
    "    else:\n",
    "        new_params = params_undefined_chain.invoke({\"query\": query, \"context\": [], \"params\": params, \"CSs\": CSs})\n",
    "        \n",
    "print(new_params)\n",
    "\n",
    "workbook = load_workbook(filename='../models/DEModel.xlsx')\n",
    "\n",
    "if \"new_values\" in new_params:\n",
    "    # The scenario sheet should be modified\n",
    "    print('scenario')\n",
    "    \n",
    "    scen_sheet = workbook['Scenario']\n",
    "    new_params = new_params['new_values']\n",
    "    coords = ['','','','']\n",
    "    \n",
    "    for idx, row in enumerate(scen_sheet.rows):\n",
    "        if idx == 0:\n",
    "            for i in range(len(row)):\n",
    "                if row[i].value == 'scenario_name':\n",
    "                    coords[0] = row[i].coordinate[0]\n",
    "                if row[i].value == 'discount_rate':\n",
    "                    coords[1] = row[i].coordinate[0]\n",
    "                if row[i].value == 'annual_co2_limit':\n",
    "                    coords[2] = row[i].coordinate[0]\n",
    "                if row[i].value == 'co2_price':\n",
    "                    coords[3] = row[i].coordinate[0]\n",
    "        else:\n",
    "            if scen_sheet[f'{coords[0]}{idx+1}'].value == 'Base':\n",
    "                for i in range(len(coords)):\n",
    "                    coords[i] = f'{coords[i]}{idx+1}'\n",
    "                break\n",
    "    \n",
    "    final_answer = []\n",
    "    col_names = ['discount_rate', 'annual_co2_limit', 'co2_price']\n",
    "    print(coords)\n",
    "    for i in range(1,4):\n",
    "        old_value = scen_sheet[coords[i]].value\n",
    "        if old_value != new_params[i-1]:\n",
    "            scen_sheet[coords[i]].value = new_params[i-1]\n",
    "            final_answer = final_answer + [f'{col_names[i-1]} modified from {old_value} to {new_params[i-1]}']\n",
    "            if debug:\n",
    "                print(f'{col_names[i-1]} modified from {old_value} to {new_params[i-1]}')\n",
    "            \n",
    "else:\n",
    "    # The conversion subprocess sheet should be modified\n",
    "    print('cs')\n",
    "    \n",
    "    cs_sheet = workbook['ConversionSubProcess']\n",
    "    \n",
    "    new_params = new_params['param_cs_selection']\n",
    "    params_list = []\n",
    "    \n",
    "    for i in range(len(new_params)):\n",
    "        parameter = new_params[i][0]\n",
    "        if ' - ' in parameter:\n",
    "            parameter = parameter.split(' - ')[0]\n",
    "        cs = new_params[i][1]\n",
    "        new_value = new_params[i][2]\n",
    "        \n",
    "        # Defined param format: [parameter, [cs_list], new_value]\n",
    "        if type(cs) == list and len(cs) > 1:\n",
    "            data = []\n",
    "            for j in range(len(cs)):\n",
    "                elements = cs[j].split('@')\n",
    "                data.append([j+1,elements[0],elements[1],elements[2]])\n",
    "                table = tabulate(data, headers=[\"Index\", \"CP\", \"CIN\", \"COUT\"])\n",
    "            \n",
    "            print('More than one match were found for one of the selected conversion subprocesses.')\n",
    "            print(table)\n",
    "            cs_select = int(input(f'Input the number of the correct CS (0 if none, and {len(cs)+1} if all):\\n')) - 1\n",
    "            \n",
    "            if cs_select == len(cs):\n",
    "                for j in range(len(cs)):\n",
    "                    params_list = params_list + [[parameter, cs[j], new_value]]\n",
    "            elif cs_select >= 0:\n",
    "                params_list = params_list + [[parameter, cs[cs_select], new_value]]\n",
    "        elif type(cs) == list:\n",
    "            params_list = params_list + [[parameter, cs[0], new_value]]\n",
    "            \n",
    "        # Undefined param format: [[param_list], cs, [new_value_list]]\n",
    "        if type(parameter) == list and len(parameter) > 1:\n",
    "            for i in range(len(parameter)):\n",
    "                params_list = params_list + [[parameter[i], cs, new_value[i]]]\n",
    "        elif type(parameter) == list:\n",
    "            params_list = params_list + [[parameter[0], cs, new_value[0]]]\n",
    "    \n",
    "    #open workbook\n",
    "    final_answer = []\n",
    "    for i in range(len(params_list)):\n",
    "        parameter = params_list[i][0]\n",
    "        if ' - ' in parameter:\n",
    "            parameter = parameter.split(' - ')[0]\n",
    "        cs = params_list[i][1]\n",
    "        split_cs = cs.split('@')\n",
    "        new_value = params_list[i][2]\n",
    "        \n",
    "        param_idx = '0'\n",
    "        cs_idx = '0'\n",
    "        \n",
    "        full_cs_found = False\n",
    "        cs_sub = ''\n",
    "        for idx, row in enumerate(cs_sheet.rows):\n",
    "            if idx == 0:\n",
    "                for i in range(len(row)):\n",
    "                    if row[i].value == parameter:\n",
    "                        param_idx = row[i].coordinate\n",
    "            else:\n",
    "                if f'{row[0].value}@{row[1].value}@{row[2].value}' == cs:\n",
    "                    cs_idx = row[0].coordinate\n",
    "                    full_cs_found = True\n",
    "                elif f'{row[0].value}@{split_cs[1]}@{row[2].value}' == cs and not full_cs_found:\n",
    "                    cs_idx = row[0].coordinate\n",
    "                    cs_sub = f'{row[0].value}@{split_cs[1]}@{row[2].value}'\n",
    "        if param_idx == '0' or cs_idx == '0':\n",
    "            final_answer = final_answer + [f'{parameter} of {cs} not found.']\n",
    "            if debug:\n",
    "                print(f'{parameter} for {cs} not found.')\n",
    "        else:\n",
    "            if not full_cs_found and len(cs_sub) > 0:\n",
    "                cs = cs_sub\n",
    "            old_value = cs_sheet[f'{param_idx[0]}{cs_idx[1:]}'].value\n",
    "            cs_sheet[f'{param_idx[0]}{cs_idx[1:]}'].value = new_value\n",
    "            \n",
    "            final_answer = final_answer + [f'{parameter} of {cs} modified from {old_value} to {new_value}']\n",
    "            if debug:\n",
    "                print(f'{parameter} of {cs} modified from {old_value} to {new_value}')\n",
    "                \n",
    "try:\n",
    "    workbook.save(filename=\"../models/DEModel_modified.xlsx\")\n",
    "    print(final_answer)\n",
    "except:\n",
    "    print('Failed to save the modifications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yearly_variations(techmap_file):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "\n",
    "    results = []\n",
    "    for col in df.columns[10:-2]:\n",
    "        cond = (~df[col].isna()) & (df[col].str.contains(';'))\n",
    "        values = df[col][cond].tolist()\n",
    "        CPs = df['conversion_process_name'][cond].tolist()\n",
    "        cin = df['commodity_in'][cond].tolist()\n",
    "        cout = df['commodity_out'][cond].tolist()\n",
    "        for i in range(len(values)):\n",
    "            results = results + [[f'{CPs[i]}@{cin[i]}@{cout[i]}', values[i]]]\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_cs_param_selection(techmap_file, cs_list, param_list):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "    result = []\n",
    "    \n",
    "    for cs in cs_list:\n",
    "        split_cs = cs.split('@')\n",
    "\n",
    "        cond = df['conversion_process_name'] == split_cs[0]\n",
    "        cond = cond & (df['commodity_in'] == split_cs[1])\n",
    "        cond = cond & (df['commodity_out'] == split_cs[2])\n",
    "        \n",
    "        for param in param_list:\n",
    "            try:\n",
    "                result = result + [cs, param, df[param][cond].values[0]]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return result\n",
    "\n",
    "def consult_info(query, techmap_file):\n",
    "    consult_type = query['consult_type']\n",
    "    \n",
    "    if consult_type == 'yearly_variation':\n",
    "        info = get_yearly_variations(techmap_file)\n",
    "    elif consult_type == 'cs_param_selection':\n",
    "        info = get_cs_param_selection(techmap_file, query['cs'], query['param'])\n",
    "    else:\n",
    "        info = 'Consult type not recognized'\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'consult_type': 'cs_param_selection', 'cs': ['PP_Biomass@Biomass@Electricity', 'PP_Coal@Coal@Electricity', 'PP_Gas@Gas@Electricity'], 'param': ['efficiency', 'technical_availability', 'cap_max']}\n",
      "['PP_Biomass@Biomass@Electricity', 'efficiency', 0.3, 'PP_Biomass@Biomass@Electricity', 'technical_availability', 0.86, 'PP_Biomass@Biomass@Electricity', 'cap_max', '[2016 9.9138;2017 NaN]', 'PP_Coal@Coal@Electricity', 'efficiency', 0.38, 'PP_Coal@Coal@Electricity', 'technical_availability', 0.86, 'PP_Coal@Coal@Electricity', 'cap_max', '[2016 20.4;2017 NaN]', 'PP_Gas@Gas@Electricity', 'efficiency', 0.4, 'PP_Gas@Gas@Electricity', 'technical_availability', 0.95, 'PP_Gas@Gas@Electricity', 'cap_max', '[2016 12.5;2017 NaN]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the dominant changes in 2050 compared to now are primarily driven by the transition to a more sustainable energy system. The main message is that there is a significant shift towards renewable energy sources and a decrease in fossil fuel usage.\\n\\nIn 2050, the energy system is expected to undergo significant changes, with a focus on reducing greenhouse gas emissions and meeting the annual CO2 limit of 240. This is achieved through the increased adoption of renewable energy sources, such as biomass, solar, and wind power, which replace traditional fossil fuels like coal, lignite, and natural gas.\\n\\nThe power sector is expected to undergo a significant transformation, with a shift towards more decentralized and efficient power generation. The use of combined heat and power (CHP) systems, which can generate both electricity and heat, is expected to increase.\\n\\nIn the transportation sector, there is a shift towards more sustainable propulsion methods, such as electric vehicles and hydrogen fuel cell vehicles, which replace traditional internal combustion engines.\\n\\nThe main message is that the energy system in 2050 is expected to be more sustainable, efficient, and decentralized, with a focus on reducing greenhouse gas emissions and meeting the annual CO2 limit.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consult_model_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are a specialist at identifying from the user's QUERY the correct consultation that\n",
    "        should be made to the model to return the required information to the user. \\n\n",
    "        \n",
    "        You'll receive CONVERSION_PROCESSES, CONVERSION_SUBPROCESSES, PARAMETERS and SCENARIO_INFO\n",
    "        as context, as well as MODEL_INFOS, which is a list of information about the model that was\n",
    "        already gathered. You should always focus on completing the necessary information, while\n",
    "        avoiding to get information that already exists. \\n\n",
    "        \n",
    "        CONVERSION_PROCESSES is a list with the name of all conversion processes. They are the general \n",
    "        processes of energy conversion and generation, better fragmentalized in CONVERSION_SUBPROCESSES. \\n\n",
    "        \n",
    "        CONVERSION_SUBPROCESSES is presented to you in the format \"'cp'@'cin'@'cout' - 'description'\" and\n",
    "        describes the processes of energy exchange, such as gas to heat, or gas to electricity, for each\n",
    "        of the conversion processes. \\n\n",
    "        \n",
    "        PARAMETERS is a list with all parameters that can be modeled for the conversion subprocesses along\n",
    "        with their descriptions. \\n\n",
    "        \n",
    "        SCENARIO_INFO is a dictionary with the relevant information about the scenario to consider\n",
    "        if asked by the user. It contains the discount rate of the model, the anual limits of CO2 emission\n",
    "        and the defined price for CO2. \\n\n",
    "        \n",
    "        Beyond the already available information, you can ask for two other types of information,\n",
    "        ['yearly_variation', 'cs_param_selection']. \\n\n",
    "        \n",
    "        - yearly_variation: provides you with all parameters of specific conversion subprocesses that\n",
    "        have yearly variation. The parameter is displayed as 'conversion_process'@'cin'@'cout'@'parameter', and\n",
    "        the value comes as a list with years and values, such as [2016 10; 2020 20; 2030 30] for example. This\n",
    "        would show you yearly variation of parameters for different conversion subprocesses;\n",
    "        - cs_param_selection: allows you to choose a set of conversion subprocesses and parameters\n",
    "        to get their values from the model. \\n\n",
    "        \n",
    "        Your only output should be a JSON with three keys, 'consult_type', 'cs' and 'param'. 'consult_type' must\n",
    "        be 'yearly_variation' or 'cs_param_selection, 'cs' and 'param' are optional, and should only be used\n",
    "        in the 'cs_param_selection' case. \\n\n",
    "        \n",
    "        Whenever you need to use 'cs_param_selection', you should get 'cs' from CONVERSION_SUBPROCESS and\n",
    "        'param' from PARAMETERS. You MUST NOT modify the entries, you should use them exactly as they are given\n",
    "        to you in the input lists, you are also NOT ALLOWED to guess CSs or params, use only the ones\n",
    "        available in the lists. The values in 'cs' and 'param' should always be lists, even if you want\n",
    "        to know a single value. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        QUERY: {query} \\n\n",
    "        CONVERSION_PROCESSES: {cp} \\n\n",
    "        CONVERSION_SUBPROCESSES: {cs} \\n\n",
    "        PARAMETERS: {param} \\n\n",
    "        SCENARIO_INFO: {scen_infos} \\n\n",
    "        MODEL_INFOS: {model_infos} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\",\"cp\",\"cs\",\"param\",\"scen_infos\",\"model_infos\"],\n",
    ")\n",
    "\n",
    "sum_up_info_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are an specialist at sumarizing model information based on the user's QUERY. \\n\n",
    "        \n",
    "        You'll receive CONVERSION_PROCESSES, CONVERSION_SUBPROCESSES, PARAMETERS SCENARIO_INFO\n",
    "        and CONSULTED_INFO as context. \\n\n",
    "        \n",
    "        CONVERSION_PROCESSES is a list with the name of all conversion processes. They are the general \n",
    "        processes of energy conversion and generation, better fragmentalized in CONVERSION_SUBPROCESSES. \\n\n",
    "        \n",
    "        CONVERSION_SUBPROCESSES is presented to you in the format \"'cp'@'cin'@'cout' - 'description'\" and\n",
    "        describes the processes of energy exchange, such as gas to heat, or gas to electricity, for each\n",
    "        of the conversion processes. \\n\n",
    "        \n",
    "        PARAMETERS is a list with all parameters that can be modeled for the conversion subprocesses along\n",
    "        with their descriptions. \\n\n",
    "        \n",
    "        SCENARIO_INFO is a dictionary with the relevant information about the scenario to consider\n",
    "        if asked by the user. It contains the discount rate of the model, the anual limits of CO2 emission\n",
    "        and the defined price for CO2. \\n\n",
    "        \n",
    "        CONSULTED_INFO contains possible extra information about the model that you may need to evaluate\n",
    "        the answer. \\n\n",
    "        \n",
    "        Your goal with all this context is to sumarize the information requested by the user about the\n",
    "        model based on the QUERY. Make the summarization the most natural possible, without citing the name\n",
    "        of the source inputs. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        QUERY: {query} \\n\n",
    "        CONVERSION_PROCESSES: {cp} \\n\n",
    "        CONVERSION_SUBPROCESSES: {cs} \\n\n",
    "        PARAMETERS: {param} \\n\n",
    "        SCENARIO_INFO: {scen_infos} \\n\n",
    "        CONSULTED_INFO: {info} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\",\"cp\",\"cs\",\"param\",\"scen_infos\",\"info\"],\n",
    ")\n",
    "\n",
    "consult_model_chain = consult_model_prompt | json_model | JsonOutputParser()\n",
    "sum_up_info_chain = sum_up_info_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "model = '../models/DEModel.xlsx'\n",
    "params, CSs = get_params_and_cs_list(model)\n",
    "CPs = get_conversion_processes(model)\n",
    "scen_infos = get_scenario_params(model)\n",
    "model_info = []\n",
    "\n",
    "query = 'What energy forms were modeled? And what is the CO2 limit defined for 2040?'\n",
    "query = 'what are the dominant changes in 2050 compared to now? What is the main message?'\n",
    "\n",
    "output = consult_model_chain.invoke({\"query\": query,\"cp\": CPs,\"cs\": CSs,\"param\": params,\"scen_infos\": scen_infos,\"model_infos\": model_info})\n",
    "\n",
    "print(output)\n",
    "\n",
    "info = consult_info(output, '../models/DEModel.xlsx')\n",
    "\n",
    "print(info)\n",
    "\n",
    "sum_up_info_chain.invoke({\"query\": query, \"cp\": CPs,\"cs\": CSs,\"param\": params,\"scen_infos\": scen_infos, \"info\": info})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG node\n",
    "\n",
    "Updating to use PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG QUESTIONS\n",
    "search_rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best questions to ask our knowledge agent to get the best info for the customer.\n",
    "\n",
    "    Given the INITIAL_QUERY, work out the best questions that will find the best \\\n",
    "    info for helping to write the final answer. Write the questions to our knowledge system not to the customer.\n",
    "\n",
    "    Return a JSON with a single key 'questions' with no more than 3 strings of and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "question_rag_chain = search_rag_prompt | json_model | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'What are the main benefits of using LangSmith for developing a tool to levarage LLMs?'\n",
    "# print(question_rag_chain.invoke({\"initial_query\": query}))\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load the data that will be used by the retriever\n",
    "loader = PyPDFLoader('../rag_source/energies-14-08084-v2.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "# Set the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Split the data and vectorize it\n",
    "# text_splitter = RecursiveCharacterTextSplitter()\n",
    "# documents = text_splitter.split_documents(pages)\n",
    "# vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# # Define a chain to gather data and a retriever\n",
    "# retriever = vector.as_retriever()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "#retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, embeddings, doc_embeddings):\n",
    "    vectorquery = embeddings.embed_query(query)\n",
    "    docs = doc_embeddings.similarity_search_with_score(query,k=5)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chain to gather data and a retriever\n",
    "#docs = retriever.invoke(\"How can you describe combined heat and power?\")\n",
    "\n",
    "query = \"How do we determine today's electricity consumption?\"\n",
    "docs = retrieve(query, embeddings, vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensity resulting from simulated future electricity mixes forms a key factor for determin-\n",
      "ing suitable emission reduction measures in local neighborhoods that are interlinked with\n",
      "their environment.\n"
     ]
    }
   ],
   "source": [
    "print(docs[4][0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Energies 2021 ,14, 8084 17 of 18\\nReferences\\n1. Brndlinger, T.; Knig, J.E.; Frank, O.; Grndig, D.; Jugel, C.; Kraft, P .; Krieger, O.; Mischinger, S.; Prein, P .; Seidl, H.; et al. Dena-\\nLeitstudieIntegrierte EnergiewendeImpulse fr die Gestaltung des Energiesystems bis 2050. 2018. Available online: https://\\nwww.dena.de/newsroom/publikationsdetailansicht/pub/dena-leitstudie-integrierte-energiewende-ergebnisbericht/ (accessed\\non 29 November 2021)\\n2. Gerbert, P .; Herhold, P .; Burchardt, J.; Schnberger, S.; Rechenmacher, F.; Kirchner, A.; Kemmler, A.; Wnsch, M. Klimapfade fr\\nDeutschland ; BCG, The Boston Consulting Group: Mnchen, Germany, 2018.\\n3. Ausfelder, F.; Drake, F.D.; Erlach, B.; Fischedick, M.; Henning, H.M.; Kost, C.; Mnch, W. SektorkopplungUntersuchungen und\\nUberlegungen zur Entwicklung eines Integrierten Energiesystems (Schriftenreihe Energiesysteme der Zukunft) ; DECHEMA eV: Mnchen,\\nGermany, 2017.\\n4. Morrison, R. Energy system modeling: Public transparency, scientic reproducibility, and open development. Energy Strategy\\nRev. 2018 ,20, 4963. [CrossRef]\\n5. Pfenninger, S.; DeCarolis, J.; Hirth, L.; Quoilin, S.; Staffell, I. The importance of open data and software: Is energy research\\nlagging behind? Energy Policy 2017 ,101, 211215. [CrossRef]\\n6. Pfenninger, S.; Hirth, L.; Schlecht, I.; Schmid, E.; Wiese, F.; Brown, T.; Davis, C.; Gidden, M.; Heinrichs, H.; Heuberger, C.; et al.\\nOpening the black box of energy modelling: Strategies and lessons learned. Energy Strategy Rev. 2018 ,19, 6371. [CrossRef]\\n7. Chang, M.; Thellufsen, J.Z.; Zakeri, B.; Pickering, B.; Pfenninger, S.; Lund, H.; stergaard, P .A. Trends in tools and approaches for\\nmodelling the energy transition. Appl. Energy 2021 ,290, 116731. [CrossRef]\\n8. Herbst, A.; Toro, F.; Reitze, F.; Jochem, E. Introduction to energy systems modelling. Swiss J. Econ. Stat. 2012 ,148, 111135.\\n[CrossRef]\\n9. Openmod. Open Energy Modelling Initiative. Available online: https://www.openmod-initiative.org (accessed on 29\\nNovember 2021).\\n10. Open Energy Platform. Available online: https://openenergy-platform.org (accessed on 29 November 2021).\\n11. Nikas, A.; Doukas, H.; Papandreou, A. A detailed overview and consistent classication of climate-economy models. In A\\nDetailed Overview and Consistent Classication of Climate-Economy Models , 1st ed.; Doukas, H., Flamos, A., Lieu, J., Eds.; Springer:\\nCham, Switzerland, 2019; Volume 1, pp. 154.\\n12. Fishbone, L.G.; Abilock, H. Markal, a linear-programming model for energy systems analysis: Technical description of the bnl\\nversion. Int. J. Energy Res. 1981 ,5, 353375. [CrossRef]\\n13. Loulou, R.; Goldstein, G.; Noble, K. Documentation for the MARKAL Family of Models. 2004. Available online: https:\\n//iea-etsap.org/MrklDoc-I_StdMARKAL.pdf (accessed on 29 November 2021)\\n14. der Voort, E.V . The EFOM 12C energy supply model within the EC modelling system. Omega 1982 ,10, 507523. [CrossRef]\\n15. Loulou, R.; Labriet, M. ETSAP-TIAM: The TIMES integrated assessment model Part I: Model structure. Comput. Manag. Sci. 2008 ,\\n5, 740. [CrossRef]\\n16. Howells, M.; Rogner, H.; Strachan, N.; Heaps, C.; Huntington, H.; Kypreos, S.; Hughes, A.; Silveira, S.; DeCarolis, J.; Bazillian, M.;\\net al. OSeMOSYS: The Open Source Energy Modeling System: An introduction to its ethos, structure and development. Energy\\nPolicy 2011 ,39, 58505870.\\n17. Pfenninger, S.; Pickering, B. Calliope: A multi-scale energy systems modelling framework. J. Open Source Softw. 2018 ,3, 825.\\n[CrossRef]\\n18. Hilpert, S.; Kaldemeyer, C.; Krien, U.; Gnther, S.; Wingenbach, C.; Plessmann, G. The Open Energy Modelling Framework\\n(oemof)A new approach to facilitate open science in energy system modelling. Energy Strategy Rev. 2018 ,22, 1625. [CrossRef]\\n19. PyPSA Model of the South African Energy System. Available online: https://github.com/PyPSA/pypsa-za (accessed on 29\\nNovember 2021).', metadata={'source': '../rag_source/energies-14-08084-v2.pdf', 'page': 16})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG Chain\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUESTION: {question} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_rag(state):\n",
    "\n",
    "    print(\"---RAG LANGSMITH RETRIEVER---\")\n",
    "    initial_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    questions = question_rag_chain.invoke({\"initial_query\": initial_query})\n",
    "    questions = questions['questions']\n",
    "\n",
    "    rag_results = []\n",
    "    for idx, question in enumerate(questions):\n",
    "        print(f'QUESTION {idx}: {question}')\n",
    "        temp_docs = rag_chain.invoke(question)\n",
    "        print(f'ANSWER FOR QUESTION {idx}: {temp_docs}')\n",
    "        question_results = question + '\\n\\n' + temp_docs + \"\\n\\n\\n\"\n",
    "        if rag_results is not None:\n",
    "            rag_results.append(question_results)\n",
    "        else:\n",
    "            rag_results = [question_results]\n",
    "    print(f'FULL ANSWERS: {rag_results}\\n')\n",
    "    \n",
    "    processed_searches = answer_analyzer_chain.invoke({\"query\": initial_query, \"search_results\": rag_results, \"context\": context})\n",
    "    \n",
    "    return {\"context\": context + [processed_searches],\n",
    "            \"rag_questions\": questions,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = secrets['llama-cloud'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LLAMAPARSE #####\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "#\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "#\n",
    "import joblib\n",
    "import os\n",
    "import nest_asyncio  # noqa: E402\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_parse_data():\n",
    "    data_file = \"../rag_source/parsed_data.pkl\"\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        # Load the parsed data from the file\n",
    "        parsed_data = joblib.load(data_file)\n",
    "    else:\n",
    "        # Perform the parsing step and store the result in llama_parse_documents\n",
    "        parsingInstructionUber10k = \"\"\"The provided document is a scientific paper related to energy systems\n",
    "        and energy transition. There are many tables and formulas that can be used as data sources.\n",
    "        Try to be precise while answering the questions\"\"\"\n",
    "        parser = LlamaParse(result_type=\"markdown\",\n",
    "                            parsing_instruction=parsingInstructionUber10k,\n",
    "                            max_timeout=5000,)\n",
    "        llama_parse_documents = parser.load_data(\"../rag_source/energies-14-08084-v2.pdf\")\n",
    "\n",
    "\n",
    "        # Save the parsed data to a file\n",
    "        print(\"Saving the parse results in .pkl format ..........\")\n",
    "        joblib.dump(llama_parse_documents, data_file)\n",
    "\n",
    "        # Set the parsed data to the variable\n",
    "        parsed_data = llama_parse_documents\n",
    "\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector database\n",
    "def create_vector_database():\n",
    "    \"\"\"\n",
    "    Creates a vector database using document loaders and embeddings.\n",
    "\n",
    "    This function loads urls,\n",
    "    splits the loaded documents into chunks, transforms them into embeddings using OllamaEmbeddings,\n",
    "    and finally persists the embeddings into a Chroma vector database.\n",
    "\n",
    "    \"\"\"\n",
    "    # Call the function to either load or parse the data\n",
    "    llama_parse_documents = load_or_parse_data()\n",
    "    \n",
    "    try:\n",
    "        os.remove('../rag_source/output.md')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with open('../rag_source/output.md', 'a') as f:  # Open the file in append mode ('a')\n",
    "        for doc in llama_parse_documents:\n",
    "            f.write(doc.text + '\\n')\n",
    "\n",
    "    markdown_path = \"../rag_source/output.md\"\n",
    "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "\n",
    "    #loader = DirectoryLoader('data/', glob=\"**/*.md\", show_progress=True)\n",
    "    documents = loader.load()\n",
    "    # Split loaded documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    #len(docs)\n",
    "    print(f\"length of documents loaded: {len(documents)}\")\n",
    "    print(f\"total number of document chunks generated :{len(docs)}\")\n",
    "    #docs[0]\n",
    "\n",
    "    # Initialize Embeddings\n",
    "    #embed_model = OllamaEmbeddings(model=\"llama3\")\n",
    "    embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    # Create and persist a Chroma vector database from the chunked documents\n",
    "    vs = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"chroma_db_llamaparse2\",  # Local mode with in-memory storage only\n",
    "        collection_name=\"rag\"\n",
    "    )\n",
    "\n",
    "    #query it\n",
    "    #query = \"what is the agend of Financial Statements for 2022 ?\"\n",
    "    #found_doc = qdrant.similarity_search(query, k=3)\n",
    "    #print(found_doc[0][:100])\n",
    "    #print(qdrant.get())\n",
    "\n",
    "    print('Vector DB created successfully !')\n",
    "    return vs,embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of documents loaded: 1\n",
      "total number of document chunks generated :61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db91ba26c5746b6adc804bb83a08749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB created successfully !\n"
     ]
    }
   ],
   "source": [
    "vs,embed_model = create_vector_database()\n",
    "\n",
    "chat_model = ChatGroq(temperature=0,\n",
    "                      model_name=\"llama3-70b-8192\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(embedding_function=embed_model,\n",
    "                    persist_directory=\"chroma_db_llamaparse2\",\n",
    "                    collection_name=\"rag\")\n",
    "#\n",
    "retriever=vectorstore.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke('How to determine todays energy consumption?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractions of Input Energy form Consumption, pin,y/pin,y\n",
      "\n",
      "These fractions are used to model CHPs. We consider an electricity-driven operation with electrical efficiency el and maximal overall efficiency CHP. Each CHP is defined by four different processes, joined by an auxiliary commodity cp aux. More specifically, a process transforms the fuel into cp aux, and the other three use cp aux to supply: (i) the exogenous energy form to represent the conversion losses, with pin,y equal to 1  CHP; (ii) Electricity, with pin,y/pin,y equal to el; and (iii) LTIH, here no limitation of the energy consumption fraction is necessary. The efficiencies are taken from [38] for biomass and from [35] for coal and gas CHPs.\n",
      "\n",
      "2.3.2. Model-Instance-Specific Parameters\n",
      "\n",
      "Energy Demands and Other Annual Energy Output Limitations, ep,y out/ep,yout\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "#\n",
    "prompt = set_custom_prompt()\n",
    "prompt\n",
    "\n",
    "########################### RESPONSE ###########################\n",
    "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=chat_model,\n",
    "                               chain_type=\"stuff\",\n",
    "                               retriever=retriever,\n",
    "                               return_source_documents=True,\n",
    "                               chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How to determine todays energy consumption?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(page_content='considering the different heating technologies for buildings. The model includes gas, oil, biomass, and resistive heaters as well as heat pumps and district heating. LTIH is the input commodity for District Heating.', metadata={'source': '../rag_source/output.md'}),\n",
       "  Document(page_content='considering the different heating technologies for buildings. The model includes gas, oil, biomass, and resistive heaters as well as heat pumps and district heating. LTIH is the input commodity for District Heating.', metadata={'source': '../rag_source/output.md'}),\n",
       "  Document(page_content='considering the different heating technologies for buildings. The model includes gas, oil, biomass, and resistive heaters as well as heat pumps and district heating. LTIH is the input commodity for District Heating.', metadata={'source': '../rag_source/output.md'})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke({\"query\": \"How to determine todays energy consumption?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = secrets['llama-cloud'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|| 1/1 [00:03<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "import glob\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "\n",
    "parse_documents = LlamaParse(result_type='markdown').load_data(glob.glob('../rag_source/*.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(api_key=secrets['qdrant']['api-key'], url=secrets['qdrant']['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = secrets['groq'][0]\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5878d8e9ab9847fa96871aba406f9975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c959f4205c3e47f1991d1fa88354c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name='llama_parse_qdrant_rag')\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents=parse_documents, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name='llama_parse_qdrant_rag')\n",
    "test = VectorStoreIndex.from_vector_store(vector_store, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gutriv/miniconda3/envs/llama/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "query_engine = test.as_query_engine()\n",
    "\n",
    "query = \"How can we define combined heat and power processes?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Combined heat and power (CHP) processes can be defined as multi-energy conversion processes that simultaneously supply electricity and heat. The interdependence of the output commodities and the related efficiencies are considered, where the overall efficiency of a CHP process describes the ratio between the output of both energy forms and the fuel consumption.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "\n",
    "pdf_list = [f.split('/')[-1] for f in glob.glob('../rag_source/*.pdf')]\n",
    "\n",
    "try:\n",
    "    with open('../rag_source/metadata.pickle', 'rb') as handle:\n",
    "        old_pdf_list = pickle.load(handle)\n",
    "    update_collection = False if old_pdf_list == pdf_list else True\n",
    "    print('1')\n",
    "except:\n",
    "    update_collection = True\n",
    "\n",
    "if update_collection:\n",
    "    print('2')\n",
    "    with open('../rag_source/metadata.pickle', 'wb') as handle:\n",
    "        pickle.dump(pdf_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from CESM.core.plotter import Plotter, PlotType\n",
    "from CESM.core.data_access import DAO\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_dir_path = '../CESM/Runs'\n",
    "simulation = 'DEModel-Base'\n",
    "fname_model = 'db.sqlite'\n",
    "db_path = os.path.join(runs_dir_path, simulation, fname_model)\n",
    "conn = sqlite3.connect(db_path)\n",
    "dao = DAO(conn)\n",
    "plotter = Plotter(dao)\n",
    "plot_types = [i for i in PlotType.__dict__.keys() if not i.startswith('__')]\n",
    "available_plots = {}\n",
    "for plot_type in plot_types:\n",
    "    p_type = getattr(PlotType, plot_type)\n",
    "    available_plots[plot_type] = list(p_type.__members__.keys())\n",
    "    \n",
    "commodities = [str(c) for c in dao.get_set(\"commodity\")]\n",
    "      # remove Dummy\n",
    "commodities.remove('Dummy')\n",
    "years = [int(y) for y in dao.get_set(\"year\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bar': ['ENERGY_CONSUMPTION',\n",
       "  'ENERGY_PRODUCTION',\n",
       "  'ACTIVE_CAPACITY',\n",
       "  'NEW_CAPACITY',\n",
       "  'CO2_EMISSION',\n",
       "  'PRIMARY_ENERGY'],\n",
       " 'TimeSeries': ['ENERGY_CONSUMPTION',\n",
       "  'ENERGY_PRODUCTION',\n",
       "  'POWER_CONSUMPTION',\n",
       "  'POWER_PRODUCTION'],\n",
       " 'Sankey': ['SANKEY'],\n",
       " 'SingleValue': ['CAPEX', 'OPEX', 'TOTEX']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "arrangement": "perpendicular",
         "link": {
          "source": [
           36,
           33,
           5,
           30,
           0,
           28,
           6,
           30,
           35,
           12,
           15,
           49,
           3,
           12,
           30,
           15,
           0,
           30,
           52,
           28,
           38,
           15,
           35,
           30,
           12,
           13,
           13,
           13,
           15,
           19,
           19,
           19,
           35,
           21,
           25,
           39,
           18,
           55,
           31,
           1,
           42,
           37,
           9,
           54,
           29,
           2,
           40,
           50,
           24,
           14,
           27,
           51,
           47,
           48,
           4,
           43,
           26,
           16,
           44,
           10,
           22,
           53,
           41,
           7,
           7,
           7,
           45,
           45,
           45,
           32,
           21,
           21
          ],
          "target": [
           37,
           9,
           34,
           20,
           46,
           17,
           11,
           8,
           23,
           54,
           29,
           2,
           40,
           47,
           48,
           4,
           43,
           26,
           16,
           44,
           10,
           22,
           53,
           41,
           7,
           7,
           7,
           7,
           45,
           45,
           45,
           45,
           32,
           21,
           38,
           12,
           36,
           49,
           33,
           3,
           52,
           35,
           15,
           30,
           30,
           30,
           30,
           30,
           30,
           30,
           30,
           30,
           0,
           0,
           0,
           28,
           28,
           28,
           5,
           5,
           5,
           5,
           5,
           13,
           30,
           28,
           19,
           30,
           28,
           6,
           21,
           6
          ],
          "value": [
           999844.9972871008,
           619332.492691764,
           685882.352941176,
           510999.99999999994,
           250000,
           245000,
           210000,
           11000,
           135000,
           302697.1527797165,
           30094.247088137323,
           285753.07852047356,
           95829.09090909139,
           144578.31325301164,
           40000.00000000001,
           135006.14368987913,
           38255.83650538449,
           67091.5995615868,
           69000.00000000023,
           121038.06228373642,
           68909.2916146541,
           379865.3960040653,
           161530.26427962474,
           17147.05882352941,
           138803.5909155306,
           47214.43479295903,
           41641.07727465918,
           49948.07884791239,
           74366.70590968216,
           14873.34118193643,
           31234.016482066505,
           28259.348245679226,
           583333.3333330242,
           1.1147624157104019e-7,
           68909.2916146541,
           586079.0569482582,
           999844.9972871008,
           285753.07852047356,
           619332.492691764,
           95829.09090909139,
           69000,
           879863.597612649,
           619332.492691764,
           115024.91805629217,
           12037.69883525493,
           105728.63905257548,
           31623.6,
           31984.86398663264,
           33287.999999999854,
           15993.580450709635,
           83025.535600309,
           144656.7286466164,
           120000,
           40000,
           128255.83650538503,
           38255.83650538449,
           201274.79868475962,
           48299.99999999999,
           108934.25605536274,
           42723.76080108553,
           368469.43412394344,
           148607.84313725488,
           17147.05882352941,
           138803.5909155306,
           41641.07727465919,
           49948.078847912344,
           74366.70590968216,
           31234.016482066505,
           28259.348245679226,
           209999.999999888,
           1.1147624157104019e-7,
           1.1147624157104019e-7
          ]
         },
         "node": {
          "color": [
           "rgba(255, 0, 0, 0.8)",
           "rgba(0, 255, 0, 0.8)",
           "rgba(179, 128, 51, 0.8)",
           "rgba(0, 255, 0, 0.8)",
           "rgba(57, 131, 29, 0.8)",
           "rgba(223, 130, 108, 0.8)",
           "rgba(177, 74, 237, 0.8)",
           "rgba(16, 16, 16, 0.8)",
           "rgba(117, 108, 139, 0.8)",
           "rgba(14, 84, 225, 0.8)",
           "rgba(0, 179, 0, 0.8)",
           "rgba(17, 33, 252, 0.8)",
           "rgba(0, 0, 0, 0.8)",
           "rgba(113, 142, 223, 0.8)",
           "rgba(0, 0, 255, 0.8)",
           "rgba(149, 28, 201, 0.8)",
           "rgba(118, 60, 226, 0.8)",
           "rgba(77, 91, 50, 0.8)",
           "rgba(153, 153, 153, 0.8)",
           "rgba(128, 90, 96, 0.8)",
           "rgba(112, 179, 40, 0.8)",
           "rgba(133, 25, 64, 0.8)",
           "rgba(6, 238, 138, 0.8)",
           "rgba(238, 125, 200, 0.8)",
           "rgba(242, 32, 29, 0.8)",
           "rgba(0, 179, 0, 0.8)",
           "rgba(0, 17, 255, 0.8)",
           "rgba(0, 0, 128, 0.8)",
           "rgba(255, 194, 180, 0.8)",
           "rgba(188, 60, 61, 0.8)",
           "rgba(0, 196, 154, 0.8)",
           "rgba(128, 0, 0, 0.8)",
           "rgba(153, 153, 153, 0.8)",
           "rgba(160, 0, 0, 0.8)",
           "rgba(65, 14, 161, 0.8)",
           "rgba(153, 153, 153, 0.8)",
           "rgba(153, 153, 153, 0.8)",
           "rgba(190, 230, 241, 0.8)",
           "rgba(21, 96, 100, 0.8)",
           "rgba(0, 0, 0, 0.8)",
           "rgba(233, 97, 199, 0.8)",
           "rgba(168, 241, 120, 0.8)",
           "rgba(153, 153, 102, 0.8)",
           "rgba(243, 104, 85, 0.8)",
           "rgba(224, 151, 108, 0.8)",
           "rgba(160, 0, 0, 0.8)",
           "rgba(70, 247, 243, 0.8)",
           "rgba(0, 0, 0, 0.8)",
           "rgba(92, 48, 122, 0.8)",
           "rgba(179, 128, 51, 0.8)",
           "rgba(255, 204, 0, 0.8)",
           "rgba(0, 0, 128, 0.8)",
           "rgba(153, 153, 102, 0.8)",
           "rgba(165, 33, 30, 0.8)",
           "rgba(0, 0, 0, 0.8)",
           "rgba(179, 128, 51, 0.8)"
          ],
          "label": [
           "Industrial_Heat_HT",
           "Import_Uranium",
           "PP_Lignite",
           "Uranium",
           "Furnace_Gas",
           "Decentral_Heat",
           "Propulsion_of_Vehicles",
           "Coal_CHP",
           "Demand_Rail_Traffic",
           "Integrate_Gas",
           "Decentral_Biomass_Heater",
           "Demand_Propulsion_of_Vehicles",
           "Coal",
           "Help_Coal_CHP",
           "PP_WindOff_Res",
           "Gas",
           "Central_Waste_Boiler",
           "Demand_Industrial_Heat_LT",
           "Import_Crude_Oil",
           "Help_Gas_CHP",
           "Demand_Electricity",
           "DEBUG",
           "Decentral_Gas_Heater",
           "Demand_International_Transport",
           "PP_Run_of_River",
           "Import_Biomass",
           "Central_Heat_Pump",
           "PP_WindOn_Res",
           "Industrial_Heat_LT",
           "PP_Gas",
           "Electricity",
           "Import_Natural_Gas",
           "ICV",
           "Natural_Gas",
           "Demand_Decentral_Heat",
           "Liquid_Fuel",
           "Crude_Oil",
           "Oil_Processing",
           "Biomass",
           "Import_Coal",
           "PP_Nuclear",
           "Decentral_Resistive_Heater",
           "Import_Waste",
           "Temperature_Downgrade",
           "District_Heating",
           "Gas_CHP",
           "Demand_Industrial_Heat_HT",
           "Furnace_Coal",
           "Furnace_Electric",
           "Lignite",
           "PP_PV_Res",
           "PP_WindOn_New",
           "Waste",
           "Decentral_Oil_Heater",
           "PP_Coal",
           "Import_Lignite"
          ],
          "line": {
           "color": "black",
           "width": 0.01
          },
          "thickness": 10
         },
         "type": "sankey"
        }
       ],
       "layout": {
        "font": {
         "size": 12
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2020 Sankey"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"587fb8c6-43dc-44c2-a74e-e1df80ad96bb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"587fb8c6-43dc-44c2-a74e-e1df80ad96bb\")) {                    Plotly.newPlot(                        \"587fb8c6-43dc-44c2-a74e-e1df80ad96bb\",                        [{\"arrangement\":\"perpendicular\",\"link\":{\"source\":[36,33,5,30,0,28,6,30,35,12,15,49,3,12,30,15,0,30,52,28,38,15,35,30,12,13,13,13,15,19,19,19,35,21,25,39,18,55,31,1,42,37,9,54,29,2,40,50,24,14,27,51,47,48,4,43,26,16,44,10,22,53,41,7,7,7,45,45,45,32,21,21],\"target\":[37,9,34,20,46,17,11,8,23,54,29,2,40,47,48,4,43,26,16,44,10,22,53,41,7,7,7,7,45,45,45,45,32,21,38,12,36,49,33,3,52,35,15,30,30,30,30,30,30,30,30,30,0,0,0,28,28,28,5,5,5,5,5,13,30,28,19,30,28,6,21,6],\"value\":[999844.9972871008,619332.492691764,685882.352941176,510999.99999999994,250000.0,245000.0,210000.0,11000.0,135000.0,302697.1527797165,30094.247088137323,285753.07852047356,95829.09090909139,144578.31325301164,40000.00000000001,135006.14368987913,38255.83650538449,67091.5995615868,69000.00000000023,121038.06228373642,68909.2916146541,379865.3960040653,161530.26427962474,17147.05882352941,138803.5909155306,47214.43479295903,41641.07727465918,49948.07884791239,74366.70590968216,14873.34118193643,31234.016482066505,28259.348245679226,583333.3333330242,1.1147624157104019e-7,68909.2916146541,586079.0569482582,999844.9972871008,285753.07852047356,619332.492691764,95829.09090909139,69000.0,879863.597612649,619332.492691764,115024.91805629217,12037.69883525493,105728.63905257548,31623.6,31984.86398663264,33287.999999999854,15993.580450709635,83025.535600309,144656.7286466164,120000.0,40000.0,128255.83650538503,38255.83650538449,201274.79868475962,48299.99999999999,108934.25605536274,42723.76080108553,368469.43412394344,148607.84313725488,17147.05882352941,138803.5909155306,41641.07727465919,49948.078847912344,74366.70590968216,31234.016482066505,28259.348245679226,209999.999999888,1.1147624157104019e-7,1.1147624157104019e-7]},\"node\":{\"color\":[\"rgba(255, 0, 0, 0.8)\",\"rgba(0, 255, 0, 0.8)\",\"rgba(179, 128, 51, 0.8)\",\"rgba(0, 255, 0, 0.8)\",\"rgba(57, 131, 29, 0.8)\",\"rgba(223, 130, 108, 0.8)\",\"rgba(177, 74, 237, 0.8)\",\"rgba(16, 16, 16, 0.8)\",\"rgba(117, 108, 139, 0.8)\",\"rgba(14, 84, 225, 0.8)\",\"rgba(0, 179, 0, 0.8)\",\"rgba(17, 33, 252, 0.8)\",\"rgba(0, 0, 0, 0.8)\",\"rgba(113, 142, 223, 0.8)\",\"rgba(0, 0, 255, 0.8)\",\"rgba(149, 28, 201, 0.8)\",\"rgba(118, 60, 226, 0.8)\",\"rgba(77, 91, 50, 0.8)\",\"rgba(153, 153, 153, 0.8)\",\"rgba(128, 90, 96, 0.8)\",\"rgba(112, 179, 40, 0.8)\",\"rgba(133, 25, 64, 0.8)\",\"rgba(6, 238, 138, 0.8)\",\"rgba(238, 125, 200, 0.8)\",\"rgba(242, 32, 29, 0.8)\",\"rgba(0, 179, 0, 0.8)\",\"rgba(0, 17, 255, 0.8)\",\"rgba(0, 0, 128, 0.8)\",\"rgba(255, 194, 180, 0.8)\",\"rgba(188, 60, 61, 0.8)\",\"rgba(0, 196, 154, 0.8)\",\"rgba(128, 0, 0, 0.8)\",\"rgba(153, 153, 153, 0.8)\",\"rgba(160, 0, 0, 0.8)\",\"rgba(65, 14, 161, 0.8)\",\"rgba(153, 153, 153, 0.8)\",\"rgba(153, 153, 153, 0.8)\",\"rgba(190, 230, 241, 0.8)\",\"rgba(21, 96, 100, 0.8)\",\"rgba(0, 0, 0, 0.8)\",\"rgba(233, 97, 199, 0.8)\",\"rgba(168, 241, 120, 0.8)\",\"rgba(153, 153, 102, 0.8)\",\"rgba(243, 104, 85, 0.8)\",\"rgba(224, 151, 108, 0.8)\",\"rgba(160, 0, 0, 0.8)\",\"rgba(70, 247, 243, 0.8)\",\"rgba(0, 0, 0, 0.8)\",\"rgba(92, 48, 122, 0.8)\",\"rgba(179, 128, 51, 0.8)\",\"rgba(255, 204, 0, 0.8)\",\"rgba(0, 0, 128, 0.8)\",\"rgba(153, 153, 102, 0.8)\",\"rgba(165, 33, 30, 0.8)\",\"rgba(0, 0, 0, 0.8)\",\"rgba(179, 128, 51, 0.8)\"],\"label\":[\"Industrial_Heat_HT\",\"Import_Uranium\",\"PP_Lignite\",\"Uranium\",\"Furnace_Gas\",\"Decentral_Heat\",\"Propulsion_of_Vehicles\",\"Coal_CHP\",\"Demand_Rail_Traffic\",\"Integrate_Gas\",\"Decentral_Biomass_Heater\",\"Demand_Propulsion_of_Vehicles\",\"Coal\",\"Help_Coal_CHP\",\"PP_WindOff_Res\",\"Gas\",\"Central_Waste_Boiler\",\"Demand_Industrial_Heat_LT\",\"Import_Crude_Oil\",\"Help_Gas_CHP\",\"Demand_Electricity\",\"DEBUG\",\"Decentral_Gas_Heater\",\"Demand_International_Transport\",\"PP_Run_of_River\",\"Import_Biomass\",\"Central_Heat_Pump\",\"PP_WindOn_Res\",\"Industrial_Heat_LT\",\"PP_Gas\",\"Electricity\",\"Import_Natural_Gas\",\"ICV\",\"Natural_Gas\",\"Demand_Decentral_Heat\",\"Liquid_Fuel\",\"Crude_Oil\",\"Oil_Processing\",\"Biomass\",\"Import_Coal\",\"PP_Nuclear\",\"Decentral_Resistive_Heater\",\"Import_Waste\",\"Temperature_Downgrade\",\"District_Heating\",\"Gas_CHP\",\"Demand_Industrial_Heat_HT\",\"Furnace_Coal\",\"Furnace_Electric\",\"Lignite\",\"PP_PV_Res\",\"PP_WindOn_New\",\"Waste\",\"Decentral_Oil_Heater\",\"PP_Coal\",\"Import_Lignite\"],\"line\":{\"color\":\"black\",\"width\":0.01},\"thickness\":10},\"type\":\"sankey\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"2020 Sankey\"},\"font\":{\"size\":12}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('587fb8c6-43dc-44c2-a74e-e1df80ad96bb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plotter.plot_sankey(years[1])\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plots': [['Bar', 'ACTIVE_CAPACITY', 'Coal', ''], ['Bar', 'ACTIVE_CAPACITY', 'Biomass', '']]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#000000"
         },
         "name": "Import_Coal",
         "type": "bar",
         "x": [
          2015,
          2020,
          2025,
          2030,
          2035,
          2040,
          2045,
          2050,
          2055,
          2060
         ],
         "y": [
          93.17279471165122,
          95.08594005766591,
          117.46129623003253,
          137.52549824982214,
          137.52549824982214,
          137.52549824982214,
          137.52549824982214,
          137.52549824982214,
          137.52549824982214,
          137.52549824982214
         ]
        }
       ],
       "layout": {
        "barmode": "stack",
        "font": {
         "family": "Knuth's Computer Modern"
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Active Capacity for Coal"
        },
        "xaxis": {
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "title": {
          "text": "Power [GW]"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"190674c6-d064-4c90-a777-394d78b5100b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"190674c6-d064-4c90-a777-394d78b5100b\")) {                    Plotly.newPlot(                        \"190674c6-d064-4c90-a777-394d78b5100b\",                        [{\"marker\":{\"color\":\"#000000\"},\"name\":\"Import_Coal\",\"x\":[2015,2020,2025,2030,2035,2040,2045,2050,2055,2060],\"y\":[93.17279471165122,95.08594005766591,117.46129623003253,137.52549824982214,137.52549824982214,137.52549824982214,137.52549824982214,137.52549824982214,137.52549824982214,137.52549824982214],\"type\":\"bar\"}],                        {\"barmode\":\"stack\",\"font\":{\"family\":\"Knuth's Computer Modern\"},\"showlegend\":true,\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Active Capacity for Coal\"},\"xaxis\":{\"title\":{\"text\":\"Year\"}},\"yaxis\":{\"title\":{\"text\":\"Power [GW]\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('190674c6-d064-4c90-a777-394d78b5100b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#00b300"
         },
         "name": "Import_Biomass",
         "type": "bar",
         "x": [
          2015,
          2020,
          2025,
          2030,
          2035,
          2040,
          2045,
          2050,
          2055,
          2060
         ],
         "y": [
          17.728086673042323,
          21.03918151333257,
          21.03918151333257,
          31.53766878698277,
          31.53766878698277,
          31.53766878698277,
          31.53766878698277,
          31.53766878698277,
          42.226474176769514,
          82.61930095593613
         ]
        }
       ],
       "layout": {
        "barmode": "stack",
        "font": {
         "family": "Knuth's Computer Modern"
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Active Capacity for Biomass"
        },
        "xaxis": {
         "title": {
          "text": "Year"
         }
        },
        "yaxis": {
         "title": {
          "text": "Power [GW]"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"4cc98f38-9812-43b9-85a7-b7a350f16eae\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4cc98f38-9812-43b9-85a7-b7a350f16eae\")) {                    Plotly.newPlot(                        \"4cc98f38-9812-43b9-85a7-b7a350f16eae\",                        [{\"marker\":{\"color\":\"#00b300\"},\"name\":\"Import_Biomass\",\"x\":[2015,2020,2025,2030,2035,2040,2045,2050,2055,2060],\"y\":[17.728086673042323,21.03918151333257,21.03918151333257,31.53766878698277,31.53766878698277,31.53766878698277,31.53766878698277,31.53766878698277,42.226474176769514,82.61930095593613],\"type\":\"bar\"}],                        {\"barmode\":\"stack\",\"font\":{\"family\":\"Knuth's Computer Modern\"},\"showlegend\":true,\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Active Capacity for Biomass\"},\"xaxis\":{\"title\":{\"text\":\"Year\"}},\"yaxis\":{\"title\":{\"text\":\"Power [GW]\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4cc98f38-9812-43b9-85a7-b7a350f16eae');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from CESM.core.plotter import Plotter, PlotType\n",
    "from CESM.core.data_access import DAO\n",
    "import sqlite3\n",
    "\n",
    "runs_dir_path = '../CESM/Runs'\n",
    "simulation = 'DEModel-Base'\n",
    "fname_model = 'db.sqlite'\n",
    "\n",
    "plot_results_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are an specialist at deciding the correct type of plot to show to the user\n",
    "        based on simulation results that we have access. \\n\n",
    "        \n",
    "        Based on the USER_INPUT you should either identify the plot requested by the user\n",
    "        directly, or try to guess the correct ones if the USER_INPUT is a bit more generic\n",
    "        and don't specify any plot. \\n\n",
    "        \n",
    "        To help you, you have access to a list called AVAILABLE_PLOTS where you have all\n",
    "        plotting possibilities. Each of the sublists is a type of plot that you can request,\n",
    "        some of them have elements show as REQUIRED, that means that they need extra information\n",
    "        to the plot to work. The format of the sublists is ['plot_type', 'plot_subtype', 'commodity', 'year']\n",
    "        and you can find the possibilities for commodity and year in COMMODITIES AND YEARS. \n",
    "        They MUST be filled in the plots that have them as REQUIRED. If it's not required you\n",
    "        can keep the values empty. \\n\n",
    "        \n",
    "        You must output a JSON object containing a single key 'plots'. The value of this key\n",
    "        will be a list of lists, where each sublist consists of a plot selection. The format of\n",
    "        the sublists is ['plot_type', 'plot_subtype', 'commodity', 'year'], it must stay in this\n",
    "        exact order. \\n\n",
    "        \n",
    "        All commodities and years that you select MUST be in COMMODITIES and YEARS, you are not\n",
    "        allowed to guess any of them, since the tool cannot plot data for inexistent commodities\n",
    "        or years. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT: {user_input} \\n\n",
    "        AVAILABLE_PLOTS: {available_plots} \\n\n",
    "        COMMODITIES: {commodities} \\n\n",
    "        YEARS: {years} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\",\"available_plots\",\"commodities\",\"years\"],\n",
    ")\n",
    "\n",
    "plot_results_chain = plot_results_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "db_path = os.path.join(runs_dir_path, simulation, fname_model)\n",
    "conn = sqlite3.connect(db_path)\n",
    "dao = DAO(conn)\n",
    "plotter = Plotter(dao)\n",
    "\n",
    "available_plots = [['Bar', 'ENERGY_CONSUMPTION', 'REQUIRED', ''],\n",
    "  ['Bar', 'ENERGY_PRODUCTION', 'REQUIRED', ''],\n",
    "  ['Bar', 'ACTIVE_CAPACITY', 'REQUIRED', ''],\n",
    "  ['Bar', 'NEW_CAPACITY', 'REQUIRED', ''],\n",
    "  ['Bar', 'CO2_EMISSION', '', ''],\n",
    "  ['Bar', 'PRIMARY_ENERGY', '', ''],\n",
    "  ['TimeSeries', 'ENERGY_CONSUMPTION', 'REQUIRED', 'REQUIRED'],\n",
    "  ['TimeSeries','ENERGY_PRODUCTION', 'REQUIRED', 'REQUIRED'],\n",
    "  ['TimeSeries','POWER_CONSUMPTION', 'REQUIRED', 'REQUIRED'],\n",
    "  ['TimeSeries','POWER_PRODUCTION', 'REQUIRED', 'REQUIRED'],\n",
    "  ['Sankey', 'SANKEY', '', 'REQUIRED'],\n",
    "  ['SingleValue', 'CAPEX', '', ''],\n",
    "  ['SingleValue', 'OPEX', '', ''],\n",
    "  ['SingleValue', 'TOTEX', '', '']]\n",
    "    \n",
    "commodities = [str(c) for c in dao.get_set(\"commodity\")]\n",
    "commodities.remove('Dummy')\n",
    "\n",
    "years = [int(y) for y in dao.get_set(\"year\")]\n",
    "\n",
    "query = 'Show me all the sankey plot of the model until 2040'\n",
    "query = 'What happens to the model if we remove all the renewable sources in 2030?'\n",
    "query = 'Show me the active capacity of coal and biomass'\n",
    "\n",
    "output = plot_results_chain.invoke({\"user_input\": query,\n",
    "                                     \"available_plots\": available_plots,\n",
    "                                     \"commodities\": commodities,\n",
    "                                     \"years\": years})\n",
    "\n",
    "print(output)\n",
    "\n",
    "selected_plots = output['plots']\n",
    "\n",
    "for plot in selected_plots:\n",
    "    plot_type = plot[0]\n",
    "    plot_subtype = plot[1]\n",
    "    commodity = plot[2]\n",
    "    year = plot[3]\n",
    "    \n",
    "    p_type = getattr(PlotType, plot_type)\n",
    "    \n",
    "    if plot_type == 'Bar':\n",
    "        # Combination\n",
    "        if plot_subtype in  ['PRIMARY_ENERGY', 'CO2_EMISSION']:\n",
    "            plotter.plot_bars(getattr(p_type, plot_subtype))\n",
    "        else:\n",
    "            plotter.plot_bars(getattr(p_type, plot_subtype), commodity=commodity)\n",
    "\n",
    "    elif plot_type == 'TimeSeries':\n",
    "        plotter.plot_timeseries(getattr(p_type, plot_subtype), year=year, commodity=commodity)\n",
    "    \n",
    "    elif plot_type == 'Sankey':\n",
    "        f = plotter.plot_sankey(year)\n",
    "        f.show()\n",
    "\n",
    "    elif plot_type == 'SingleValue':\n",
    "        plotter.plot_single_value([getattr(p_type, plot_subtype)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from CESM.core.data_access import DAO\n",
    "import sqlite3\n",
    "\n",
    "runs_dir_path = '../CESM/Runs'\n",
    "simulation_base = 'DEModel-Base'\n",
    "simulation_new = 'DEModelMock-Base'\n",
    "fname_model = 'db.sqlite'\n",
    "\n",
    "db_path_base = os.path.join(runs_dir_path, simulation_base, fname_model)\n",
    "db_path_new = os.path.join(runs_dir_path, simulation_new, fname_model)\n",
    "conn_base = sqlite3.connect(db_path_base)\n",
    "conn_new = sqlite3.connect(db_path_new)\n",
    "dao_base = DAO(conn_base)\n",
    "dao_new = DAO(conn_new)\n",
    "\n",
    "non_t_variables = [\"Cap_new\",\n",
    "                   \"Cap_active\",\n",
    "                   \"Cap_res\",\n",
    "                   \"Eouttot\",\n",
    "                   \"Eintot\",\n",
    "                   \"E_storage_level_max\"]\n",
    "\n",
    "single_values = [\"OPEX\",\n",
    "                 \"CAPEX\",\n",
    "                 \"TOTEX\"]\n",
    "\n",
    "for idx, variable in enumerate(non_t_variables):\n",
    "    df_value_base = dao_base.get_as_dataframe(variable)\n",
    "    df_value_new = dao_new.get_as_dataframe(variable)\n",
    "    if idx == 0:\n",
    "        df_base = df_value_base.rename(columns={'value': variable})\n",
    "        df_new = df_value_new.rename(columns={'value': variable})\n",
    "    else:\n",
    "        df_base[variable] = df_value_base['value']\n",
    "        df_new[variable] = df_value_new['value']\n",
    "\n",
    "for idx, variable in enumerate(single_values):\n",
    "    df_value_base = dao_base.get_as_dataframe(variable)\n",
    "    df_value_new = dao_new.get_as_dataframe(variable)\n",
    "    if idx == 0:\n",
    "        df_single_base = df_value_base.rename(columns={'value': variable})\n",
    "        df_single_new = df_value_new.rename(columns={'value': variable})\n",
    "    else:\n",
    "        df_single_base[variable] = df_value_base['value']\n",
    "        df_single_new[variable] = df_value_new['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_single = (df_single_new / df_single_base - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "variations = ((df_new.iloc[:,4:] / df_base.iloc[:,4:] - 1) * 100).replace([np.inf, np.nan], 0).apply(np.int64)\n",
    "df_variations = df_new.copy()\n",
    "df_variations[non_t_variables] = variations[non_t_variables]\n",
    "df_variations['cs'] = df_variations[['cp','cin','cout']].agg('@'.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.unique(df_variations['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_dict = {var: {year: [] for year in years} for var in non_t_variables}\n",
    "\n",
    "for variable in non_t_variables:\n",
    "    for year in years:\n",
    "        for index, row in df_variations.loc[(df_variations['Year']==year) & (df_variations[variable]!=0)].iterrows():\n",
    "            if row['cs'] == 'DEBUG@Dummy@DEBUG':\n",
    "                continue\n",
    "            entry = f'{row[\"cs\"]} = {min(row[variable], 500)}'\n",
    "            variations_dict[variable][year].append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are an specialist at analyzing the variations in the model's output variables to summarize\n",
    "        the most relevant information given the USER_INPUT and the OUTPUT_VARIATIONS. \\n\n",
    "        \n",
    "        OUTPUT_VARIATIONS is a dictionary that contains the percentual variation of the output variables\n",
    "        of a model after the user modified it and ran it again. The first layer is has the output variables\n",
    "        as the key, the second has each year of the simulation as a key, and finally, the values of these\n",
    "        are the the variations for each subprocess (represented by a combination of \n",
    "        'conversion_process'@'commodity_in'@'commodity_out'). \\n\n",
    "        \n",
    "        You must consider that the model was modified to account for the user's modification request shown\n",
    "        in USER_INPUT, then the modified model was simulated and the variation you have available in\n",
    "        OUTPUT_VARIATIONS is the variation between the original model and the model with the modifications\n",
    "        requested by the user. Using this context you should be able to give the user the main insights\n",
    "        about the scenario that he requested. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT: {user_input} \\n\n",
    "        OUTPUT_VARIATIONS: {output_variations} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\",\"output_variations\"],\n",
    ")\n",
    "\n",
    "compare_results_chain = compare_results_prompt | test_model | StrOutputParser()\n",
    "\n",
    "user_input = \"What would happen if we progressivelly reduced the CO2 limits of the model?\"\n",
    "\n",
    "output = compare_results_chain.invoke({\"user_input\": user_input, \"output_variations\": variations_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parametrization_type': 'undefined'}\n",
      "{'cs_selection': ['PP_WindOn_Res@Dummy@Electricity', 'PP_WindOn_New@Dummy@Electricity']}\n",
      "{'PP_WindOn_Res@Dummy@Electricity': ['cap_res_max - Max residual cap', 'cap_res_min - Min residual cap', 'cap_max - Max active capacity', 'opex_cost_power - Fixed OM cost'], 'PP_WindOn_New@Dummy@Electricity': ['cap_res_max - Max residual cap', 'cap_max - Max active capacity', 'technical_lifetime - Techninal Lifetime', 'opex_cost_power - Fixed OM cost', 'capex_cost_power - Investment Cost  power']}\n",
      "{'param_selection': {'PP_WindOn_Res@Dummy@Electricity': ['cap_res_max'], 'PP_WindOn_New@Dummy@Electricity': []}}\n",
      "{'success': True, 'values': {'PP_WindOn_Res@Dummy@Electricity': [['cap_res_max', '[2016 22.7;2030 15;2040 0]', 'GW']], 'PP_WindOn_New@Dummy@Electricity': []}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_params_and_cs_list(techmap_file):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "    \n",
    "    # Put the column names (parameters) and the first row (descriptions) together in an array\n",
    "    parameters = df.columns[10:-2].to_list()\n",
    "    descriptions = df.iloc[0,10:-2].to_list()\n",
    "    param_n_desc = []\n",
    "    for i in range(len(parameters)):\n",
    "        param_n_desc.append(f'{parameters[i]} - {descriptions[i]}')\n",
    "        \n",
    "    # Filter the conversion processes and generate a new column with the full name 'cp@cin@cout'\n",
    "    df = df.loc[(df['conversion_process_name'] != 'DEBUG') & (df['conversion_process_name'].notnull())]\n",
    "    df = df.iloc[:,0:3]\n",
    "    df['cs'] = df[['conversion_process_name', 'commodity_in', 'commodity_out']].agg('@'.join, axis=1)\n",
    "    \n",
    "    return param_n_desc, df['cs'].to_list()\n",
    "\n",
    "def get_populated_params_and_cs_list(techmap_file, cs_selection):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "\n",
    "    new_row = [col if type(df.iloc[0][col]) != str else f\"{col} - {df.iloc[0][col]}\" for col in df.columns]\n",
    "    df.columns = new_row\n",
    "\n",
    "    df = df.loc[(df['conversion_process_name'] != 'DEBUG') & (df['conversion_process_name'].notnull())]\n",
    "    df['cs'] = df[['conversion_process_name', 'commodity_in', 'commodity_out']].agg('@'.join, axis=1)\n",
    "\n",
    "    populated_params = {cs: None for cs in cs_selection}\n",
    "    for i in range(len(cs_selection)):\n",
    "        df_filtered = df.loc[df['cs'] == cs_selection[i]]\n",
    "        populated_params[cs_selection[i]] = df_filtered.iloc[:, 10:-3].dropna(axis=1).columns.to_list()\n",
    "\n",
    "    return populated_params\n",
    "\n",
    "def get_values(techmap_file, selection_dict):\n",
    "    tmap = pd.ExcelFile(techmap_file)\n",
    "    df = pd.read_excel(tmap,\"ConversionSubProcess\")\n",
    "    units = df.iloc[1,:]\n",
    "    df = df.loc[(df['conversion_process_name'] != 'DEBUG') & (df['conversion_process_name'].notnull())]\n",
    "    df['cs'] = df[['conversion_process_name', 'commodity_in', 'commodity_out']].agg('@'.join, axis=1)\n",
    "    result = {}\n",
    "    \n",
    "    for key, value in selection_dict.items():\n",
    "        df_filtered = df.loc[df['cs'] == key]\n",
    "        result[key] = []\n",
    "        for param in value:\n",
    "            result[key].append([param, df_filtered[param].values[0], units[param]])\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_params_general_prompt_template() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are a specialist at identifying the correct way to modify a model based on the user's USER_INPUT. \\n\n",
    "        \n",
    "        You are part of a tool where there are two other agents ready to execute their specific tasks\n",
    "        related to model modification. Your goal is to identify which one to use depending on the user's\n",
    "        input. \\n\n",
    "        \n",
    "        The two possibilities are:\n",
    "        1. The user provides one or more specific instructions of modifications that should be done to the model,\n",
    "        with the selection of specific values for each modification;\n",
    "        2. The user asks for a specific scenario where the tool should decide which parameters and subprocesses\n",
    "        should be modified, as well as deciding the correct value to change the combinations to. \\n\n",
    "        \n",
    "        Your output must be a JSON object with a single key called 'parametrization_type', where your options are\n",
    "        'defined' for the first case and 'undefined' for the second. These are your only possibilities. \\n\n",
    "        \n",
    "        Always use double quotes in the JSON object. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT: {user_input} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\"],\n",
    "    )\n",
    "\n",
    "def get_select_cs_prompt_template() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are a specialist at analizing the USER_INPUT and determining the correct conversion\n",
    "        suprocesses that should be modified in a energy system to achieve the user request. \\n\n",
    "        \n",
    "        Your selection should come from the CONVERSION_SUBPROCESSES list provided, you are not\n",
    "        allowed to guess names of conversion subprocesses or modify the entries of the list before\n",
    "        returning them. You MUST use them as they are in the provided list. \\n\n",
    "        \n",
    "        There are two possibilities of ways for you to choose the correct subprocesses:\n",
    "        1. The user specified what he wants to modify in the model, in this case you should try\n",
    "        to match what was requested and nothing else;\n",
    "        2. The user requested a general scenario in which you will be responsible to decide\n",
    "        which subprocesses should be modified. \\n\n",
    "        \n",
    "        Your output must be a JSON object with a single key 'cs_selection' that will contain a\n",
    "        single list with all entries that you selected from CONVERSION_SUBPROCESSES. \\n\n",
    "        \n",
    "        Always use double quotes in the JSON object. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT: {user_input} \\n\n",
    "        CONVERSION_SUBPROCESSES: {cs_list} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\",\"cs_list\"],\n",
    "    )\n",
    "    \n",
    "def get_select_params_prompt_template() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are a specialist at analizing the USER_INPUT and determining the correct parameters\n",
    "        that should be modified in a energy system to achieve the user request. \\n\n",
    "        \n",
    "        Your selection should come from the PARAMETERS dictionary provided, you are not\n",
    "        allowed to guess names of parameters or modify the entries of the list before\n",
    "        returning them. You MUST use them as they are in the provided list. \\n\n",
    "        \n",
    "        The PARAMETERS dictionary shows for each conversion subprocess the available parameters,\n",
    "        you should select from among the ones available for that specific subprocess. The format\n",
    "        in which each parameter is shown is 'param_name - description', you must output only the\n",
    "        param_name, never the description. \\n\n",
    "        \n",
    "        There are two possibilities of ways for you to choose the correct parameters:\n",
    "        1. The user specified what he wants to modify in the model, in this case you should try\n",
    "        to match what was requested and nothing else;\n",
    "        2. The user requested a general scenario in which you will be responsible to decide\n",
    "        which parameters should be modified. \\n\n",
    "        \n",
    "        Your output must be a JSON object with a single key 'param_selection' that will contain a\n",
    "        single dictionary where the keys are each of the conversion subprocesses and the values\n",
    "        are the list of selected parameters for each of them. \\n\n",
    "        \n",
    "        Always use double quotes in the JSON object. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT: {user_input} \\n\n",
    "        PARAMETERS: {param_list} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\",\"param_list\"],\n",
    "    )\n",
    "\n",
    "def get_new_values_prompt_template() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are a specialist at defining new values for model parameters based on the USER_INPUT. \\n\n",
    "        \n",
    "        You have access to all values to be modified in CURRENT_VALUES, they are organized as a\n",
    "        dictionary, where each key represents a conversion subprocess and the value of each key\n",
    "        is a list of lists containing all of the current values. Each sublist of this list is\n",
    "        organized as follows: [param_name, current_value, unit]. \\n\n",
    "        \n",
    "        Your job is to understand the USER_INPUT to modify the 'current_value' in this structure.\n",
    "        You will output a JSON object that contains two keys, 'success' and 'values', where 'success'\n",
    "        should receive a boolean (always lower case) and 'values' should receive the same structure\n",
    "        as CURRENT_VALUES but with 'current_value' updated for the new value in all entries. \\n\n",
    "        \n",
    "        You have some options of how to update the value based on USER_INPUT:\n",
    "        1. The user specified a value, in this case you need to use the exact values that were asked\n",
    "        by the user;\n",
    "        2. The user didn't specify a value but gave some kind of idea of the modification, for example\n",
    "        asked you to double the value, to reduce by 30%, to remove (in this case the value should go to 0),\n",
    "        in this case you need to figure out by how much the value will be modified;\n",
    "        3. The user didn't specify a value but asked for some indirect value related to other piece of\n",
    "        information, in this case you can check CONTEXT for the necessary information;\n",
    "        4. In case you couldn't categorize the request as any of the past 3 cases, you should mark\n",
    "        'success' as false and 'values' as an empty dict. \\n\n",
    "        \n",
    "        There are two types of values that you can receive, numbers or lists. If the value is a number\n",
    "        it's simple, the number represents the value of that parameter, however, if you receive a list\n",
    "        (which will be in form of a string and should also be outputed as a string) it represents the\n",
    "        variation of the parameter by year, for example '[2015 10; 2030 20; 2050 40]'. You may modify\n",
    "        the available years and the values, as long as you keep the format and the output as string. \\n\n",
    "        \n",
    "        Always use double quotes in the JSON object. \\n\n",
    "\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT: {user_input} \\n\n",
    "        CONTEXT: {context} \\n\n",
    "        CURRENT_VALUES: {current_values} \\n\n",
    "        Answer:\n",
    "        <|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\",\n",
    "        input_variables=[\"user_input\",\"context\",\"current_values\"],\n",
    "    )\n",
    "\n",
    "params_general_prompt = get_params_general_prompt_template()\n",
    "select_cs_prompt = get_select_cs_prompt_template()\n",
    "select_params_prompt = get_select_params_prompt_template()\n",
    "new_values_prompt = get_new_values_prompt_template()\n",
    "\n",
    "params_general_chain = params_general_prompt | json_model | JsonOutputParser()\n",
    "select_cs_chain = select_cs_prompt | json_model |JsonOutputParser()\n",
    "select_params_chain = select_params_prompt | json_model |JsonOutputParser()\n",
    "new_values_chain = new_values_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "user_input = 'What happens if we reduce the residual capacity of onshore residual wind turbines?'\n",
    "#user_input = 'modify the energy cost of oil import to be 10'\n",
    "\n",
    "params, CSs = get_params_and_cs_list('../CESM/Data/Techmap/DEModel.xlsx')\n",
    "\n",
    "llm_output = params_general_chain.invoke({\"user_input\": user_input})\n",
    "parametrization_type = llm_output['parametrization_type']\n",
    "\n",
    "cs_selection = select_cs_chain.invoke({\"user_input\": user_input, \"cs_list\": CSs})\n",
    "\n",
    "if parametrization_type == 'defined':\n",
    "    available_parameters = {cs: params for cs in cs_selection['cs_selection']}\n",
    "else:\n",
    "    available_parameters = get_populated_params_and_cs_list('../CESM/Data/Techmap/DEModel.xlsx', cs_selection['cs_selection'])\n",
    "\n",
    "print(llm_output)\n",
    "print(cs_selection)\n",
    "print(available_parameters)\n",
    "\n",
    "llm_output = select_params_chain.invoke({\"user_input\": user_input, \"param_list\": available_parameters})\n",
    "param_selection = llm_output['param_selection']\n",
    "\n",
    "print(llm_output)\n",
    "\n",
    "current_values = get_values('../CESM/Data/Techmap/DEModel.xlsx', param_selection)\n",
    "\n",
    "llm_output = new_values_chain.invoke({\"user_input\": user_input, \"context\": [], \"current_values\": current_values})\n",
    "\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modify': 'yes',\n",
       " 'run': 'yes',\n",
       " 'compare': 'yes',\n",
       " 'plot': 'yes',\n",
       " 'consult': 'no'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt_template() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are an expert at understanding the USER_INPUT and modifying ACTION_HISTORY to select\n",
    "        the necessary actions to be taken to fulfill the user's request. \\n\n",
    "        \n",
    "        ACTION_HISTORY provides you with the available actions to be taken, your job is to modify the\n",
    "        dictionary to select the actions to take. You'll receive all actions as 'no', to select an action\n",
    "        simply change that to 'yes', the rest of the pipeline will take care of executing them. \\n\n",
    "        \n",
    "        The actions are 'modify' to modify the model, 'run' to run the modified model, 'compare' to\n",
    "        compare the original results with the ones of the modified model, 'plot' to show the visualization\n",
    "        of the new results to the user, and 'consult' to check details regarding the construction of the model\n",
    "        (not the details of de modeling values or results, only the theory behind the model). \\n\n",
    "        \n",
    "        There are two types of USER_INPUT:\n",
    "        1. Gives you a direct command related to one or more of the available actions, such as asking you\n",
    "        to modify a value, or to run a model, to plot some specific data, compare the already available\n",
    "        results or consult details about the modeling process. In this case, you should only request the\n",
    "        actions the user has asked;\n",
    "        2. Gives you a scenario without specifying any command, asking for details about the scenario\n",
    "        for example. In this case you need to modify, run, analyze and plot the results to the user. \\n\n",
    "        \n",
    "        IMPORTANT if the user is just request you to modify something without asking you to run the model, then\n",
    "        you should assume you should only modify it. If the user actually wanted to run, he will ask later. \\n\n",
    "        \n",
    "        You must output a JSON object with the modified dictionary. \\n\n",
    "        \n",
    "        Always use double quotes in the JSON object. \\n\n",
    "        \n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT : {user_input} \\n\n",
    "        ACTION_HISTORY: {action_history} \\n\n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"user_input\", \"action_history\"],\n",
    "    )\n",
    "    \n",
    "prompt = get_prompt_template()\n",
    "llm_chain = prompt | json_model | JsonOutputParser()\n",
    "\n",
    "user_input = 'modify the CO2 limit to be half of what it is today'\n",
    "user_input = 'compare the model results'\n",
    "user_input = 'what happens if we kill the co2 limit after 2030?'\n",
    "user_input = 'kill the co2 limit after 2030'\n",
    "user_input = 'what would change if I dont want wind turbines?'\n",
    "action_history = {'modify': 'no', 'run': 'no', 'compare': 'no', 'plot': 'no', 'consult': 'no'}\n",
    "\n",
    "llm_chain.invoke({\"user_input\": user_input, \"action_history\": action_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'what are the dominant changes in 2050 compared to now?\\n'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../metadata/chat_history.pkl\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'esperanto', 'input': 'What is your name? How are you?'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt_template() -> PromptTemplate:\n",
    "    return PromptTemplate(\n",
    "        template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "        You are responsible of verifying if the USER_INPUT is in another language other\n",
    "        than english, if so, translate the input. If the text is already in english\n",
    "        simply output the same text. \\n\n",
    "        \n",
    "        Your output must be a JSON object with two keys, 'language' and 'input', where\n",
    "        'language' is the source language the user wrote and 'input' is the translated\n",
    "        input. \\n\n",
    "        \n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        USER_INPUT : {user_input} \\n\n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "        input_variables=[\"user_input\"],\n",
    "    )\n",
    "    \n",
    "prompt = get_prompt_template()\n",
    "llm_chain = prompt | test_json_model | JsonOutputParser()\n",
    "\n",
    "user_input = 'O que acontece se a gente reduzir o custo de turbinas elicas offshore?'\n",
    "user_input = 'cosa succede se riduciamo il costo delle turbine eoliche offshore?'\n",
    "user_input = 'Was passiert, wenn wir die Kosten fr Offshore-Windturbinen senken?'\n",
    "user_input = 'What happens if we lower the cost of offshore wind turbines?'\n",
    "user_input = 'que se passe-t-il si nous rduisons le cot des oliennes en mer ?'\n",
    "user_input = 'mi trtnik, ha cskkentjk a tengeri szlturbink kltsgeit?'\n",
    "user_input = 'apa yang terjadi jika kita mengurangi biaya turbin angin lepas pantai?'\n",
    "user_input = '         ;'\n",
    "user_input = 'Mit tapahtuu, jos merituulivoimaloiden kustannuksia alennetaan?'\n",
    "user_input = '       '\n",
    "user_input = ''\n",
    "user_input = ''\n",
    "user_input = \"Kio estas via nomo? Kiel vi fartas?\"\n",
    "\n",
    "llm_chain.invoke({\"user_input\": user_input})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
