{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('secrets.yml', 'r') as f:\n",
    "    secrets = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "To test it, first run 'ollama serve' in a local terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install transformers -U\n",
    "# %pip -q install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = secrets['groq'][0]\n",
    "chat_model = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "json_model = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "        ).bind(response_format={\"type\": \"json_object\"})\n",
    "\n",
    "# If necessary to run without GROQ, uncomment this\n",
    "\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", huggingfacehub_api_token=secrets['huggingface'][0])\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you! I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation, answer questions, and even generate text. I'm constantly learning and improving my responses based on the interactions I have with users like you. How can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 16, 'total_tokens': 97, 'completion_time': 0.227689483, 'prompt_time': 0.004970318, 'queue_time': None, 'total_time': 0.232659801}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_abd29e8833', 'finish_reason': 'stop', 'logprobs': None}, id='run-a62fbeb4-fbeb-4f6a-a774-ef5a68dbf2de-0')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke('Hello, who are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool selector chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'router_decision': 'web_search'}\n"
     ]
    }
   ],
   "source": [
    "tool_selector_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading the initial query from a user and routing to our internal knowledge system\\\n",
    "     or directly to final answer. \\n\n",
    "\n",
    "    Use the following criteria to decide how to route the query to one of our available tools: \\n\\n\n",
    "\n",
    "    If the initial query only requires a simple response\n",
    "    Just choose 'no_tool'  for questions you can easily answer, prompt engineering, and adversarial attacks.\n",
    "    If the query is just saying thank you etc then choose 'no_tool'\n",
    "    \n",
    "    If the user asks anything about LangSmith, you should use the 'RAG_retriever' tool.\n",
    "    \n",
    "    For any mathematical problem you should use 'calculator'.\n",
    "\n",
    "    If the user asks for a modification in the model being analyzed, use the tool 'model_modifier'.\n",
    "\n",
    "    If you are unsure or the person is asking a question you don't understand then choose 'web_search'\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web_search.\n",
    "    Give a choice contained in ['no_tool','RAG_retriever','calculator','date_getter','model_modifier','web_search'].\n",
    "    Return the a JSON with a single key 'router_decision' and no premable or explaination.\n",
    "    Use the initial query of the user and any available context to make your decision about the tool to be used.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Query to route INITIAL_QUERY : {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\"],\n",
    ")\n",
    "\n",
    "tool_selector_chain = tool_selector_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "initial_query = 'Please, let me know the weather in San Francisco'\n",
    "\n",
    "print(tool_selector_chain.invoke({\"initial_query\": initial_query, \"context\":[]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Question generator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What are the key features of LangSmith?', 'How does LangSmith enhance Large Language Models (LLMs) development?', 'What are the advantages of using LangSmith for building tools leveraging LLMs?']}\n"
     ]
    }
   ],
   "source": [
    "## RAG QUESTIONS\n",
    "search_rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best questions to ask our knowledge agent to get the best info for the customer.\n",
    "\n",
    "    Given the INITIAL_QUERY, work out the best questions that will find the best \\\n",
    "    info for helping to write the final answer. Write the questions to our knowledge system not to the customer.\n",
    "\n",
    "    Return a JSON with a single key 'questions' with no more than 3 strings of and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_EMAIL: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "question_rag_chain = search_rag_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "research_info = None\n",
    "query = 'What are the main benefits of using LangSmith for developing a tool to levarage LLMs?'\n",
    "\n",
    "print(question_rag_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load the data that will be used by the retriever\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Set the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Split the data and vectorize it\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Define a chain to gather data and a retriever\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG Chain\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUESTION: {question} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model modifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parameter': 'wind_power_plant_lifetime', 'new_value': 50}\n"
     ]
    }
   ],
   "source": [
    "## MODEL MODIFIER\n",
    "model_modifier_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to modify a energy system model, whenever the user asks \\\n",
    "    you to modify a parameter, you will build a JSON object with the desired modifications.\n",
    "    \n",
    "    Given the INITIAL_QUERY, determine the parameter that the user wants to change, and the new value that should be applied \\\n",
    "    and with this information, return a JSON with only two keys 'parameter' and 'new_value' with no preamble or explanaition\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "model_modifier_chain = model_modifier_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'I want the lifetime of wind power plants to be modified to 50 years'\n",
    "\n",
    "print(model_modifier_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web search chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keywords': ['500m speed skating record', 'world record holder 500m', 'speed skating world records']}\n"
     ]
    }
   ],
   "source": [
    "## Search keywords\n",
    "search_keyword_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best keywords to search for in a web search to get the best info for the user.\n",
    "\n",
    "    Given the INITIAL_QUERY and the CONTEXT of past iterations, work out the best keywords that will find the info requested by the user\n",
    "    The keywords should have between 3 and 5 words each, if the query allows for it.\n",
    "\n",
    "    Return a JSON with a single key 'keywords' with no more than 3 keywords and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "search_keyword_chain = search_keyword_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Who is the current holder of the speed skating world record on 500 meters?'\n",
    "\n",
    "print(search_keyword_chain.invoke({\"initial_query\": query, \"context\": []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'operation': '^', 'op_1': 27, 'op_2': 5}\n"
     ]
    }
   ],
   "source": [
    "## CALCULATOR\n",
    "calculator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to do calculations using a calculator tool.\n",
    "    \n",
    "    You can only output a single format of JSON object consisting in two operands\n",
    "    and the operation. The name of the only three keys are 'operation', 'op_1' and 'op_2' \\n\n",
    "    \n",
    "    'operation' can only be [+,-,*,/,^]\n",
    "    'op_1' and 'op_2' must be integers or float\\n\n",
    "    \n",
    "    If you judge that the equation consists of more than one operation, solve only one,\n",
    "    the calculator can be called multiple times and the other results will be solved\n",
    "    later.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "calculator_chain = calculator_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "research_info = None\n",
    "query = 'How much is 27 to the power of 5 plus 7?'\n",
    "\n",
    "print(calculator_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output generator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_ready': False, 'message': 'To determine if your car is more powerful than a GT-R R32, I need to know the make and model of your 2010 car. Can you please provide that information?'}\n"
     ]
    }
   ],
   "source": [
    "## OUTPUT GENERATOR\n",
    "output_generator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at answering the user based on context given.\n",
    "    \n",
    "    Given the INITIAL_QUERY an CONTEXT, generate an answer for the query \\\n",
    "    asked by the user. You should make use of the provided information \\\n",
    "    to better answer the user. You will output a JSON containing two keys \\\n",
    "    'is_ready', 'message'. The first one is a boolean that should indicate \\\n",
    "    that you think you have the final answer (true) or if you need more context\\\n",
    "    to fully answer (false), the second is the message to be displayed to the user.\\\n",
    "        \n",
    "    You'll only indicate that an answer is final ('is_ready' == true) if all\\\n",
    "    of the user questions and requests were fully answered. If any info or\\\n",
    "    action is missing, indicate that 'is_ready' is false.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\"],\n",
    ")\n",
    "\n",
    "output_generator_chain = output_generator_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "research_info = None\n",
    "query = 'Is my car more powerful than a GT-R R32?'\n",
    "context = 'The car owned by the user is from 2010'\n",
    "\n",
    "print(output_generator_chain.invoke({\"initial_query\": query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Iterator Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'new_query': 'What is the horsepower of my car from 2010 and the GT-R R32?'}\n"
     ]
    }
   ],
   "source": [
    "## ANSWER ITERATOR\n",
    "answer_iterator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at analyzing an answer generated, given the initial query, \\\n",
    "    and understanding what is still missing for a full answer.\n",
    "    \n",
    "    Given the INITIAL_QUERY, the CONTEXT and the INTERMEDIATE_ANSWER, generate a new query using \\\n",
    "    the information that you already now, and trying to reach the missing information. \\\n",
    "    You will output a JSON containing a single key 'new_query' containing the new  \\\n",
    "    query that should be used to proceed torwards an acceptable answer.\\\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    INTERMEDIATE_ANSWER: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\",\"output\"],\n",
    ")\n",
    "\n",
    "answer_iterator_chain = answer_iterator_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "research_info = None\n",
    "query = 'Is my car more powerful than a GT-R R32?'\n",
    "context = 'The car owned by the user is from 2010'\n",
    "intermediate_answer = 'To determine if your car is more powerful than a Nissan GT-R R32, I would need to know the make and model of your 2010 car. The GT-R R32 has a 2.6L turbocharged engine producing around 276 horsepower. If you provide me with your car\\'s specifications, I can give you a more accurate comparison.'\n",
    "\n",
    "print(answer_iterator_chain.invoke({\"initial_query\": query, \"context\": context, \"intermediate_answer\": intermediate_answer}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        initial_query: user input\n",
    "        num_steps: number of steps\n",
    "        selected_tool: name of the selected tool\n",
    "        rag_questions: questions used for retrieval\n",
    "        tool_parameters: parameters to be used by tools\n",
    "        context: list of context generated for the query\n",
    "        intermediate_answer: generated answer before analysis\n",
    "        info_needed: whether to add search info\n",
    "        final_answer: LLM generation\n",
    "    \"\"\"\n",
    "    initial_query : str\n",
    "    num_steps : int\n",
    "    selected_tool: str\n",
    "    rag_questions : List[str]\n",
    "    tool_parameters: str\n",
    "    context : List[str]\n",
    "    intermediate_answer: str\n",
    "    info_needed : bool\n",
    "    final_answer : str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Selector Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_selector(state):\n",
    "    \n",
    "    print(\"---TOOL SELECTION---\")\n",
    "    initial_query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    router = tool_selector_chain.invoke({\"initial_query\": initial_query, \"context\": context})\n",
    "    router_decision = router['router_decision']\n",
    "    \n",
    "    print(f'SELECTED TOOL: {router_decision}\\n')\n",
    "    \n",
    "    return {\"selected_tool\": router_decision,\n",
    "            \"num_steps\": num_steps,\n",
    "            \"context\": context}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_rag(state):\n",
    "\n",
    "    print(\"---RAG LANGSMITH RETRIEVER---\")\n",
    "    initial_query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    questions = question_rag_chain.invoke({\"initial_query\": initial_query})\n",
    "    questions = questions['questions']\n",
    "\n",
    "    rag_results = []\n",
    "    for idx, question in enumerate(questions):\n",
    "        print(f'QUESTION {idx}: {question}')\n",
    "        temp_docs = rag_chain.invoke(question)\n",
    "        print(f'ANSWER FOR QUESTION {idx}: {temp_docs}')\n",
    "        question_results = question + '\\n\\n' + temp_docs + \"\\n\\n\\n\"\n",
    "        if rag_results is not None:\n",
    "            rag_results.append(question_results)\n",
    "        else:\n",
    "            rag_results = [question_results]\n",
    "    print(f'FULL ANSWERS: {rag_results}\\n')\n",
    "    \n",
    "    return {\"context\": rag_results,\n",
    "            \"rag_questions\":questions,\n",
    "            \"num_steps\":num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install -U langchain-community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = secrets['tavily'][0]\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_web(state):\n",
    "\n",
    "    print(\"---RESEARCH INFO SEARCHING---\")\n",
    "    initial_query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    # Web search\n",
    "    keywords = search_keyword_chain.invoke({\"initial_query\": initial_query, \"context\": context})\n",
    "    keywords = keywords['keywords']\n",
    "    full_searches = []\n",
    "    for idx, keyword in enumerate(keywords[:1]):\n",
    "        print(f'KEYWORD {idx}: {keyword}')\n",
    "        temp_docs = web_search_tool.invoke({\"query\": keyword})\n",
    "        if type(temp_docs) == list:\n",
    "            web_results = \"\\n\".join([d[\"content\"] for d in temp_docs])\n",
    "            web_results = Document(page_content=web_results)\n",
    "        elif type(temp_docs) == dict:\n",
    "            web_results = temp_docs[\"content\"]\n",
    "            web_results = Document(page_content=web_results)\n",
    "        else:\n",
    "            web_results = 'No results'\n",
    "        print(f'RESULTS FOR KEYWORD {idx}: {web_results}')\n",
    "        if full_searches is not None:\n",
    "            full_searches.append(web_results)\n",
    "        else:\n",
    "            full_searches = [web_results]\n",
    "    print(f'FULL RESULTS: {full_searches}\\n')\n",
    "    \n",
    "    return {\"context\": full_searches,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(state):\n",
    "\n",
    "    print(\"---CALCULATOR TOOL---\")\n",
    "    \n",
    "    query = state['initial_query']\n",
    "    parameters = calculator_chain.invoke({\"initial_query\": query})\n",
    "    operation = parameters['operation']\n",
    "    op_1 = parameters['op_1']\n",
    "    op_2 = parameters['op_2']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'OPERATION: {operation}')\n",
    "    print(f'OPERAND 1: {op_1}')\n",
    "    print(f'OPERAND 2: {op_2}')\n",
    "\n",
    "    if operation == \"+\":\n",
    "        result = op_1 + op_2\n",
    "    elif operation == \"-\":\n",
    "        result = op_1 - op_2\n",
    "    elif operation == \"/\":\n",
    "        result = op_1 / op_2\n",
    "    elif operation == \"*\":\n",
    "        result = op_1 * op_2\n",
    "    elif operation == \"^\":\n",
    "        result = op_1 ** op_2\n",
    "    else:\n",
    "        result = 'ERROR'\n",
    "        \n",
    "    if result == 'ERROR':\n",
    "        str_result = 'Unable to execute the selected operation'\n",
    "    else:\n",
    "        str_result = f'{op_1} {operation} {op_2} = {result}'\n",
    "        \n",
    "    print(f'RESULT: {str_result}\\n')\n",
    "        \n",
    "    return {\"context\": str_result,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Getter Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def date_getter(state):\n",
    "\n",
    "    print(\"---DATE GETTER TOOL---\")\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    current_date = datetime.now().strftime(\"%d %B %Y, %H:%M:%S\")\n",
    "    \n",
    "    result = f'The current date and time are {current_date}'\n",
    "    \n",
    "    print(f'CURRENT DATE: {current_date}\\n')\n",
    "\n",
    "    return {\"context\": result,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Modifier Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(state):\n",
    "\n",
    "    print(\"---MODEL MODIFIER TOOL---\")\n",
    "    query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    parameters_json = model_modifier_chain.invoke({\"initial_query\": query})\n",
    "    print(f'JSON:\\n{parameters_json}\\n')\n",
    "    \n",
    "    result = f'The model was successfully modified'\n",
    "\n",
    "    return {\"context\": result,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_generator(state):\n",
    "    print(\"---GENERATE OUTPUT---\")\n",
    "    ## Get the state\n",
    "    initial_query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    # Generate draft email\n",
    "    answer = output_generator_chain.invoke({\"initial_query\": initial_query,\n",
    "                                            \"context\": context})\n",
    "    print(f'GENERATED OUTPUT:\\n{answer}\\n')\n",
    "    \n",
    "    if answer['is_ready']:\n",
    "        return {\"final_answer\": answer['message'],\n",
    "                \"intermediate_answer\": answer,\n",
    "                \"num_steps\": num_steps}\n",
    "    else:\n",
    "        return {\"intermediate_answer\": answer,\n",
    "                \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Iterator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def iterate_over_answer(state):\n",
    "    print(\"---ANSWER ITERATOR---\")\n",
    "    ## Get the state\n",
    "    initial_query = state['initial_query']\n",
    "    context = state['context']\n",
    "    intermediate_answer = state['intermediate_answer']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    # Generate draft email\n",
    "    output = answer_iterator_chain.invoke({\"initial_query\": initial_query,\n",
    "                                              \"context\": context,\n",
    "                                              \"intermediate_answer\":intermediate_answer}\n",
    "                                            )\n",
    "    \n",
    "    new_query = output['new_query']\n",
    "    print(f'NEW QUERY: {new_query}\\n')\n",
    "    \n",
    "    new_query = f'{initial_query} \\n\\n NEW QUERY: {new_query}'\n",
    "    \n",
    "    return {\"initial_query\": new_query,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_printer(state):\n",
    "    \"\"\"print the state\"\"\"\n",
    "    print(\"------------------STATE PRINTER------------------\")\n",
    "    print(f\"Num Steps: {state['num_steps']} \\n\")\n",
    "    print(f\"Initial Query: {state['initial_query']} \\n\" )\n",
    "    print(f\"RAG Questions: {state['rag_questions']} \\n\")\n",
    "    print(f\"Tool Parameters: {state['tool_parameters']} \\n\")\n",
    "    print(f\"Context: {state['context']} \\n\" )\n",
    "    print(f\"Intermediate Answer: {state['intermediate_answer']} \\n\" )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer_printer(state):\n",
    "    \"\"\"prints final answer\"\"\"\n",
    "    print(\"------------------FINAL ANSWER------------------\")\n",
    "    print(f\"Final Answer: {state['final_answer']} \\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_tool(state):\n",
    "    \"\"\"\n",
    "    Route to the necessary tool.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    selection = state['selected_tool']\n",
    "    \n",
    "    if selection == 'RAG_retriever':\n",
    "        print(\"---ROUTE QUERY TO RAG RETRIEVER---\")\n",
    "        return \"RAG_retriever\"\n",
    "    elif selection == 'web_search':\n",
    "        print(\"---ROUTE QUERY TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif selection == 'calculator':\n",
    "        print(\"---ROUTE QUERY TO CALCULATOR---\")\n",
    "        return \"calculator\"\n",
    "    elif selection == 'model_modifier':\n",
    "        print(\"---ROUTE QUERY TO MODEL MODIFIER---\")\n",
    "        return \"model_modifier\"\n",
    "    elif selection == 'no_tool':\n",
    "        print(\"---ROUTE QUERY TO SIMPLE ANSWER---\")\n",
    "        return \"no_tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_iterate(state):\n",
    "\n",
    "    print(\"---ROUTE TO ITERATE---\")\n",
    "    intermediate_answer = state[\"intermediate_answer\"]\n",
    "\n",
    "    print(intermediate_answer)\n",
    "    print(intermediate_answer['is_ready'])\n",
    "    if intermediate_answer['is_ready']:\n",
    "        print(\"------\")\n",
    "        return \"ready\"\n",
    "    else:\n",
    "        print(\"---ROUTE TO ANALYSIS - ITERATE---\")\n",
    "        return \"iterate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"tool_selector\", tool_selector)\n",
    "workflow.add_node(\"research_info_rag\", research_info_rag) # RAG search\n",
    "workflow.add_node(\"research_info_web\", research_info_web) # web search\n",
    "workflow.add_node(\"state_printer\", state_printer)\n",
    "workflow.add_node(\"calculator\", calculator)\n",
    "workflow.add_node(\"date_getter\", date_getter)\n",
    "workflow.add_node(\"model_modifier\", model_modifier)\n",
    "workflow.add_node(\"output_generator\", output_generator)\n",
    "workflow.add_node(\"iterate_over_answer\", iterate_over_answer)\n",
    "workflow.add_node(\"final_answer_printer\", final_answer_printer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"date_getter\")\n",
    "workflow.add_edge(\"date_getter\", \"tool_selector\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"tool_selector\",\n",
    "    route_to_tool,\n",
    "    {\n",
    "        \"RAG_retriever\": \"research_info_rag\",\n",
    "        \"web_search\": \"research_info_web\",\n",
    "        \"calculator\": \"calculator\",\n",
    "        \"model_modifier\": \"model_modifier\",\n",
    "        \"no_tool\": \"output_generator\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"research_info_rag\", \"output_generator\")\n",
    "workflow.add_edge(\"research_info_web\", \"output_generator\")\n",
    "workflow.add_edge(\"calculator\", \"output_generator\")\n",
    "workflow.add_edge(\"model_modifier\", \"output_generator\")\n",
    "\n",
    "workflow.add_edge(\"output_generator\", \"state_printer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"state_printer\",\n",
    "    route_to_iterate,\n",
    "    {\n",
    "        \"iterate\": \"iterate_over_answer\",\n",
    "        \"ready\": \"final_answer_printer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"iterate_over_answer\", \"tool_selector\")\n",
    "workflow.add_edge(\"final_answer_printer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DATE GETTER TOOL---\n",
      "CURRENT DATE: 16 June 2024, 18:30:13\n",
      "\n",
      "Finished running: date_getter:\n",
      "---TOOL SELECTION---\n",
      "SELECTED TOOL: calculator\n",
      "\n",
      "---ROUTE QUERY TO CALCULATOR---\n",
      "Finished running: tool_selector:\n",
      "---CALCULATOR TOOL---\n",
      "OPERATION: /\n",
      "OPERAND 1: 828\n",
      "OPERAND 2: 43\n",
      "RESULT: 828 / 43 = 19.25581395348837\n",
      "\n",
      "Finished running: calculator:\n",
      "---GENERATE OUTPUT---\n",
      "GENERATED OUTPUT:\n",
      "{'is_ready': False, 'message': 'To complete the calculation, I need to know the current temperature in Paris (in Celsius). Please provide the temperature.'}\n",
      "\n",
      "Finished running: output_generator:\n",
      "------------------STATE PRINTER------------------\n",
      "Num Steps: 4 \n",
      "\n",
      "Initial Query: Divide the height of the Burj Khalifa by Ronaldinho Gaucho's age, then add the current temperature in Paris (in Celsius) \n",
      "\n",
      "RAG Questions: None \n",
      "\n",
      "Tool Parameters: None \n",
      "\n",
      "Context: 828 / 43 = 19.25581395348837 \n",
      "\n",
      "Intermediate Answer: {'is_ready': False, 'message': 'To complete the calculation, I need to know the current temperature in Paris (in Celsius). Please provide the temperature.'} \n",
      "\n",
      "---ROUTE TO ITERATE---\n",
      "{'is_ready': False, 'message': 'To complete the calculation, I need to know the current temperature in Paris (in Celsius). Please provide the temperature.'}\n",
      "False\n",
      "---ROUTE TO ANALYSIS - ITERATE---\n",
      "---ANSWER ITERATOR---\n",
      "NEW QUERY: What is the current temperature in Paris (in Celsius)?\n",
      "\n",
      "Finished running: iterate_over_answer:\n",
      "---TOOL SELECTION---\n",
      "SELECTED TOOL: calculator\n",
      "\n",
      "---ROUTE QUERY TO CALCULATOR---\n",
      "Finished running: tool_selector:\n",
      "---CALCULATOR TOOL---\n",
      "OPERATION: +\n",
      "OPERAND 1: 0\n",
      "OPERAND 2: 0\n",
      "RESULT: 0 + 0 = 0\n",
      "\n",
      "Finished running: calculator:\n",
      "---GENERATE OUTPUT---\n",
      "GENERATED OUTPUT:\n",
      "{'is_ready': False, 'message': 'The current temperature in Paris is needed to complete the calculation. Please provide the current temperature in Celsius.'}\n",
      "\n",
      "Finished running: output_generator:\n",
      "------------------STATE PRINTER------------------\n",
      "Num Steps: 8 \n",
      "\n",
      "Initial Query: Divide the height of the Burj Khalifa by Ronaldinho Gaucho's age, then add the current temperature in Paris (in Celsius) \n",
      "\n",
      " NEW QUERY: What is the current temperature in Paris (in Celsius)? \n",
      "\n",
      "RAG Questions: None \n",
      "\n",
      "Tool Parameters: None \n",
      "\n",
      "Context: 0 + 0 = 0 \n",
      "\n",
      "Intermediate Answer: {'is_ready': False, 'message': 'The current temperature in Paris is needed to complete the calculation. Please provide the current temperature in Celsius.'} \n",
      "\n",
      "---ROUTE TO ITERATE---\n",
      "{'is_ready': False, 'message': 'The current temperature in Paris is needed to complete the calculation. Please provide the current temperature in Celsius.'}\n",
      "False\n",
      "---ROUTE TO ANALYSIS - ITERATE---\n",
      "---ANSWER ITERATOR---\n",
      "NEW QUERY: What is the height of the Burj Khalifa?\n",
      "\n",
      "Finished running: iterate_over_answer:\n",
      "---TOOL SELECTION---\n",
      "SELECTED TOOL: calculator\n",
      "\n",
      "---ROUTE QUERY TO CALCULATOR---\n",
      "Finished running: tool_selector:\n",
      "---CALCULATOR TOOL---\n",
      "OPERATION: /\n",
      "OPERAND 1: 828\n",
      "OPERAND 2: 42\n",
      "RESULT: 828 / 42 = 19.714285714285715\n",
      "\n",
      "Finished running: calculator:\n",
      "---GENERATE OUTPUT---\n",
      "GENERATED OUTPUT:\n",
      "{'is_ready': True, 'message': \"The result of the calculation is: 19.714285714285715 + Current Temperature in Paris (in Celsius). The height of the Burj Khalifa is 828 meters. Ronaldinho Gaucho's age is 42. Please provide the current temperature in Paris (in Celsius) to get the final answer.\"}\n",
      "\n",
      "Finished running: output_generator:\n",
      "------------------STATE PRINTER------------------\n",
      "Num Steps: 12 \n",
      "\n",
      "Initial Query: Divide the height of the Burj Khalifa by Ronaldinho Gaucho's age, then add the current temperature in Paris (in Celsius) \n",
      "\n",
      " NEW QUERY: What is the current temperature in Paris (in Celsius)? \n",
      "\n",
      " NEW QUERY: What is the height of the Burj Khalifa? \n",
      "\n",
      "RAG Questions: None \n",
      "\n",
      "Tool Parameters: None \n",
      "\n",
      "Context: 828 / 42 = 19.714285714285715 \n",
      "\n",
      "Intermediate Answer: {'is_ready': True, 'message': \"The result of the calculation is: 19.714285714285715 + Current Temperature in Paris (in Celsius). The height of the Burj Khalifa is 828 meters. Ronaldinho Gaucho's age is 42. Please provide the current temperature in Paris (in Celsius) to get the final answer.\"} \n",
      "\n",
      "---ROUTE TO ITERATE---\n",
      "{'is_ready': True, 'message': \"The result of the calculation is: 19.714285714285715 + Current Temperature in Paris (in Celsius). The height of the Burj Khalifa is 828 meters. Ronaldinho Gaucho's age is 42. Please provide the current temperature in Paris (in Celsius) to get the final answer.\"}\n",
      "True\n",
      "------\n",
      "------------------FINAL ANSWER------------------\n",
      "Final Answer: The result of the calculation is: 19.714285714285715 + Current Temperature in Paris (in Celsius). The height of the Burj Khalifa is 828 meters. Ronaldinho Gaucho's age is 42. Please provide the current temperature in Paris (in Celsius) to get the final answer. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query = 'If I pay half the age of Tom Jobim plus the height of the Empire State for a car, how much I\\'ve paid?'\n",
    "#query = 'What is 10 to the power of 0.4?'\n",
    "#query = 'What is the temperature and humidity in Migliarino right now? And also, what time is it?'\n",
    "#query = 'Modify the parameter X to 24 for me please'\n",
    "#query = 'What are some of the most important things that happened today in past years?'\n",
    "#query = 'What day is today?'\n",
    "#query = 'How can LangSmith help in my project?'\n",
    "#query = 'I am always coming but never arrive. What am I?'\n",
    "#query = 'Change the lifetime of wind power plants to 25 years please'\n",
    "query = 'Divide the height of the Burj Khalifa by Ronaldinho Gaucho\\'s age, then add the current temperature in Paris (in Celsius)'\n",
    "#query = 'What are good famous and more casual board games that can be played by two players?'\n",
    "\n",
    "# run the agent\n",
    "inputs = {\"initial_query\": query, \"num_steps\": 0, \"context\": []}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished running: {key}:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
