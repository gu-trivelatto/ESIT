{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Include source to web search\n",
    "# Include a case if the user asks for more than one action on the modelling tools in a single query (for now we'll take care of just one at a time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('secrets.yml', 'r') as f:\n",
    "    secrets = yaml.load(f, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "To test it, first run 'ollama serve' in a local terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install transformers -U\n",
    "# %pip -q install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = secrets['groq'][0]\n",
    "chat_model = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "        )\n",
    "json_model = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "        ).bind(response_format={\"type\": \"json_object\"})\n",
    "\n",
    "# If necessary to run without GROQ, uncomment this\n",
    "\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\", huggingfacehub_api_token=secrets['huggingface'][0])\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you! I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation, answer questions, and even generate text. I'm constantly learning and improving my responses based on the interactions I have with users like you. So, how can I assist you today?\", response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 16, 'total_tokens': 99, 'completion_time': 0.232306828, 'prompt_time': 0.007834338, 'queue_time': None, 'total_time': 0.240141166}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_abd29e8833', 'finish_reason': 'stop', 'logprobs': None}, id='run-1d5fa37d-2c08-40fe-8e4c-4fde6b2059b4-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke('Hello, who are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type identifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_type': 'mixed'}\n"
     ]
    }
   ],
   "source": [
    "type_identifier_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at identifying the type of a query provided by the user among\n",
    "    the types \"general\", \"energy_system\" and \"mixed\".\n",
    "\n",
    "    \"general\": the query is related to some generic topic, it may consist of one or more\n",
    "    points that require searching for information. \\n\n",
    "    \n",
    "    \"energy_system\": the query is a direct command related to the energy system model, it can\n",
    "    be a request to change parameters, plot data, run simulations, or anything on this lines.\n",
    "    To be characterized as this class, it should need no external information. Names of\n",
    "    simulations, scenarios, parameters and any other potential name is assumed to be know by our tools. \\n\n",
    "    \n",
    "    \"mixed\": the query is related to the energy system model, but it requires external data for the\n",
    "    command to be complete. It MUST be related to running anything related to the energy system,\n",
    "    otherwise it is not mixed. \\n\n",
    "    \n",
    "    You must output a JSON with a single key 'query_type' containing exclusivelly the \n",
    "    selected type. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "\n",
    "type_identifier_chain = type_identifier_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Modify the cost of CO2 in 2020 to be same price as a liter of Coca Cola'\n",
    "\n",
    "print(type_identifier_chain.invoke({\"query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy System tool selector chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'selected_tool': 'model_modifier'}\n"
     ]
    }
   ],
   "source": [
    "es_tool_selector_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading the user QUERY and routing it to the correct tool in our\n",
    "    modelling system. \\n\n",
    "\n",
    "    Use the following criteria to decide how to route the query to one of our available tools: \\n\\n\n",
    "    \n",
    "    If the user asks for any modification on any particular model, select 'model_modifier'. \\n\n",
    "    \n",
    "    If the user asks to plot anything, select 'data_plotter'. \\n\n",
    "    \n",
    "    If the user asks to run a simulation of any particular model, select 'sim_runner'. \\n\n",
    "\n",
    "    You must output a JSON object with a single key 'selected_tool' containing one of\n",
    "    the following values ['model_modificator', 'data_plotter', 'sim_runner']. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "\n",
    "es_tool_selector_chain = es_tool_selector_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "initial_query = 'Modify the parameters of LowGasPrice to be 0.1 higher'\n",
    "\n",
    "print(es_tool_selector_chain.invoke({\"query\": initial_query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'complete_data': True}\n"
     ]
    }
   ],
   "source": [
    "mixed_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading the user QUERY and the available CONTEXT to decide if there\n",
    "    is already enough information gathered to fulfill the energy system related command\n",
    "    made by the user. \\n\n",
    "    \n",
    "    You must be certain that you have all the data before deciding to send it to the\n",
    "    modelling section of the pipeline.\n",
    "\n",
    "    You must output a JSON object with a single key 'complete_data' containing a boolean\n",
    "    on whether you have enough data for the user's request or not. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\",\"context\"],\n",
    ")\n",
    "\n",
    "mixed_chain = mixed_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "initial_query = 'Modify the lifetime of wind power plants to be the age of Ronaldinho Gaucho plus Oprah age'\n",
    "context = ['The current age of Ronaldinho Gaucho is 44 years old', 'Oprah is 68 years old']\n",
    "\n",
    "print(mixed_chain.invoke({\"query\": initial_query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool selector chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'router_decision': 'web_search'}\n"
     ]
    }
   ],
   "source": [
    "tool_selector_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at reading a QUERY from a user and routing to our internal knowledge system\\\n",
    "     or directly to final answer. \\n\n",
    "\n",
    "    Use the following criteria to decide how to route the query to one of our available tools: \\n\\n\n",
    "    \n",
    "    If the user asks anything about LangSmith, you should use the 'RAG_retriever' tool.\n",
    "    \n",
    "    For any mathematical problem you should use 'calculator'. Be sure that you have all the necessary\n",
    "    data before routing to this tool.\n",
    "\n",
    "    If you are unsure or the person is asking a question you don't understand then choose 'web_search'\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web_search.\n",
    "    Give a choice contained in ['RAG_retriever','calculator','web_search'].\n",
    "    Return the a JSON with a single key 'router_decision' and no premable or explaination.\n",
    "    Use the initial query of the user and any available context to make your decision about the tool to be used.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY : {query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "\n",
    "tool_selector_chain = tool_selector_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "initial_query = 'Please, let me know the weather in San Francisco'\n",
    "\n",
    "print(tool_selector_chain.invoke({\"query\": initial_query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Question generator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': ['What are the key features of LangSmith?', 'How does LangSmith support LLM development?', 'What are the advantages of using LangSmith over other LLM development tools?']}\n"
     ]
    }
   ],
   "source": [
    "## RAG QUESTIONS\n",
    "search_rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best questions to ask our knowledge agent to get the best info for the customer.\n",
    "\n",
    "    Given the INITIAL_QUERY, work out the best questions that will find the best \\\n",
    "    info for helping to write the final answer. Write the questions to our knowledge system not to the customer.\n",
    "\n",
    "    Return a JSON with a single key 'questions' with no more than 3 strings of and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "question_rag_chain = search_rag_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'What are the main benefits of using LangSmith for developing a tool to levarage LLMs?'\n",
    "\n",
    "print(question_rag_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install beautifulsoup4\n",
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load the data that will be used by the retriever\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Set the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# Split the data and vectorize it\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Define a chain to gather data and a retriever\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG Chain\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUESTION: {question} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    Answer:\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever , \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model modifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parameter': 'wind_power_plant_lifetime', 'new_value': 50}\n"
     ]
    }
   ],
   "source": [
    "## MODEL MODIFIER\n",
    "model_modifier_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to modify a energy system model, whenever the user asks \\\n",
    "    you to modify a parameter, you will build a JSON object with the desired modifications.\n",
    "    \n",
    "    Given the INITIAL_QUERY, determine the parameter that the user wants to change, and the new value that should be applied \\\n",
    "    and with this information, return a JSON with only two keys 'parameter' and 'new_value' with no preamble or explanaition\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\"],\n",
    ")\n",
    "\n",
    "model_modifier_chain = model_modifier_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'I want the lifetime of wind power plants to be modified to 50 years'\n",
    "context = []\n",
    "\n",
    "print(model_modifier_chain.invoke({\"initial_query\": query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data plotter chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'SimpleDemo', 'scenario_name': 'LowGasPrice', 'plot_type': 'Sankey'}\n"
     ]
    }
   ],
   "source": [
    "## DATA PLOTTER\n",
    "data_plotter_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to plot data from an energy system model, whenever the user asks\n",
    "    you to plot data from the model, you will build a JSON object with the desired plot and scenario.\n",
    "    \n",
    "    Given the INITIAL_QUERY and the CONTEXT, determine the details required by the plot. You will return\n",
    "    a JSON object with only three keys, 'model_name', 'scenario_name', 'plot_type'.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\", \"context\"],\n",
    ")\n",
    "\n",
    "data_plotter_chain = data_plotter_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Give me the Sankey plot of the model SimpleDemo and scenario LowGasPrice'\n",
    "context = []\n",
    "\n",
    "print(data_plotter_chain.invoke({\"initial_query\": query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sim runner chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'SimpleDemo'}\n"
     ]
    }
   ],
   "source": [
    "## SIM RUNNER\n",
    "sim_runner_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to run simulations of an energy system model, whenever the user asks\n",
    "    you to run a new simulation of the model, you will build a JSON object with the desired model.\n",
    "    \n",
    "    Given the INITIAL_QUERY and the CONTEXT, determine the details required by the plot. You will return\n",
    "    a JSON object with a single key, 'model_name'.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\", \"context\"],\n",
    ")\n",
    "\n",
    "sim_runner_chain = sim_runner_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Run the SimpleDemo model'\n",
    "context = []\n",
    "\n",
    "print(sim_runner_chain.invoke({\"initial_query\": query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web search chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keywords': ['speed skating world records', '500m world record holder', 'fastest 500m skater']}\n"
     ]
    }
   ],
   "source": [
    "## Search keywords\n",
    "search_keyword_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a master at working out the best keywords to search for in a web search to get the best info for the user.\n",
    "\n",
    "    Given the INITIAL_QUERY, work out the best keywords that will find the info requested by the user\n",
    "    The keywords should have between 3 and 5 words each, if the query allows for it.\n",
    "\n",
    "    Return a JSON with a single key 'keywords' with no more than 3 keywords and no preamble or explaination.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "search_keyword_chain = search_keyword_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Who is the current holder of the speed skating world record on 500 meters?'\n",
    "\n",
    "print(search_keyword_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web/RAG answer analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A liter of Coca-Cola in Brazil costs approximately 1.72 USD (0.86 USD for 0.5 liters).\n"
     ]
    }
   ],
   "source": [
    "answer_analyzer_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at summarizing a bunch of data to extract only the important bits from it.\n",
    "\n",
    "    Given the user's QUERY and the SEARCH_RESULTS, summarize as briefly as possible the information\n",
    "    searched by the user. Don't give any preamble or introduction, go directly to the summary\n",
    "    of the requested information.\n",
    "    \n",
    "    If it helps to provide a more precise answer, you can also make use of the CONTEXT.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    QUERY: {query} \\n\n",
    "    SEARCH_RESULTS: {search_results} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"query\",\"search_results\",\"context\"],\n",
    ")\n",
    "\n",
    "query = 'How much does a liter of Coca Cola cost in Brazil?'\n",
    "search = page_content=\"Brazil - Coca-Cola - price, May 2024. The price is 0.86 USD. The average price for all countries is 1.04 USD. The database includes 90 countries. Definition: The Coca - Cola prices are for a bottle of 0.5 l. Adjustments were made to the various measuring units across countries to arrive at a uniform measure of 0.5 l.\\nBased on 90 countries included in our data base, the average price is 1.04 USD. Looking at the latest data, the lowest price was 0.22 USD (Nigeria) and the highest price was 2.60 USD (Norway). Definition: The Coca - Cola prices are for a bottle of 0.5 l.\\nSee current prices by country for prices of items we do track. You can see prices only for countries where we have decent number of contributors. Prices by Country of Coke/Pepsi (0.33 liter bottle) (Restaurants)\\nCoca-Cola FEMSA is the largest independent bottler of Coca-Cola products in the world, and the largest of several local bottling partners in Brazil. Within FEMSA's South America operating division (of which Brazil is the largest single market), FEMSA reported a 25.9%% increase in the average price per unit case for the first six months of 2022.\\nLarge corporations. There are three main soda companies in the country. Data below is provided by Afrebras. Coca-Cola Company, which has a market share of 55%% in volume and 62%% in value. AmBev, with a market share of 19%% in volume and 21%% in value. Brasil Kirin, with a market share of 5%% in volume and 4%% in value.\"\n",
    "\n",
    "answer_analyzer_chain = answer_analyzer_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "print(answer_analyzer_chain.invoke({\"query\": query, \"search_results\": search, \"context\": []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'operation': '^', 'op_1': 27, 'op_2': 5}\n"
     ]
    }
   ],
   "source": [
    "## CALCULATOR\n",
    "calculator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at building JSON to do calculations using a calculator tool.\n",
    "    \n",
    "    You can only output a single format of JSON object consisting in two operands\n",
    "    and the operation. The name of the only three keys are 'operation', 'op_1' and 'op_2' \\n\n",
    "    \n",
    "    'operation' can only be [+,-,*,/,^]\n",
    "    'op_1' and 'op_2' must be integers or float\\n\n",
    "    \n",
    "    If you judge that the equation consists of more than one operation, solve only one,\n",
    "    the calculator can be called multiple times and the other results will be solved\n",
    "    later.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\"],\n",
    ")\n",
    "\n",
    "calculator_chain = calculator_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'How much is 27 to the power of 5 plus 7?'\n",
    "\n",
    "print(calculator_chain.invoke({\"initial_query\": query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output generator chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information you provided, it seems unlikely that your 2010 car with 100 hp would be more powerful than a Nissan GT-R R32, which has a 2.6-liter twin-turbo engine producing around 276 horsepower.\n",
      "\n",
      "The GT-R R32 is a high-performance sports car, and its engine is significantly more powerful than your car's 100 hp engine. So, unfortunately, it's unlikely that your car would be more powerful than a GT-R R32.\n"
     ]
    }
   ],
   "source": [
    "## OUTPUT GENERATOR\n",
    "output_generator_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at answering the user based on context given. \\n\n",
    "    \n",
    "    Given the INITIAL_QUERY and a CONTEXT, generate an answer for the query\n",
    "    asked by the user. You should make use of the provided information\n",
    "    to answer the user in the best possible way. If you think the answer\n",
    "    does not answer the user completely, ask the user for the necessary\n",
    "    information if possible. \\n\n",
    "    \n",
    "    It's important never to cite that you got it from a context, the user should\n",
    "    think that you know the information.\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\"],\n",
    ")\n",
    "\n",
    "output_generator_chain = output_generator_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "query = 'Is my car more powerful than a GT-R R32?'\n",
    "context = 'The car owned by the user is from 2010 and has 100 hp'\n",
    "\n",
    "print(output_generator_chain.invoke({\"initial_query\": query, \"context\": context}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context analyzer chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ready_to_answer': False, 'next_query': 'What is the horsepower of your car?'}\n"
     ]
    }
   ],
   "source": [
    "## ANSWER ITERATOR\n",
    "context_analyzer_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a specialist at deciding if the already available information is enough to\n",
    "    fully answer the user query. \\n\n",
    "    \n",
    "    Given a INITIAL_QUERY and the available CONTEXT, decide if the available information\n",
    "    is already enough to answer the query proposed by the user. \\n\n",
    "    \n",
    "    Your job is to coordinate the usage of many tools, one at a time. To do this you will\n",
    "    decide what information you need next, with the restriction that you can only get one\n",
    "    information per iteration, and request it to the pipeline. \\n\n",
    "    \n",
    "    Your output should be a JSON object containing two keys, 'ready_to_answer' and\n",
    "    'next_query'. 'ready_to_answer' is a boolean that indicates if all necessary\n",
    "    info is present and 'next_query' is a query that you should develop so the next\n",
    "    agent in the pipeline can search for the required information. \\n\n",
    "    \n",
    "    In the following situations you must output 'next_query' as \"<KEEP_QUERY>\":\n",
    "    - User asks to modify parameters or characteristics of an energy system model;\n",
    "    - Plotting, they don't require extra information, the tools can handle it perfectly;\n",
    "    - User asks you to run a new simulation on an energy modeling system;\n",
    "    - User gives you a direct command related to modelling;\n",
    "    - The user asks anything about LangSmith (understand that as having the word LangSmith) \\n\n",
    "    \n",
    "    You also have access to the last NEXT_QUERY you generated, to avoid repeating yourself.\n",
    "    Never output the same 'next_query' that you've already asked in NEXT_QUERY. \\n\n",
    "    \n",
    "    Consider that for you boolean answer the words false and true should always be written\n",
    "    in full lower case. \\n\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    INITIAL_QUERY: {initial_query} \\n\n",
    "    CONTEXT: {context} \\n\n",
    "    NEXT_QUERY: {next_query} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"initial_query\",\"context\",\"next_query\"],\n",
    ")\n",
    "\n",
    "context_analyzer_chain = context_analyzer_prompt | json_model | JsonOutputParser()\n",
    "\n",
    "query = 'Is my car more powerful than a GT-R R32?'\n",
    "context = ['The car owned by the user is from 2010']\n",
    "\n",
    "print(context_analyzer_chain.invoke({\"initial_query\": query, \"context\": context, \"next_query\": ''}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        initial_query: user input\n",
    "        next_query: partial query generated by the agent\n",
    "        num_steps: number of steps\n",
    "        selected_tool: name of the selected tool\n",
    "        rag_questions: questions used for retrieval\n",
    "        tool_parameters: parameters to be used by tools\n",
    "        context: list of context generated for the query\n",
    "        complete_data: indicates completeness of data\n",
    "        final_answer: LLM generation\n",
    "    \"\"\"\n",
    "    initial_query : str\n",
    "    query_type: str\n",
    "    next_query: str\n",
    "    num_steps : int\n",
    "    selected_tool: str\n",
    "    rag_questions : List[str]\n",
    "    tool_parameters: str\n",
    "    context : List[str]\n",
    "    complete_data : bool\n",
    "    final_answer : str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Identifier Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_identifier(state):\n",
    "    \n",
    "    print(\"---TYPE IDENTIFIER---\")\n",
    "    query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "\n",
    "    gen = type_identifier_chain.invoke({\"query\": query})\n",
    "    selected_type = gen['query_type']\n",
    "    \n",
    "    print(f'IDENTIFIED_TYPE: {selected_type}\\n')\n",
    "    \n",
    "    return {\"query_type\": selected_type,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy System Tool Selector Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_tool_selector(state):\n",
    "    \n",
    "    print(\"---ENERGY SYSTEM TOOL SELECTION---\")\n",
    "    query = state['initial_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "\n",
    "    router = es_tool_selector_chain.invoke({\"query\": query})\n",
    "    router_decision = router['selected_tool']\n",
    "    \n",
    "    print(f'SELECTED TOOL: {router_decision}\\n')\n",
    "    \n",
    "    return {\"selected_tool\": router_decision,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed(state):\n",
    "    \n",
    "    print(\"---TOOL SELECTION---\")\n",
    "    query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "    print(f'CONTEXT: {context}')\n",
    "\n",
    "    decision = mixed_chain.invoke({\"query\": query, \"context\": context})\n",
    "    decision = decision['complete_data']\n",
    "    \n",
    "    print(f'DATA IS COMPLETE: {decision}\\n')\n",
    "    \n",
    "    return {\"complete_data\": decision,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Selector Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_selector(state):\n",
    "    \n",
    "    print(\"---TOOL SELECTION---\")\n",
    "    query = state['next_query']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'QUERY: {query}')\n",
    "\n",
    "    router = tool_selector_chain.invoke({\"query\": query})\n",
    "    router_decision = router['router_decision']\n",
    "    \n",
    "    print(f'SELECTED TOOL: {router_decision}\\n')\n",
    "    \n",
    "    return {\"selected_tool\": router_decision,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_rag(state):\n",
    "\n",
    "    print(\"---RAG LANGSMITH RETRIEVER---\")\n",
    "    initial_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    questions = question_rag_chain.invoke({\"initial_query\": initial_query})\n",
    "    questions = questions['questions']\n",
    "\n",
    "    rag_results = []\n",
    "    for idx, question in enumerate(questions):\n",
    "        print(f'QUESTION {idx}: {question}')\n",
    "        temp_docs = rag_chain.invoke(question)\n",
    "        print(f'ANSWER FOR QUESTION {idx}: {temp_docs}')\n",
    "        question_results = question + '\\n\\n' + temp_docs + \"\\n\\n\\n\"\n",
    "        if rag_results is not None:\n",
    "            rag_results.append(question_results)\n",
    "        else:\n",
    "            rag_results = [question_results]\n",
    "    print(f'FULL ANSWERS: {rag_results}\\n')\n",
    "    \n",
    "    processed_searches = answer_analyzer_chain.invoke({\"query\": initial_query, \"search_results\": rag_results, \"context\": context})\n",
    "    \n",
    "    return {\"context\": context + [processed_searches],\n",
    "            \"rag_questions\": questions,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install -U langchain-community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = secrets['tavily'][0]\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_info_web(state):\n",
    "\n",
    "    print(\"---RESEARCH INFO SEARCHING---\")\n",
    "    initial_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    # Web search\n",
    "    keywords = search_keyword_chain.invoke({\"initial_query\": initial_query, \"context\": context})\n",
    "    keywords = keywords['keywords']\n",
    "    full_searches = []\n",
    "    for idx, keyword in enumerate(keywords):\n",
    "        print(f'KEYWORD {idx}: {keyword}')\n",
    "        temp_docs = web_search_tool.invoke({\"query\": keyword})\n",
    "        if type(temp_docs) == list:\n",
    "            web_results = \"\\n\".join([d[\"content\"] for d in temp_docs])\n",
    "            web_results = Document(page_content=web_results)\n",
    "        elif type(temp_docs) == dict:\n",
    "            web_results = temp_docs[\"content\"]\n",
    "            web_results = Document(page_content=web_results)\n",
    "        else:\n",
    "            web_results = 'No results'\n",
    "        print(f'RESULTS FOR KEYWORD {idx}: {web_results}')\n",
    "        if full_searches is not None:\n",
    "            full_searches.append(web_results)\n",
    "        else:\n",
    "            full_searches = [web_results]\n",
    "    print(f'FULL RESULTS: {full_searches}\\n')\n",
    "    \n",
    "    processed_searches = answer_analyzer_chain.invoke({\"query\": initial_query, \"search_results\": full_searches, \"context\": context})\n",
    "    \n",
    "    print(f'PROCESSED RESULT: {processed_searches}')\n",
    "    \n",
    "    return {\"context\": context + [processed_searches],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(state):\n",
    "\n",
    "    print(\"---CALCULATOR TOOL---\")\n",
    "    \n",
    "    query = state['next_query']\n",
    "    context = state['context']\n",
    "    parameters = calculator_chain.invoke({\"initial_query\": query})\n",
    "    operation = parameters['operation']\n",
    "    op_1 = parameters['op_1']\n",
    "    op_2 = parameters['op_2']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    print(f'OPERATION: {operation}')\n",
    "    print(f'OPERAND 1: {op_1}')\n",
    "    print(f'OPERAND 2: {op_2}')\n",
    "\n",
    "    if operation == \"+\":\n",
    "        result = op_1 + op_2\n",
    "    elif operation == \"-\":\n",
    "        result = op_1 - op_2\n",
    "    elif operation == \"/\":\n",
    "        result = op_1 / op_2\n",
    "    elif operation == \"*\":\n",
    "        result = op_1 * op_2\n",
    "    elif operation == \"^\":\n",
    "        result = op_1 ** op_2\n",
    "    else:\n",
    "        result = 'ERROR'\n",
    "        \n",
    "    if result == 'ERROR':\n",
    "        str_result = 'Unable to execute the selected operation'\n",
    "    else:\n",
    "        str_result = f'{op_1} {operation} {op_2} = {result}'\n",
    "        \n",
    "    print(f'RESULT: {str_result}\\n')\n",
    "        \n",
    "    return {\"context\": context + [str_result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Getter Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def date_getter(state):\n",
    "\n",
    "    print(\"---DATE GETTER TOOL---\")\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    current_date = datetime.now().strftime(\"%d %B %Y, %H:%M:%S\")\n",
    "    \n",
    "    result = f'The current date and time are {current_date}'\n",
    "    \n",
    "    print(f'CURRENT DATE: {current_date}\\n')\n",
    "\n",
    "    return {\"context\": context + [result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Modifier Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_modifier(state):\n",
    "\n",
    "    print(\"---MODEL MODIFIER TOOL---\")\n",
    "    query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    parameters_json = model_modifier_chain.invoke({\"initial_query\": query, \"context\": context})\n",
    "    print(f'JSON:\\n{parameters_json}\\n')\n",
    "    \n",
    "    result = f'The model was successfully modified'\n",
    "\n",
    "    return {\"context\": context + [result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_plotter(state):\n",
    "\n",
    "    print(\"---DATA PLOTTER TOOL---\")\n",
    "    query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    parameters_json = data_plotter_chain.invoke({\"initial_query\": query, \"context\": context})\n",
    "    print(f'JSON:\\n{parameters_json}\\n')\n",
    "    \n",
    "    result = f'The plot was successfully generated'\n",
    "\n",
    "    return {\"context\": context + [result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_runner(state):\n",
    "\n",
    "    print(\"---SIMULATION RUNNER TOOL---\")\n",
    "    query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    \n",
    "    parameters_json = sim_runner_chain.invoke({\"initial_query\": query, \"context\": context})\n",
    "    print(f'JSON:\\n{parameters_json}\\n')\n",
    "    \n",
    "    result = f'The new simulation was successfully submited'\n",
    "\n",
    "    return {\"context\": context + [result],\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generator Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_generator(state):\n",
    "    print(\"---GENERATE OUTPUT---\")\n",
    "    ## Get the state\n",
    "    initial_query = state['initial_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    # Generate draft email\n",
    "    answer = output_generator_chain.invoke({\"initial_query\": initial_query,\n",
    "                                            \"context\": context})\n",
    "    print(f'GENERATED OUTPUT:\\n{answer}\\n')\n",
    "    \n",
    "    return {\"final_answer\": answer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Analyzer Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_analyzer(state):\n",
    "    print(\"---CONTEXT ANALYZER---\")\n",
    "    ## Get the state\n",
    "    initial_query = state['initial_query']\n",
    "    next_query = state['next_query']\n",
    "    context = state['context']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "\n",
    "    output = context_analyzer_chain.invoke({\"initial_query\": initial_query,\n",
    "                                           \"next_query\": next_query,\n",
    "                                           \"context\": context\n",
    "                                           })\n",
    "    \n",
    "    if output['next_query'] == '<KEEP_QUERY>':\n",
    "        output['next_query'] = state['initial_query']\n",
    "    \n",
    "    return {\"next_query\": output,\n",
    "            \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_printer(state):\n",
    "    \"\"\"print the state\"\"\"\n",
    "    print(\"------------------STATE PRINTER------------------\")\n",
    "    print(f\"Num Steps: {state['num_steps']} \\n\")\n",
    "    print(f\"Initial Query: {state['initial_query']} \\n\" )\n",
    "    print(f\"Next Query: {state['next_query']} \\n\" )\n",
    "    print(f\"RAG Questions: {state['rag_questions']} \\n\")\n",
    "    print(f\"Tool Parameters: {state['tool_parameters']} \\n\")\n",
    "    print(f\"Context: {state['context']} \\n\" )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer_printer(state):\n",
    "    \"\"\"prints final answer\"\"\"\n",
    "    print(\"------------------FINAL ANSWER------------------\")\n",
    "    print(f\"Final Answer: {state['final_answer']} \\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_type(state):\n",
    "    \"\"\"\n",
    "    Route to the right path based on query type.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    type = state['query_type']\n",
    "    \n",
    "    if type == 'general':\n",
    "        print(\"---ROUTE QUERY TO GENERAL PATH---\")\n",
    "        return \"general\"\n",
    "    elif type == 'energy_system':\n",
    "        print(\"---ROUTE QUERY TO ENERGY SYSTEM PATH---\")\n",
    "        return \"energy_system\"\n",
    "    elif type == 'mixed':\n",
    "        print(\"---ROUTE QUERY TO MIXED PATH---\")\n",
    "        return \"mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_from_mix(state):\n",
    "\n",
    "    print(\"---ROUTE TO MIX---\")\n",
    "    data_completeness = state['complete_data']\n",
    "\n",
    "    print(data_completeness)\n",
    "    if data_completeness:\n",
    "        print(\"---APPLY COMMAND---\")\n",
    "        return \"complete_data\"\n",
    "    else:\n",
    "        print(\"---GATHER MORE CONTEXT---\")\n",
    "        return \"needs_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_es_tool(state):\n",
    "    \"\"\"\n",
    "    Route to the necessary tool.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    selection = state['selected_tool']\n",
    "    \n",
    "    if selection == 'data_plotter':\n",
    "        print(\"---ROUTE QUERY TO DATA PLOTTER---\")\n",
    "        return \"data_plotter\"\n",
    "    elif selection == 'sim_runner':\n",
    "        print(\"---ROUTE QUERY TO SIMULATION RUNNER---\")\n",
    "        return \"sim_runner\"\n",
    "    elif selection == 'model_modifier':\n",
    "        print(\"---ROUTE QUERY TO MODEL MODIFIER---\")\n",
    "        return \"model_modifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_tool(state):\n",
    "    \"\"\"\n",
    "    Route to the necessary tool.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    selection = state['selected_tool']\n",
    "    \n",
    "    if selection == 'RAG_retriever':\n",
    "        print(\"---ROUTE QUERY TO RAG RETRIEVER---\")\n",
    "        return \"RAG_retriever\"\n",
    "    elif selection == 'web_search':\n",
    "        print(\"---ROUTE QUERY TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif selection == 'calculator':\n",
    "        print(\"---ROUTE QUERY TO CALCULATOR---\")\n",
    "        return \"calculator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_iterate(state):\n",
    "\n",
    "    print(\"---ROUTE TO ITERATE---\")\n",
    "    next_query = state['next_query']\n",
    "\n",
    "    print(next_query)\n",
    "    if next_query['ready_to_answer']:\n",
    "        print(\"---GENERATE FINAL ANSWER---\")\n",
    "        return \"ready_to_answer\"\n",
    "    else:\n",
    "        print(\"---GATHER MORE CONTEXT---\")\n",
    "        return \"need_context\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "\n",
    "# %pip install -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"type_identifier\", type_identifier)\n",
    "workflow.add_node(\"es_tool_selector\", es_tool_selector)\n",
    "workflow.add_node(\"mixed\", mixed)\n",
    "workflow.add_node(\"tool_selector\", tool_selector)\n",
    "workflow.add_node(\"research_info_rag\", research_info_rag) # RAG search\n",
    "workflow.add_node(\"research_info_web\", research_info_web) # web search\n",
    "workflow.add_node(\"state_printer\", state_printer)\n",
    "workflow.add_node(\"calculator\", calculator)\n",
    "workflow.add_node(\"date_getter\", date_getter)\n",
    "workflow.add_node(\"model_modifier\", model_modifier)\n",
    "workflow.add_node(\"data_plotter\", data_plotter)\n",
    "workflow.add_node(\"sim_runner\", sim_runner)\n",
    "workflow.add_node(\"output_generator\", output_generator)\n",
    "workflow.add_node(\"context_analyzer\", context_analyzer)\n",
    "workflow.add_node(\"final_answer_printer\", final_answer_printer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"date_getter\")\n",
    "workflow.add_edge(\"date_getter\", \"type_identifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"type_identifier\",\n",
    "    route_to_type,\n",
    "    {\n",
    "        \"general\": \"context_analyzer\",\n",
    "        \"energy_system\": \"es_tool_selector\",\n",
    "        \"mixed\": \"mixed\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"mixed\",\n",
    "    route_from_mix,\n",
    "    {\n",
    "        \"complete_data\": \"es_tool_selector\",\n",
    "        \"needs_data\": \"context_analyzer\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"es_tool_selector\",\n",
    "    route_to_es_tool,\n",
    "    {\n",
    "        \"model_modifier\": \"model_modifier\",\n",
    "        \"data_plotter\": \"data_plotter\",\n",
    "        \"sim_runner\": \"sim_runner\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"model_modifier\", \"output_generator\")\n",
    "workflow.add_edge(\"data_plotter\", \"output_generator\")\n",
    "workflow.add_edge(\"sim_runner\", \"output_generator\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"context_analyzer\",\n",
    "    route_to_iterate,\n",
    "    {\n",
    "        \"ready_to_answer\": \"output_generator\",\n",
    "        \"need_context\": \"tool_selector\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"tool_selector\",\n",
    "    route_to_tool,\n",
    "    {\n",
    "        \"RAG_retriever\": \"research_info_rag\",\n",
    "        \"web_search\": \"research_info_web\",\n",
    "        \"calculator\": \"calculator\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"research_info_rag\", \"state_printer\")\n",
    "workflow.add_edge(\"research_info_web\", \"state_printer\")\n",
    "workflow.add_edge(\"calculator\", \"state_printer\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"state_printer\",\n",
    "    route_to_type,\n",
    "    {\n",
    "        \"general\": \"context_analyzer\",\n",
    "        \"mixed\": \"mixed\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"output_generator\", \"final_answer_printer\")\n",
    "workflow.add_edge(\"final_answer_printer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DATE GETTER TOOL---\n",
      "CURRENT DATE: 20 June 2024, 22:15:39\n",
      "\n",
      "Finished running: date_getter:\n",
      "---TYPE IDENTIFIER---\n",
      "QUERY: Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.\n",
      "IDENTIFIED_TYPE: mixed\n",
      "\n",
      "---ROUTE QUERY TO MIXED PATH---\n",
      "Finished running: type_identifier:\n",
      "---TOOL SELECTION---\n",
      "QUERY: Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.\n",
      "CONTEXT: ['The current date and time are 20 June 2024, 22:15:39']\n",
      "DATA IS COMPLETE: False\n",
      "\n",
      "---ROUTE TO MIX---\n",
      "False\n",
      "---GATHER MORE CONTEXT---\n",
      "Finished running: mixed:\n",
      "---CONTEXT ANALYZER---\n",
      "---ROUTE TO ITERATE---\n",
      "{'ready_to_answer': False, 'next_query': 'What is the current price of one liter of Coca Cola in Brazil?'}\n",
      "---GATHER MORE CONTEXT---\n",
      "Finished running: context_analyzer:\n",
      "---TOOL SELECTION---\n",
      "QUERY: {'ready_to_answer': False, 'next_query': 'What is the current price of one liter of Coca Cola in Brazil?'}\n",
      "SELECTED TOOL: web_search\n",
      "\n",
      "---ROUTE QUERY TO WEB SEARCH---\n",
      "Finished running: tool_selector:\n",
      "---RESEARCH INFO SEARCHING---\n",
      "KEYWORD 0: Coca Cola Brazil price\n",
      "RESULTS FOR KEYWORD 0: page_content=\"Brazil - Coca-Cola - price, May 2024. The price is 0.86 USD. The average price for all countries is 1.04 USD. The database includes 90 countries. Definition: The Coca - Cola prices are for a bottle of 0.5 l. Adjustments were made to the various measuring units across countries to arrive at a uniform measure of 0.5 l.\\nBased on 90 countries included in our data base, the average price is 1.04 USD. Looking at the latest data, the lowest price was 0.22 USD (Nigeria) and the highest price was 2.60 USD (Norway). Definition: The Coca - Cola prices are for a bottle of 0.5 l.\\nSee current prices by country for prices of items we do track. You can see prices only for countries where we have decent number of contributors. Prices by Country of Coke/Pepsi (0.33 liter bottle) (Restaurants)\\nCoca-Cola FEMSA is the largest independent bottler of Coca-Cola products in the world, and the largest of several local bottling partners in Brazil. Within FEMSA's South America operating division (of which Brazil is the largest single market), FEMSA reported a 25.9% increase in the average price per unit case for the first six months of 2022.\\nLarge corporations. There are three main soda companies in the country. Data below is provided by Afrebras. Coca-Cola Company, which has a market share of 55% in volume and 62% in value. AmBev, with a market share of 19% in volume and 21% in value. Brasil Kirin, with a market share of 5% in volume and 4% in value.\"\n",
      "FULL RESULTS: [Document(page_content=\"Brazil - Coca-Cola - price, May 2024. The price is 0.86 USD. The average price for all countries is 1.04 USD. The database includes 90 countries. Definition: The Coca - Cola prices are for a bottle of 0.5 l. Adjustments were made to the various measuring units across countries to arrive at a uniform measure of 0.5 l.\\nBased on 90 countries included in our data base, the average price is 1.04 USD. Looking at the latest data, the lowest price was 0.22 USD (Nigeria) and the highest price was 2.60 USD (Norway). Definition: The Coca - Cola prices are for a bottle of 0.5 l.\\nSee current prices by country for prices of items we do track. You can see prices only for countries where we have decent number of contributors. Prices by Country of Coke/Pepsi (0.33 liter bottle) (Restaurants)\\nCoca-Cola FEMSA is the largest independent bottler of Coca-Cola products in the world, and the largest of several local bottling partners in Brazil. Within FEMSA's South America operating division (of which Brazil is the largest single market), FEMSA reported a 25.9% increase in the average price per unit case for the first six months of 2022.\\nLarge corporations. There are three main soda companies in the country. Data below is provided by Afrebras. Coca-Cola Company, which has a market share of 55% in volume and 62% in value. AmBev, with a market share of 19% in volume and 21% in value. Brasil Kirin, with a market share of 5% in volume and 4% in value.\")]\n",
      "\n",
      "PROCESSED RESULT: Based on the user's query and the search results, here is a brief summary of the information:\n",
      "\n",
      "The current price of one liter of Coca Cola in Brazil is approximately 1.72 USD (0.86 USD for 0.5 liter).\n",
      "Finished running: research_info_web:\n",
      "------------------STATE PRINTER------------------\n",
      "Num Steps: 6 \n",
      "\n",
      "Initial Query: Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil. \n",
      "\n",
      "Next Query: {'ready_to_answer': False, 'next_query': 'What is the current price of one liter of Coca Cola in Brazil?'} \n",
      "\n",
      "RAG Questions: None \n",
      "\n",
      "Tool Parameters: None \n",
      "\n",
      "Context: ['The current date and time are 20 June 2024, 22:15:39', \"Based on the user's query and the search results, here is a brief summary of the information:\\n\\nThe current price of one liter of Coca Cola in Brazil is approximately 1.72 USD (0.86 USD for 0.5 liter).\"] \n",
      "\n",
      "---ROUTE QUERY TO MIXED PATH---\n",
      "---TOOL SELECTION---\n",
      "QUERY: Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.\n",
      "CONTEXT: ['The current date and time are 20 June 2024, 22:15:39', \"Based on the user's query and the search results, here is a brief summary of the information:\\n\\nThe current price of one liter of Coca Cola in Brazil is approximately 1.72 USD (0.86 USD for 0.5 liter).\"]\n",
      "DATA IS COMPLETE: True\n",
      "\n",
      "---ROUTE TO MIX---\n",
      "True\n",
      "---APPLY COMMAND---\n",
      "Finished running: mixed:\n",
      "---ENERGY SYSTEM TOOL SELECTION---\n",
      "QUERY: Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.\n",
      "SELECTED TOOL: model_modifier\n",
      "\n",
      "---ROUTE QUERY TO MODEL MODIFIER---\n",
      "Finished running: es_tool_selector:\n",
      "---MODEL MODIFIER TOOL---\n",
      "JSON:\n",
      "{'parameter': 'lifetime_wind_power_plants', 'new_value': 1.72}\n",
      "\n",
      "Finished running: model_modifier:\n",
      "---GENERATE OUTPUT---\n",
      "GENERATED OUTPUT:\n",
      "Based on the provided context, I understand that you want to modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.\n",
      "\n",
      "According to the context, the current price of one liter of Coca Cola in Brazil is approximately 1.72 USD.\n",
      "\n",
      "Therefore, I will modify the lifetime of wind power plants to be 1.72 years. This means that the wind power plants will now have a lifetime of 1 year and 9.2 months.\n",
      "\n",
      "Please note that this is an unusual modification, as wind power plants typically have a lifetime of 20-30 years. This modified lifetime may affect the overall efficiency and feasibility of the wind power plants.\n",
      "\n",
      "If you have any further clarification or adjustments, please let me know!\n",
      "\n",
      "Finished running: output_generator:\n",
      "------------------FINAL ANSWER------------------\n",
      "Final Answer: Based on the provided context, I understand that you want to modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.\n",
      "\n",
      "According to the context, the current price of one liter of Coca Cola in Brazil is approximately 1.72 USD.\n",
      "\n",
      "Therefore, I will modify the lifetime of wind power plants to be 1.72 years. This means that the wind power plants will now have a lifetime of 1 year and 9.2 months.\n",
      "\n",
      "Please note that this is an unusual modification, as wind power plants typically have a lifetime of 20-30 years. This modified lifetime may affect the overall efficiency and feasibility of the wind power plants.\n",
      "\n",
      "If you have any further clarification or adjustments, please let me know! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query = 'If I pay half the age of Tom Jobim plus the height of the Empire State for a car, how much I\\'ve paid?'\n",
    "#query = 'What is 10 to the power of 0.4?'\n",
    "#query = 'What is the temperature and humidity in Migliarino right now? And also, what time is it?'\n",
    "#query = 'Modify the parameter X to 24 for me please'\n",
    "#query = 'What are some of the most important things that happened today in past years?'\n",
    "#query = 'What day is today?'\n",
    "#query = 'How can LangSmith help in my project?'\n",
    "#query = 'I am always coming but never arrive. What am I?'\n",
    "#query = 'Change the lifetime of wind power plants to 25 years please'\n",
    "query = 'Divide the height of the Burj Khalifa by Ronaldinho Gaucho\\'s age, then add the current temperature in Paris (in Celsius)'\n",
    "#query = 'What are good famous and more casual board games that can be played by two players?'\n",
    "#query = 'Divide the number of visitors that the Eiffel tower receives yearly by the number of cars in the city of So Paulo, Brazil'\n",
    "#query = 'Change the lifetime of wind power plants to be the age of Olaf Scholz'\n",
    "query = 'Modify the lifetime of wind power plants to be the same value as the price of one liter of Coca Cola in Brazil.'\n",
    "\n",
    "# run the agent\n",
    "inputs = {\"initial_query\": query, \"next_query\": '', \"num_steps\": 0, \"context\": []}\n",
    "for output in app.stream(inputs, {\"recursion_limit\": 50}):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished running: {key}:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
